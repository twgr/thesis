% !TEX root = ../main.tex

\chapter{Inference}
\label{sec:inf}

\section{The Challenge of Bayesian Inference}
\label{sec:inf:challenge}

In the previous chapter we introduced the concept of Bayesian modelling and showed how we
can combine prior information $p(x)$ and a likelihood model $p(y|x)$ using Bayes' rule,
i.e.~\eqref{eq:bayes}, to produce a posterior $p(x|y)$ that
characterizes both our prior information and information from data.  We now consider the
problem of how to calculate (or more typically approximate) this posterior, a process 
known as Bayesian \emph{inference}.
At first this may seem like a straight forward problem: by Bayes' rule we have that
$p(x|y)\propto p(y|x)p(x)$ and so we already know the relative probability of any one
value of $x$ compared to another.  In practice, this could hardly be further from the
truth.  Bayesian inference for the general class of graphical models is in fact an 
NP-hard problem \citep{cooper1990computational,dagum1993approximating}.  We can break
down in the problem in to key challenges: calculating the normalization constant
$p(y) = \int p(y|x)p(x)dx$ and providing a useful characterization of the posterior, for
example an object we can draw samples from.  Many inference schemes, for example Markov
chain Monte Carlo (MCMC) \citep{hastings1970monte}, will not
try to tackle these challenges directly and instead look to generate samples directly from 
the posterior.  However, this breakdown will still prove useful in illustrating the intuitions
about the difficulties presented by Bayesian inference.

Calculating the normalization constant in Bayesian inference is essentially a problem of
integration.  Our target, $p(y)$, is the expectation of the likelihood under the prior,
hence the name \emph{marginal likelihood}.  When $p(y)$ is known, the posterior can be evaluated
exactly at any possible input point using~\eqref{eq:bayes} directly.  When it is unknown, we lack
a scaling in the evaluation of any point and so we have no concept of how relatively 
significant that point is relative to the distribution as a whole.  For example, for a discrete
problem then if we know the normalizing constant, we can evaluate the exact probability of any
particular $x$ by evaluating that point alone.  If we do not know the normalizing constant, we do
not know if there are other substantially more probable events that we have thus-far missed, which
would in turn imply that the queried point has a negligible chance of occurring.
 
To give a more explicit example, consider a model where $x \in \{1,2,3\}$ with a corresponding uniform prior $P(x) = 1/3$
for each $x$.  Now presume that for some reason that we are only able to evaluate the likelihood at 
$x=1$ and $x=2$, giving $p(y|x=1)=1$ and $p(y|x=2)=10$ respectively.  Depending on the marginal
likelihood $p(y)$, the posterior probability of $P(x=2 | y)$ will vary wildly.  For example,
$p(y)=4$ gives $P(x=2 | y) = 5/6$, while $p(y)=1000$ gives $P(x=2 | y) = 1/100$.  Though this
example may seem far-fetched, this is the scenario almost always seen in practice for realistic
models, at least all those with non-trivial solutions.  A common scenario is that it is not
possible to enumerate all the possible values of $x$ in reasonable time and we are left 
wondering - how much probability mass is left that we have not seen?  The problem is even worse 
in the setting where $x$ is continuous, for
which it is naturally impossible to evaluate all possible values for $x$.  
Knowing the posterior only up to a normalization constant is deceptively unhelpful - we never
know how much of the probability mass we have missed and therefore whether the probability (or
probability density) where we have looked so far is tiny compared to some other dominant region
we are yet to explore.  At its heart, the problem of Bayesian inference is effectively the problem
about where to concentrate our finite computational resources so that we can effectively characterize
the posterior.  If $p(y)$ is known, then we immediately know whether we are looking in the right
place or whether we are yet to find where most of the posterior mass is hidden.  This brings us onto
our second challenge - knowing the posterior in closed form is often not enough!

\todo[inline]{Maybe talk about things in terms of the explore-exploit dilema a bit?}

\input{inf/particles}