% !TEX root = ../main.tex

\section{Monte Carlo}
\label{sec:inf:mc}

Monte Carlo~\citep{metropolis1949monte} is the characterization of a probability distribution 
through random sampling. It is the foundation for a huge array of methods for numerical 
integration, optimization, and scientific simulation; forming the underlying principle 
for all stochastic computation.
\mc provides us with a means of dealing with complex models and problems in a
statistically principled manner.  Without it, one would have to resort to deterministic
approximation methods whenever the target problem is too complex to permit analytic
solution.  As we will show, it is a highly composable framework that will allow to output
of one system to be input directly to another.  For example, the \mc samples from a joint
distribution will also have the correct marginal distribution over any of its individual components,
while sampling from the marginal distribution then sampling from the conditional distribution
given these samples will give samples distributed according to the joint.  As \mc
will be key to most methods for Bayesian inference that we will consider, we take the time
in this section key related concepts.

The most common usage of \mc in this work will be the \mc estimation of expectations, 
sometimes known as \mc integration.  
The critical importance of \mc estimation stems from the fact that most of the example
target tasks laid out in~\ref{sec:inf:challenge:post} can be formulated as expectations.
Even when our intention is simply to generate samples from a target distribution, we can
usually think of this as being an implicit expectation of an, as-yet unknown, target function.
Here our implicit aims is to minimize the bias and variance of whatever process the samples are eventually
used for, even if that process is simply visual inspection.  

Consider the problem of calculating the expectation of some function
$f(\theta)$ under the distribution $\theta\sim \pi(\theta)$ ($= p(\theta | \mathcal{D})$ for the Bayesian
inference case), which we will denote 
as\footnote{We will often implicitly use the notion of a density
	$\pi(\theta)$ for both continuous and discrete random variables in the interest
	convenience.  We will also at times be intentionally carefree about delineating between random
	variables and the inputs to density functions.  More generally, we will avoid the use of measure
	theory notation throughout this thesis when possible.  These notational decisions have been made
	primarily in the interest of accessibility as most of the ideas introduced do not require a deep
	knowledge of measure theory to understand.  Nonetheless, we note that a measure theoretic
	approach to probability is essential for a more rigorous introduction.  We refer
	the interested reader to~\cite{durrett2010probability} for an introduction to measure-theoretic
	probability theory and~\cite{robert2004monte} for a correspondingly more rigorous introduction
	to \mc methods.}
\begin{align}
	\label{eq:inf:expt}
I:=\E_{\pi(\theta)} \left[f(\theta)\right]=\int f(\theta) \pi(\theta) d\theta.
\end{align}
This can be approximated using the following \mc estimate $I_N$ where
\begin{align}
	\label{eq:inf:mc-est}
	I \approx I_N := \frac{1}{N} \sum_{n=1}^{N}f(\hat{\theta}_n)
	\quad \text{where} \quad \hat{\theta}_n \sim \pi(\theta).
\end{align}
The first result we note is that~\eqref{eq:inf:mc-est} is an \emph{unbiased} estimate for $I$.  This can
be easily shown by noting that
\begin{align}
\label{eq:inf:unbiased}
\E \left[I_N\right] = \E \left[\frac{1}{N} \sum_{n=1}^{N}f(\hat{\theta}_n)\right]
= \frac{1}{N} \sum_{n=1}^{N} \E \left[f(\hat{\theta}_n)\right]
= \frac{1}{N} \sum_{n=1}^{N} \E \left[f(\hat{\theta}_1)\right]
= I
\end{align}
where we have first moved the sum outside of expectation using
linearity,\footnote{Note that this presumes that $N$ is independent
	of the samples.  This is usually the case, but care is necessary in some situations.}
then the fact that each $\hat{\theta}_n$ is identically distributed to note that
each $\E \left[f(\hat{\theta}_n)\right]= \E \left[f(\hat{\theta}_1)\right]$, and finally
that $\E \left[f(\hat{\theta}_1)\right] = I$ by the definition of $I$ and the distribution
on $\hth_1$.  This is an important result as it means that \mc does not introduce
any systematic error, i.e. bias, into the approximation: in expectation it does not
pathologically overestimate or underestimate the target.  This is not to say though that it is
equally likely to overestimate or underestimate as it may, for example, typically underestimate
by a small amount and then rarely overestimate by a large amount.  Instead, it means that if we
were to repeat the estimation an infinite number of times and average the results, we would
get the true value of $I$.  This now hints at another important question -- do we also get
recover the true value of $I$ when we conduct one infinitely large estimation, namely if we
take $N\rightarrow\infty$?  This is known as \emph{consistency} of a statistical estimator,
which we will now consider next.  Before moving
on, we make the important note that many common \mc inference methods, for example MCMC as
discussed in Section~\ref{sec:inf:foundation:mcmc}), are in fact biased.  
This is because it is often not possible to $\hth_n \sim \pi(\theta)$
exactly as we have assumed in~\eqref{eq:inf:mc-est}, with the bias resulting from
the approximation.  The convergence of such methods relies on the bias 
diminishing to $0$ as $N\rightarrow\infty$, such that they remain unbiased in the limit.

\subsection{The Law of Large Numbers}
\label{sec:inf:mc:law}

A key mathematical idea underpinning the convergence of many Monte Carlo methods is the 
law of large numbers (LLN).  Informally, the LLN states that the empirical average of 
independent and identically distributed (i.i.d.)  random variables converges to 
the true expected value of the underlying process as the number of samples in the
average increases.  We can there use it to prove the consistency of Monte Carlo estimators
were the samples are drawn independently from the same distribution, as is the case
in for example rejection sampling and importance sampling.  The high level idea for the LLN can be shown by
considering the  \emph{mean squared error} of a \mc estimator as 
follows\footnote{Note here that we use the notation of expectation typically used within the statistics literature,
whereby it corresponds to the expectation over all randomness contained in the system.}
\begin{align}
\E &\left[(I_N-I)^2\right] = \E\left[\left(\frac{1}{N}\sum_{n=1}^{N}f(\hth_n) - I\right)^2\right] \nonumber \\
&= \frac{1}{N^2}\sum_{n=1}^{N} \E\left[ \left(f(\hth_n)-I\right)^2\right] + 
\frac{1}{N(N-1)}\sum_{n=1}^{N}\sum_{m=1,m\neq n}^{N} \E\left[ (f(\hth_n)-I)(f(\hth_m)-I)\right] \nonumber \\
&= \frac{1}{N^2}\sum_{n=1}^{N} \E\left[ \left(f(\hth_1)-I\right)^2\right] + 
\frac{1}{N(N-1)}\sum_{n=1}^{N}\sum_{m=1,m\neq n}^{N} \cancelto{0}{\left(\E\left[(f(\hth_1)-I)\right]\right)^2} \nonumber \\
&= \frac{\sigma_{\theta}^2}{N}  \quad \text{where} \quad \sigma_{\theta}^2 := \E\left[ \left(f(\hth_1)-I\right)^2\right]
= \var \left[f(\theta)\right].\label{eq:inf:LLN-informal}
\end{align}
Here the second line follows from the first simply by expanding the square and using linearity
to move the sum outside of the expectation as in the unbiasedness derivation.
The first term in the third line follows from the equivalent term in the second line by again noting that
each $\hth_n$ has the same distribution.  The second term in the third line
follows from the assumption that the samples are drawn independently such that
\[
\E\left[ (f(\hth_n)-I)(f(\hth_m)-I)\right] = \E\left[ (f(\hth_n)-I)\right] \E\left[(f(\hth_m)-I)\right],
\]
The last line simply notes that $\E\left[ \left(f(\hth_1)-I\right)^2\right]$ is a constant,
namely the variance of $f(\theta)$.
  Our final result has a simple and intuitive form -- the mean squared error for
our estimator using $N$ samples, is $1/N$ times the mean squared error of an estimator that only uses
a single sample, which is itself equal to the variance of $f(\theta)$.  As $N\rightarrow\infty$, we thus
have that our expected error goes to $0$.

\subsection{Convergence of Random Variables}
\label{sec:inf:mc:conv}

To introduce the concept the LLN more precisely, we now consider some more formal notations
of convergence of random variables.  There will be times (in particular in Chapter~\ref{chp:nest})
when mathematical rigour will require us to distinguish between these alternative notations of convergence.
However, for most practical purposes, any one of these notions of convergence will be sufficient 
to informally guarantee that an estimator will return the correct answer if provided 
with sufficient samples.  Therefore those less interested in theoretical details may wish to
skip Sections~\ref{sec:inf:mc:conv:prob} and~\ref{sec:inf:mc:conv:as} on first reading.

\subsubsection{$L^p$-Convergence}
\label{sec:inf:mc:conv:Lr}

We start by introducing the notion of $L^p$-convergence, also known as convergence in expectation,
as this is the type of convergence we have just alluded to in our informal proof of the LLN.
At a high level, $L^p$-convergence means that the expected value of the related error metric
tends to zero as $N\rightarrow \infty$.  More precisely, we first define the $L^p$-norm for
a random variable $X$ as
\begin{align}
\label{eq:inf:Lp-norm}
\norm{X}_p = \left(\E \left[\left|X\right|^{p}\right]\right)^{\frac{1}{p}}
\end{align}
where $\left|\cdot\right|$ denotes the absolute value.  For example, we can write the
mean squared error used in~\eqref{eq:inf:LLN-informal} as the squared $L^2$-norm:
$\E \left[(I_N-I)^2\right] = \norm{I_N-I}_2^2$.  We further define the notion of $L^p$-space
as being the space of random variables for which $\norm{X}_p < \infty$.  We can now
formally define $L^p$-convergence as follow.
\begin{definition}[$L^p$-convergence]
A sequence $X_N$ converges in its $L^p$-norm to $X$ if $X\in L^p$, each $X_N \in L^p$, and
\begin{align}
\lim\limits_{N\rightarrow\infty} \norm{X_N-X}_p=0. \label{eq:inf:Lp-conv-formal}
\end{align}
\end{definition}
A key point to note is that $\Vert X_N-X\rVert_p\ge0 \; \forall X_N, X$ by definition of the $L^p$-norm and so
rather than this simply being a statement of asymptotic unbiasedness, $L^p$-convergence says that the expected
\emph{magnitude} of the error tends to zero as $N\rightarrow\infty$.
Different values of $p$ correspond to different metrics for the error, with larger values of
$p$ constituting stronger converge guarantees, such that $L^{p_1}$-convergence implies
$L^{p_2}$-convergence whenever $p_1>p_2$.

%
%\subsubsection{Convergence in Distribution}
%\label{sec:inf:mc:conv:dist}
%
%Convergence in distribution is a weaker form of convergence than is implied by all the
%other forms of convergence that we will discuss.

\subsubsection{Convergence in Probability}
\label{sec:inf:mc:conv:prob}

At a high level, convergence in probability between two random variables (or between a random variable and
a constant) means that they become arbitrary close to one another with high probability.  More
formally we have the following definition.
\begin{definition}[Convergence in probability]
A sequence $X_N$ converges in probability to $X$ if, for every $\varepsilon>0$,
\begin{align}
\lim\limits_{N\rightarrow\infty} P(\left|X_N-X\right|\ge\varepsilon)=0.
\end{align}
\end{definition}
As $\varepsilon$ can be made arbitrarily small, this ensures that $X_N$ becomes arbitrarily
close to $X$ in the limit of large $N$.  Estimators are \emph{consistent} if they converge
in probability.
Convergence in probability is, in general, a weaker form of convergence that $L^p$ convergence
as $L^p$ convergence for any $p\ge1$ implies convergence in probability~\cite{williams1991probability}.

In~\eqref{eq:inf:LLN-informal} we demonstrated of the $L^2$ convergence of the Monte Carlo
estimator as we have that 
\[
\lim\limits_{N\rightarrow\infty} \norm{I_N-I}_2=\lim\limits_{N\rightarrow\infty} \frac{\sigma_\theta}{\sqrt{N}} = 0.
\]
By the above result we therefore also have the Monte Carlo estimator convergences in
probability to the expectation and thus that it is consistent.  This is known as the
\emph{weak law of large numbers}.  We can also prove this more explicitly as follows
\begin{theorem}[Weak law of large numbers]
If $I$ and $I_N$ are defined as per~\eqref{eq:inf:expt} and~\eqref{eq:inf:mc-est} respectively,
$I\in L^2$, each $I_N \in L^2$,
 and each $\hth_n$ in~\eqref{eq:inf:mc-est} is drawn independently, then $I_N$ converges to $I$
in probability:
\begin{align}
	\lim\limits_{N\rightarrow\infty} P(\left|I_N-I\right|\ge \varepsilon)=0 \quad \forall \varepsilon>0.
\end{align}
\end{theorem}
\begin{proof}
By~\eqref{eq:inf:unbiased} we have that $\E [I_N]=I$ and by~\eqref{eq:inf:LLN-informal} we have that 
\[
\norm{I_N-I}_2^2 = \frac{\sigma_{\theta}^2}{N}
\]
where $\sigma_{\theta}^2$ is an unknown finite constant by the assumption that $I\in L^2$.
Chebyshev's inequality states that if $\E [I_N] = I$, then for any $k>0$
\[
P(\left|I_N-I\right|\ge\varepsilon)\le\frac{\var(I_N)}{\varepsilon^2}.
\]
By further noting that as $I_N$ is unbiased, we have that $\var(I_N) = \norm{I_N-I}_2^2$
and therefore
\[
\lim\limits_{N\rightarrow\infty} P(\left|I_N-I\right|\ge\varepsilon)
\le\lim\limits_{N\rightarrow\infty} \frac{\sigma_{\theta}^2}{\varepsilon^2 N}
=0 \quad \forall \varepsilon>0
\]
as required.
\end{proof}

\subsubsection{Almost Sure Convergence}
\label{sec:inf:mc:conv:as}

