% !TEX root = ../main.tex

\section{Monte Carlo}
\label{sec:inf:mc}

Monte Carlo~\citep{metropolis1949monte} is the characterization of a probability distribution 
through random sampling. It is the foundation for a huge array of methods for numerical 
integration, optimization, and scientific simulation; forming the underlying principle 
for all stochastic computation.
\mc provides us with a means of dealing with complex models and problems in a
statistically principled manner.  Without it, one would have to resort to deterministic
approximation methods whenever the target problem is too complex to permit an analytic
solution.  As we will show, it is a highly composable framework that will allow the output
of one system to be input directly to another.  For example, the \mc samples from a joint
distribution will also have the correct marginal distribution over any of its individual components,
while sampling from the marginal distribution then sampling from the conditional distribution
given these samples, will give samples distributed according to the joint.  As \mc
will be key to most methods for Bayesian inference that we will consider, we take the time
in this section to introduce Monte Carlo at a foundational level.

The most common usage of \mc in this work will be the \mc estimation of expectations, 
sometimes known as \mc integration.  
The critical importance of \mc estimation stems from the fact that most of the example
target tasks laid out in~\ref{sec:inf:challenge:post} can be formulated as expectations.
Even when our intention is simply to generate samples from a target distribution, we can
usually think of this as being an implicit expectation of an, as yet unknown, target function.
Here our implicit aim is to minimize the bias and variance of whatever process the samples are eventually
used for, even if that process is simply visual inspection.  

Consider the problem of calculating the expectation of some function
$f(\theta)$ under the distribution $\theta\sim \pi(\theta)$ ($= p(\theta | \mathcal{D})$ for the Bayesian
inference case), which we will denote 
as
\begin{align}
	\label{eq:inf:expt}
I:=\E_{\pi(\theta)} \left[f(\theta)\right]=\int f(\theta) \pi(\theta) d\theta.
\end{align}
This can be approximated using the \mc estimator $I_N$ where
\begin{align}
	\label{eq:inf:mc-est}
	I \approx I_N := \frac{1}{N} \sum_{n=1}^{N}f(\hat{\theta}_n)
	\quad \text{and} \quad \hat{\theta}_n \sim \pi(\theta).
\end{align}
The first result we note is that~\eqref{eq:inf:mc-est} is an \emph{unbiased} estimator for $I$, i.e. we have
\begin{align}
\label{eq:inf:unbiased}
\E \left[I_N\right] = \E \left[\frac{1}{N} \sum_{n=1}^{N}f(\hat{\theta}_n)\right]
= \frac{1}{N} \sum_{n=1}^{N} \E \left[f(\hat{\theta}_n)\right]
= \frac{1}{N} \sum_{n=1}^{N} \E \left[f(\hat{\theta}_1)\right]
= I
\end{align}
where we have first moved the sum outside of expectation using
linearity,\footnote{Note that this presumes that $N$ is independent
	of the samples.  This is usually the case, but care is necessary in some situations, namely when
	the number of samples taken is adaptively chosen based on the sample values, for example in
	adaptive stratified sampling~\citep{etore2010adaptive}.}
then the fact that each $\hat{\theta}_n$ is identically distributed to note that
each $\E \left[f(\hat{\theta}_n)\right]= \E \left[f(\hat{\theta}_1)\right]$, and finally
that $\E \left[f(\hat{\theta}_1)\right] = I$ by the definition of $I$ and the distribution
on $\hth_1$.  This is an important result as it means that \mc does not introduce
any systematic error, i.e. bias, into the approximation: in expectation, it does not
pathologically overestimate or underestimate the target.  This is not to say though that it is
equally likely to overestimate or underestimate as it may, for example, typically underestimate
by a small amount and then rarely overestimate by a large amount.  Instead, it means that if we
were to repeat the estimation an infinite number of times and average the results, we would
get the true value of $I$.  This now hints at another important question -- do we also
recover the true value of $I$ when we conduct one infinitely large estimation, namely if we
take $N\rightarrow\infty$?  This is known as \emph{consistency} of a statistical estimator,
which we will now consider next.  

Before moving
on, we make the important note that many common \mc inference methods, for example MCMC, are in fact biased.  
This is because it is often not possible to independently sample $\hth_n \sim \pi(\theta)$
exactly as we have assumed in~\eqref{eq:inf:mc-est}, with the bias resulting from
the approximation.  The convergence of such methods relies on the bias 
diminishing to $0$ as $N\rightarrow\infty$, such that they remain unbiased in the limit.

\subsection{The Law of Large Numbers}
\label{sec:inf:mc:law}

A key mathematical idea underpinning the convergence of many Monte Carlo methods is the 
law of large numbers (LLN).  Informally, the LLN states that the empirical average of 
independent and identically distributed (i.i.d.)  random variables converges to 
the true expected value of the underlying process as the number of samples in the
average increases.  We can, therefore, use it to prove the consistency of Monte Carlo estimators
where the samples are drawn independently from the same distribution, as is the case
in for example rejection sampling and importance sampling.  The high-level idea for the LLN can be shown by
considering the  \emph{mean squared error} of a \mc estimator as 
follows
%\footnote{Note here that we use the notation of expectation typically used within the statistics literature,
%whereby it corresponds to the expectation over all randomness contained in the system.}
\begin{align}
\E &\left[(I_N-I)^2\right] = \E\left[\left(\frac{1}{N}\sum_{n=1}^{N}f(\hth_n) - I\right)^2\right]
= \frac{1}{N^2}\E\left[\left(\sum_{n=1}^{N} \left(f(\hth_n) - I\right)\right)^2\right] \nonumber \\
&= \frac{1}{N^2}\sum_{n=1}^{N} \E\left[ \left(f(\hth_n)-I\right)^2\right] + 
\frac{1}{N^2}\sum_{n=1}^{N}\sum_{m=1,m\neq n}^{N} \E\left[ (f(\hth_n)-I)(f(\hth_m)-I)\right] \nonumber \\
&= \frac{1}{N^2}\sum_{n=1}^{N} \E\left[ \left(f(\hth_1)-I\right)^2\right] + 
\frac{1}{N^2}\sum_{n=1}^{N}\sum_{m=1,m\neq n}^{N} \cancelto{0}{\left(\E\left[(f(\hth_1)-I)\right]\right)^2} \nonumber \\
&= \frac{\sigma_{\theta}^2}{N}  \quad \text{where} \quad \sigma_{\theta}^2 := \E\left[ \left(f(\hth_1)-I\right)^2\right]
= \var \left[f(\theta)\right].\label{eq:inf:LLN-informal}
\end{align}
Here the second line follows from the first simply by expanding the square and using linearity
to move the sum outside of the expectation as in the unbiasedness derivation.
The first term in the third line follows from the equivalent term in the second line by again noting that
each $\hth_n$ has the same distribution.  The second term in the third line
follows from the assumption that the samples are drawn independently such that
\[
\E\left[ (f(\hth_n)-I)(f(\hth_m)-I)\right] = \E\left[ (f(\hth_n)-I)\right] \E\left[(f(\hth_m)-I)\right]=0.
\]
by unbiasedness of the estimator. The last line simply notes that $\E\left[ \left(f(\hth_1)-I\right)^2\right]$ is a constant,
namely the variance of $f(\theta)$.
  Our final result has a simple and intuitive form -- the mean squared error for
our estimator using $N$ samples is $1/N$ times the mean squared error of an estimator that only uses
a single sample, which is itself equal to the variance of $f(\theta)$.  As $N\rightarrow\infty$, we thus
have that our expected error goes to $0$.

A key upshot of this result is that the difference between our empirical estimate and the true value (i.e. $I_N-I$)
 should be of order $O(1/\sqrt{N})$.  In some way this is rather slow: deterministic numerical
integration schemes often have much faster theoretical convergence rates.  For example,
Simpson's rule has a convergence rate of $O(1/N^4)$ for one-dimensional functions~\citep[Chapter 7]{owen2013mc}.  
As such, \mc is often an inferior way of estimating integrals for smooth functions
in low dimensions.  However, these deterministic numerical integration schemes
require smoothness assumptions on $f$ and, more critically, their convergence rates diminish rapidly (typically
exponentially quickly) with the dimensionality.   
By comparison, the dimensionality only effects the \mc convergence rate through changes in the 
constant factor $\sigma_{\theta}$
and though this will typically increase with the dimensionality, this 
scaling will usually be substantially more graceful than deterministic numerical methods.
%generally do so substantially less aggressively
%then typical exponential decrease in converge rate of deterministic numerical integration schemes
%with dimension.  
%\mc therefore tends to dominate for moderate to high dimensional problems.

\subsection{Convergence of Random Variables}
\label{sec:inf:mc:conv}

To introduce the concept the LLN more precisely, we now consider some more formal notations
of convergence of random variables.  There will be times (e.g. Chapter~\ref{chp:nest})
when mathematical rigor will require us to distinguish between alternative notations of convergence.
However, those less interested in theoretical details may wish to
skip Sections~\ref{sec:inf:mc:conv:prob},~\ref{sec:inf:mc:conv:as}, and~\ref{sec:inf:mc:conv:dist}
on first reading, as the notion of $L^p$ convergence will be sufficient, for most practical purposes,
to guarantee that an estimator will return the correct answer if provided 
with sufficient samples. 

\subsubsection{$L^p$-Convergence}
\label{sec:inf:mc:conv:Lr}

We start by introducing the notion of $L^p$-convergence, also known as convergence in expectation,
as this is the type of convergence we have just alluded to in our informal proof of the LLN.
At a high level, $L^p$-convergence means that the expected value of the related error metric
tends to zero as $N\rightarrow \infty$.  More precisely, we first define the $L^p$-norm for
a random variable $X$ as
\begin{align}
\label{eq:inf:Lp-norm}
\norm{X}_p = \left(\E \left[\left|X\right|^{p}\right]\right)^{\frac{1}{p}}
\end{align}
where $\left|\cdot\right|$ denotes the absolute value.  For example, we can write the
mean squared error used in~\eqref{eq:inf:LLN-informal} as the squared $L^2$-norm:
$\E \left[(I_N-I)^2\right] = \norm{I_N-I}_2^2$.  We further define the notion of $L^p$-space
as being the space of random variables for which $\norm{X}_p < \infty$.  We can now
formally define $L^p$-convergence as follow.
\begin{definition}[$L^p$-convergence]
A sequence of random variables $X_N$ converges in its $L^p$-norm to 
$X$ (where $p\ge1$) if $X\in L^p$, each $X_N \in L^p$, and
\begin{align}
\lim\limits_{N\rightarrow\infty} \norm{X_N-X}_p=0. \label{eq:inf:Lp-conv-formal}
\end{align}
\end{definition}
\noindent A key point to note is that $\Vert X_N-X\rVert_p\ge0 \; \forall X_N, X$ by definition of the $L^p$-norm and so
rather than this simply being a statement of asymptotic unbiasedness, $L^p$-convergence says that the expected
\emph{magnitude} of the error tends to zero as $N\rightarrow\infty$.
Different values of $p$ correspond to different metrics for the error, with larger values of
$p$ constituting stronger converge guarantees, such that $L^{p_2}$-convergence implies
$L^{p_1}$-convergence whenever $p_2>p_1$.  Similarly, if a random variable satisfies
$X \in L^{p_2}$, then it follows that $X \in L^{p_1}$.

%
%\subsubsection{Convergence in Distribution}
%\label{sec:inf:mc:conv:dist}
%
%Convergence in distribution is a weaker form of convergence than is implied by all the
%other forms of convergence that we will discuss.

\subsubsection{Convergence in Probability}
\label{sec:inf:mc:conv:prob}

At a high level, convergence in probability between two random variables (or between a random variable and
a constant) means that they become arbitrarily close to one another.  More
formally we have the following definition.
\begin{definition}[Convergence in probability]
A sequence of random variables $X_N$ converges in probability to $X$ if, for every $\varepsilon>0$,
\begin{align}
\lim\limits_{N\rightarrow\infty} P(\left|X_N-X\right|\ge\varepsilon)=0.
\end{align}
\end{definition}
\noindent As $\varepsilon$ can be made arbitrarily small, this ensures that $X_N$ becomes arbitrarily
close to $X$ in the limit of large $N$.  Estimators are \emph{consistent} if they converge
in probability.

In~\eqref{eq:inf:LLN-informal} we demonstrated the $L^2$ convergence of the Monte Carlo
estimator as we have that 
\[
\lim\limits_{N\rightarrow\infty} \norm{I_N-I}_2=\lim\limits_{N\rightarrow\infty} \frac{\sigma_\theta}{\sqrt{N}} = 0.
\]
Convergence in probability is, in general, a weaker form of convergence than $L^p$ convergence
as $L^p$ convergence implies convergence in probability~\citep{williams1991probability}.
We therefore also have the Monte Carlo estimator convergences in
probability to its expectation.  This is known as the
\emph{weak law of large numbers}, which we can also prove more explicitly as follows
\begin{theorem}[Weak law of large numbers]
	\label{the:inf:weak-law}
If $I$ and $I_N$ are defined as per~\eqref{eq:inf:expt} and~\eqref{eq:inf:mc-est} respectively,
$I\in L^1$, each $I_N \in L^1$,
 and each $\hth_n$ in~\eqref{eq:inf:mc-est} is drawn independently, then $I_N$ converges to $I$
in probability:
	$\lim\limits_{N\rightarrow\infty} P(\left|I_N-I\right|\ge \varepsilon)=0 \quad \forall \varepsilon>0$.
\end{theorem}
\begin{proof}
In the interest of exposition, we prove the result in the case where the stronger 
assumptions that $I\in L^2$ and each $I_N \in L^2$ hold.  In practice this is not needed as
the theorem can be proved by other means, see for example~\cite[Theorem 2.2.7]{durrett2010probability}.

By~\eqref{eq:inf:unbiased} we have that $\E [I_N]=I$ and by~\eqref{eq:inf:LLN-informal} we have that 
$\norm{I_N-I}_2^2 = \frac{\sigma_{\theta}^2}{N}$
where $\sigma_{\theta}^2$ is an unknown, but finite, constant by the assumption that $I\in L^2$.
Chebyshev's inequality states that if $\E [I_N] = I$, then for any $k>0$
\[
P(\left|I_N-I\right|\ge\varepsilon)\le\frac{\var(I_N)}{\varepsilon^2}.
\]
By further noting that as $I_N$ is unbiased, we have that $\var(I_N) = \norm{I_N-I}_2^2$
and therefore
\[
\lim\limits_{N\rightarrow\infty} P(\left|I_N-I\right|\ge\varepsilon)
\le\lim\limits_{N\rightarrow\infty} \frac{\sigma_{\theta}^2}{\varepsilon^2 N}
=0 \quad \forall \varepsilon>0.
\]
\end{proof}
%\noindent As an interesting aside, we can prove that convergence in probability follows from
%$L^p$ convergence more generally in a similar fashion.  This can be done
%by using the more general form of Chebyshev's inequality given in, for example,
%Theorem 1.6.4 and Lemma 2.2.2 of~\cite{durrett2010probability} to show that
%\begin{align}
%\label{eq:inf:prob-from-lp}
%\begin{split}
%\lim\limits_{N\rightarrow\infty} P(\left|I_N-I\right|\ge\varepsilon)
%&\le \lim\limits_{N\rightarrow\infty} \frac{\E \left[\left|I_N-I\right|^{p}\right]}{\varepsilon^p} = 
%\left(\frac{1}{\varepsilon}\lim\limits_{N\rightarrow\infty} \norm{I_N-I}_p\right)^p.
%\end{split}
%\end{align}
%If $I_N \Lp I$ then $\lim\limits_{N\rightarrow\infty} \norm{I_N-I}_p = 0$ by definition
%and we have convergence in probability as required.

\subsubsection{Almost Sure Convergence}
\label{sec:inf:mc:conv:as}

Almost sure convergence, also known as strong convergence, is a similar, but stronger, form of convergence than convergence in
probability.  At a high level, the difference between
convergence in probability and almost sure converge is a difference in the tail behavior:
convergence in probability suggests the rate at which an event happens tends to zero; almost
sure convergence means that there is some point in time after which the event never happens
again.  More formally we have the following definition
\begin{definition}[Almost sure convergence]
A sequence of random variables $X_N$ converges almost surely to $X$ if
\begin{align}
	P\left(\lim\limits_{N\rightarrow\infty} X_N=X\right)=1.
\end{align}
\end{definition}
\noindent Almost sure convergence implies convergence in probability, but not vice-versa -- the rate
at which events occur might tend to zero without there ever being a point at which the
event never occurs again.  It does not imply, nor is it implied by, $L^p$ convergence.
The \emph{strong law of large numbers} is the dual for the weak law of large numbers as
follows.
\begin{theorem}[Strong law of large numbers]
	Assuming the setup of Theorem~\ref{the:inf:weak-law} then $I_N$ converges to $I$ almost surely
	\begin{align}
	P\left(\lim\limits_{N\rightarrow\infty} I_N=I\right)=1.
	\end{align}
\end{theorem}
The proof is somewhat more complicated than the weak law and so is not provided here, but can
be found in, for example,~\cite[Theorem 2.4.1]{durrett2010probability}.

\subsubsection{Convergence in Distribution}
\label{sec:inf:mc:conv:dist}

At a  high level, convergence in distribution, also known as weak convergence, states that a sequence of variables become
increasingly closely distributed to a target distribution.  Whereas our previous notions of
convergence ensure that our sequence of random variables converge to a particular value, 
convergence in distribution only implies that our sequence of random variables tends towards
having a particular distribution.  For example, a variable might converge in distribution to
having a unit normal distribution, whereas a different variable might converge in probability
to zero. More formally, we can define convergence in distribution as follows.
\begin{definition}[Convergence in distribution]
	A sequence of random variables $X_N$ converges in distribution $X$  if
	\begin{align}
		\lim\limits_{N\rightarrow\infty} P(X_N \le x) = P(X \le x)
	\end{align}
	for every $x$ at which $P(X\le x)$ is continuous, where $P(X_N \le x) $ and $P(X \le x)$
	are the cumulative distribution functions for $X_N$ and $X$ respectively.
\end{definition}
\noindent Because it only requires that a variable has a particular distribution, rather than an
 exact value, convergence in distribution is a weaker form of convergence than those previously
 discussed and is implied by any of the other forms of convergence.  
\vspace{5pt}

\subsection{The Central Limit Theorem}
\label{sec:inf:mc:clt}

The central limit theorem (CLT) is a core result in the study of \mc methods.  In its simplest form,
it states that the empirical mean of $N$ i.i.d. random variables tends towards a Gaussian in the limit $N\to\infty$.
While the LLN demonstrated the convergence of the average of i.i.d. random variables towards
the true mean, the CLT provides a means of constructing consistent confidence intervals of our estimate
$I_N$ by exploiting its asymptotic normality.  Furthermore, it has variants that do not
require that the variables are i.i.d., meaning we can use it do demonstrate convergence in scenarios
where samples are correlated, such as occurs when doing MCMC inference as shown in
Section~\ref{sec:inf:foundation:mcmc}.
In the i.i.d.~case, the CLT is as follows.
\begin{theorem}[Central Limit Theorem]
	\label{the:inf:clt}
Assume $X_1,\dots,X_N$ is a sequence of i.i.d. random variables with 
$\E [X_i] = I$ and $\E[X_i^2]=\sigma<\infty$ for all  $i \in \{1,\dots,N\}$. Let 
$I_N := \frac{1}{N} \sum_{n=1}^{N} X_n$ be the
sample average of this sequence.  Then $\sqrt{N}\left(I_N-I\right)/\sigma$ converges
in distribution to the unit normal:
\begin{align}
\lim\limits_{N\rightarrow\infty} P\left(\frac{\sqrt{N}\left(I_N-I\right)}{\sigma} \le z\right)
=\Phi(z)= \int_{-\infty}^{z} \frac{1}{2\pi} \exp\left(-\frac{\zeta^2}{2}\right) d\zeta \quad \forall z\in\real
\end{align}
where $\Phi(z)$ is the cumulative distribution function for the unit normal.
Furthermore, if $\sigma_N^2 = \frac{1}{N-1} \sum_{n=1}^{N} \left(X_N-I_N\right)^2$ is
the empirical estimate of the variance of $X_N$ (including Bessel's correction), then
$\sqrt{N}\left(I_N-I\right)/\sigma_N$ also converges
in distribution to the unit normal.
\end{theorem}
\begin{proof}
	See, for example,~\cite[Chapter 3]{durrett2010probability}.
\end{proof}
\noindent The second result here is especially key as it will allow us to calculate consistent confidence intervals
on our estimates without needing to know $\sigma$ as we can instead use our empirical estimate
\begin{align}
	\label{eq:inf:emprical-var}
	\sigma_N^2 = \frac{1}{N-1} \sum_{n=1}^{N} \left(X_N-I_N\right)^2 \quad \mathrm{where} 
	\quad I_N = \frac{1}{N}\sum_{n=1}^{N}X_N.
\end{align}
The form of~\eqref{eq:inf:emprical-var} might at first look at bit strange - why is the normalizing
term $1/(N-1)$ instead of $1/N$?  Although replacing the $1/(N-1)$ term with $1/N$ would still lead
to a consistent estimator, this estimator turns out to be biased whereas~\eqref{eq:inf:emprical-var}
is in fact unbiased.  The bias correction $N/(N-1)$ is known as Bessel's correction, while the proof
that~\eqref{eq:inf:emprical-var} is unbiased follows from straightforward algebraic manipulations.
Note though that the estimator of the standard deviation, $\sigma_N$, is still biased by Jensen's
inequality.
As a consequence of the unbiasedness, $\sigma_N^2 \asto \sigma^2$ by the strong LLN, 
from which the second result
in Theorem~\ref{the:inf:clt} follows from the first by Slutsky's theorem.

Imagine we want to construct a confidence interval on our estimate $I_N$ such that there is
a probability $0<\alpha<1$ that $I$ is in the range $[I_N-\varepsilon,I_N+\varepsilon]$.  Using
the CLT we can do this as follows
\begin{align}
	\label{eq:inf:conf-int}
	\alpha :&= P(I_N-\varepsilon \le I \le I_N+\varepsilon ) = P(-\varepsilon \le I_N-I \le \varepsilon) \nonumber\\
	&= P\left(-\frac{\sqrt{N}\varepsilon}{\sigma_N}\le \frac{\sqrt{N}\left(I_N-I\right)}{\sigma_N}
				\le\frac{\sqrt{N}\varepsilon}{\sigma_N}\right) \nonumber\\
	&\approx \Phi\left(\frac{\sqrt{N}\varepsilon}{\sigma_N}\right)-
					\Phi\left(-\frac{\sqrt{N}\varepsilon}{\sigma_N}\right) = 2\Phi\left(\frac{\sqrt{N}\varepsilon}{\sigma_N}\right)-1.
\end{align}
Rearranging for $\epsilon$ we have that 
$\varepsilon = \left(\sigma_N/\sqrt{N}\right)\Phi^{-1}\left(\frac{\alpha+1}{2}\right)$
and therefore the confidence interval
\begin{align}
P\left(I_N-\frac{\sigma_N\Phi^{-1}\left(\frac{\alpha+1}{2}\right)}{\sqrt{N}} 
	\le I \le I_N+\frac{\sigma_N\Phi^{-1}\left(\frac{\alpha+1}{2}\right)}{\sqrt{N}} \right)  = \alpha.
\end{align}
Thus for example, if we set $\alpha=0.99$ then $\Phi^{-1}\left(\frac{\alpha+1}{2}\right)\approx 2.576$
and our confidence interval $99\%$ confidence interval for $I$ is
 $I_N \pm \frac{2.576 \sigma_N}{\sqrt{N}}$.  We reiterate that these confidence intervals are exact
 in the limit $N\to\infty$, but approximate for finite $N$.  As one is often far from the asymptotic
 regime, care is often required in interpreting them.  Nonetheless, the ability to
 estimate realistic uncertainties for sufficiently large $N$ is an extremely powerful result.
 
Through most of this section, we have assumed that our random variables are i.i.d..
In fact, neither the assumption of being identically distributed nor that of independence is actually
necessary for the CLT to hold.  One can instead use the concept of \emph{strong mixing}, namely
that variables sufficiently far apart in the sequence are independent, to generalize beyond the i.i.d.
setting~\citep{jones2004markov}.  This is critical for the numerous \mc inference schemes, 
such as MCMC methods, that
do not produce independent samples but instead rely on the samples converging in distribution
to a target distribution.  In most MCMC settings, one can also prove a CLT using reversibility of
the Markov chain~\citep{kipnis1986central}.
It is beyond the scope of this thesis to go into these more general forms of the CLT
in depth, so we simply note their critical importance and refer the reader to~\cite{durrett2010probability}
for a comprehensive introduction.