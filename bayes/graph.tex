% !TEX root = ../main.tex

\section{Graphical Models}
\label{sec:bayes:paradigm:graph}

Generative models will typically have many variables and a complex \emph{dependency structure}.
In other words, many of variables will be conditionally independent of one another given values for
other variables.  Graphical models are a ubiquitously used method for representing and reasoning
about generative models with a particular focus on the dependency structure.  At a high-level, they
capture how the joint probability distribution can be broken down into a product of different factors, 
each defined over
a subset of the variables.  They are formed of a number of connected nodes, where each node
represents a random variable in the model and each variable has its own node.  Links between nodes in
the model represent dependencies: any two connected nodes have an explicit dependency.
Various independence assumptions can be deduced from the graphical model, though the exact nature
of these deductions will depend or the type of graphical model -- nodes without direct links
between them will often still be dependent.

Graphical models can be seperated into two distinct classes: directed graphical
models and undirected graphical models.  Undirected graphical models, also known as Markov random
fields, imply no ordering on their factorization and are used only to express conditional independences
between variables.  They are used in scenarios where it is difficult to specify the target distribution in a
structured generative way.  For example, if modeling whether it will rain at various locations then there
is a clear dependence between nearby locations, but not a natural ordering to the joint probability
distribution of where it will rain.  Boltzmann machines are a common example of undirected graphical
models~\citep{ackley1985learning}.  Independence in undirected graphical models can be deduced
through the \emph{global Markov property} which states that any two non-intersecting subsets of the 
variables $A$ and $B$ are
conditionally independent given a third, separating, subset $C$ if there is no path between $A$ and
$B$ that does not pass through $C$.  This means, for example, that each variable is conditionally
independent of all the other variables given its neighbors.

Our main focus, though, will instead be on directed graphical models and in particular directed acyclic 
graphical models (DAGs), i.e. directed graphical models containing no cycles or loops one can follow 
and arrive back in the starting position.  DAGs, also known as Bayesian networks, are particularly
useful in the context of Bayesian modeling because they can be used to express \emph{casual relationships}.
As such, they can be used as a piecewise explanation for how the joint distribution is generated.
Not only does this form a natural means to describe and design models in the first place, 
we can carefully order the breakdown to factorize the distribution into only terms we know.  For example,
in the linear regression model we did not know (at least when the model was first defined) 
$p(\mathcal{D})$ but we did know $p(\mathbf{w})$ and $p(\mathcal{D} | \mathbf{w})$.  Therefore even
though we could factorize our joint $p(\mathcal{D}, \mathbf{w})$ as 
$p(\mathbf{w} | \mathcal{D})p(\mathcal{D})$ and construct a DAG this way, it is much more convenient
to factorize and construct the DAG the other way round, namely as 
$p(\mathcal{D} | \mathbf{w})p(\mathbf{w})$.
As a good rule of thumb, when we define a model using a DAG, we need to be able to define the 
probability of each variable given its \emph{parents}, i.e. all the nodes with links from that node
towards the node the question.  We will generally not have access to also possible factorizations
in an analytic form as otherwise there would be no need to perform inference.

The demonstrate this factorization more explicitly and give a concrete example of a DAG, consider a joint model
$p(a,b,c)$.  By the product rule, we can break down this joint distribution in to a number of different
factorizations.  However, some will typically be more useful then others.  Imagine a medical diagnostic
example where $a$ represents lifestyle and genetic factors of a patient such as whether they smoke
or have unknown preexisting conditions, 
$b$ represents the presence of lung cancer, and $c$ represents symptoms 
such as a persistent cough.  Here we have the following natural breakdown of the joint distribution
\begin{align}
\label{eq:bayes:example-graph}
p(a,b,c) = p(a) p(b|a) p(c|a,b).
\end{align}
The lifestyle and genetic factors will generally either be known or can be estimated from tests or
simply prevalence within the population.  These factors, the likelihood of somebody developing
lung cancer can be modeled using existing data and domain expertise.   Given the lifestyle and
genetic factors and the knowledge of whether lung cancer is present, we can predict the likelihood
of observing the observed symptoms.  We can express our model and this factorization using
the DAG shown in Figure INSERT.\todo{Add DAG}
  Here we have shaded in $c$ to express the fact that this is
observed.  The graphical model expresses our dependency structure as we have the probability
of each node given its parents.  As shown in~\eqref{eq:bayes:example-graph}, the product of
all these factors is equal to our joint distribution.  The DAG has thus formed a convenient means
of representing our generative process.

Clearly our aim for this problem will be to find the marginal probability $p(b|c)$.  To calculate this
we will need to perform Bayesian inference as explained.
An importance feature of the breakdown of graphical models will become apparent when we
consider how we can conduct Bayesian inference more a general class of 
models where the solution is no analytic in Chapter~\ref{chp:inf}.  Here knowing the dependency
structure, and in particular the independence relationships, will be essential to many inference 
schemes such as sequential Monte Carlo and message
passing schemes.\todo{Add refs to relevant sections}

A natural question is now how can we deduce the independence relationships from DAG?
This can be done by introducing the notion of \emph{d-separation}~\citep{pearl2014probabilistic}.
Consider three arbitrary, non-intersecting, subsets of our DAG $A$, $B$, and $C$.  $A$ and $B$
are conditionally independent given $C$ if there is no \emph{unblocked} paths from $A$ to $B$
(or equivalently from $B$ to $A$), in which case $A$ is said to be d-separated from $B$ by $C$.  
Paths do not need to be in the directions defined by the DAG but are blocked if either
\begin{enumerate}
	\item Two consecutive arrows in the path both point towards a node that is not $C$ and
	has no descendants in $C$, i.e. we cannot get to any of nodes in $C$ by following the arrows
	from this node.
	\item Two consecutive arrows in the path meet at a node in $C$ and one of them
	points away from the node.
\end{enumerate}
Examples of blocked paths are shown in Figure INSERT while examples of unblocked paths
are shown in Figure INSERT, explanations for which are shown in the captions.\todo{Add Figures and captions}
For a more comprehensive introduction to establishing independence in DAGs we
refer the reader to Section 8.2 of~\cite{bishop2006pattern}.

In the simple example of Figure INSERT there were no independence relationships and so
we gain little from working with the DAG compared to just the joint probability distribution.
A more advanced example where there are substantial independene relationships which can
be exploited is shown in Figure INSERT.  This model is known as a HMM\dots Markov property
etc \dots

Improving models - addding hyperparameters to the HMM example\dots