% !TEX root = ../main.tex
%
%\begin{savequote}[8cm]
%	\textlatin{Le doute n'est pas une état bien agréable, mais l'assurance est un état ridicule.}
%	
%	Doubt is not a very agreeable status, but certainty is a ridiculous one.
%	\qauthor{--- Voltaire}
%\end{savequote}


\chapter{Probabilistic Machine Learning}
\label{chp:bayes}

In this chapter we will provide a high-level introduction to  some of the core approaches to
machine learning.  We will distinguish between discriminative and generative approaches,
outlining some of the key features that indicate when problems are more suited to one approach
or the other.  Our focus then settles on probabilistic generative approaches, which
will be the main focus of this thesis.  We will explain how the \emph{Bayesian paradigm} provides
a powerful framework for generative machine learning that allows us to combine data with existing
expertise.  We will then go on to show how \emph{graphical models} can be used as a convenient
framework to express Bayesian models and extract important features.  
We continue by introducing one of the main competitors to the Bayesian approach -- frequentist
modeling -- and present arguments for why neither a Bayesian nor a frequentist approach is
entirely satisfactory.  In particular, we will carefully outline the, oft forgotten, fundamental underlying
assumptions made by each approach and explain why the differing suitability of these
assumptions to different tasks means that both are  essential tools in the machine learning
arsenal.
Though the focus of this thesis will be on Bayesian approaches, understanding its limitations
is essential for understanding when the methods we discuss should be used and critically when they
should definitely not be.  We finish the chapter by discussing some of the key practical challenges for Bayesian modeling and outline how we hope to address these in the present
work.

\input{bayes/discrim.tex}
\input{bayes/paradigm.tex}
\input{bayes/graph.tex}
\input{bayes/religion.tex}


\section{Challenges of the Bayesian Approach}
\label{sec:bayes:challenges}

Di Finetti and i.i.d. assumption of data.

More data means more likely the model is wrong.