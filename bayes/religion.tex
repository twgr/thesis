% !TEX root = ../main.tex

\section{Frequentism vs Bayesianism}
\label{sec:bayes:religions}

We have just introduced the Bayesian approach to generative modeling, but this is far from
the only possible approach.  In this section, we will briefly introduce and compare the alternative, \emph{frequentist},
approach.
We have a come a long way from the absolutism of Feller~\citep{feller1950introduction,gelman2013not}, but there is still
often a substantial, and sometimes acrimonious, divide between those
who advocate Bayesian or frequentist methods within the statistics and machine learning communities.  Many researchers
have very strong views one way or another in the debate and it is easy, certainly as a graduate student,
to be sufficiently isolated on one side of the divide to completely detract oneself from the considerations of the
other.  
%It sometimes feels that the beliefs of some advocates of each side are so deep-rooted and 
%fundamental that this almost a religious, rather than scientific divide.  
Given the fierceness of the debate,
the actual statistical differences between the approaches (rather than the well-known philosophical differences we touched
on in Section~\ref{sec:prob:prob} which are often dubiously used for justification) are often surprisingly poorly understood
by those in the early stages of their research careers.
Our aim in this section is not to advocate the use of one approach over the other, but to (hopefully objectively)
highlight these statistical differences and demonstrate that \emph{both} approaches have drawbacks, such that ``head in the sand''
mentalities either way can be highly detrimental.  
We note that whereas Bayesian methods are always, at least in theory, generative~\citep[Section~14.1]{gelman2014bayesian},
frequentist methods can be either generative or discriminative. 
As we have already discussed differences between generative and
discriminative modeling in Section~\ref{sec:bayes:discrim} we will omit this difference from our subsequent discussion.

At their root, the difference between Bayesian and frequentist methods stem from distinct fundamental
assumptions: frequentist modeling presumes fixed parameters, Bayesian modeling assumes fixed data~\citep{jordan2009you}.  In many ways,
both of these assumptions are somewhat dubious. Why assume fixed parameters when we do not have enough
information about from the data to be certain of the correct value?  Why ignore that fact that other data could have 
been generated by the underlying process for the same underlying true parameter?  
%Perhaps this is why people have
%such strong views either way -- the arguments against both methods are very strong, so much so that advocates are
%often, sometimes unintentionally, willing to overlook the shortcomings of their favored approach.  

To elucidate the
different assumptions further and start looking into why they are made, we will now step into a decision-theoretic
framework.  Let's presume that the universe gives us some data $X$ and some true parameter $\theta$, the former of which
we can access but the latter of which is unknown.  We can alternatively think about this as $X$ being some information
that we actually receive and $\theta$ as some underlying truth or oracle from which we could make optimal predictions.  There is
no need for $\theta$ to be some explicit finite parameterization.  Any machine learning approach will take this data as input and
return some artifact or decision, for example, predictions on test data.  Let's call this our action, or decision, $a(X)$
and further presume that for a given $X$, $a(X)$ is deterministic.\footnote{If we allow our predictions to be probability
distributions this assumption is effectively equivalent to assuming we can solve any analysis required by our approach exactly.}
Presuming that our analysis is not frivolous, there will be some loss function $L(a(X),\theta)$ associated with the action we take
and the true parameter $\theta$, even if this loss function is subjective or unknown.  At a high level, our aim is always to
minimize this loss, but what we mean by minimizing the loss changes between the Bayesian and frequentist settings.  

In the
frequentist setting, $X$ is a random variable but $\theta$ is not.  Therefore, one takes an expectation over possible data
that could have been generated, giving the frequentist risk~\cite{vapnik1998statistical}
\begin{align}
\label{eq:bayes:freq-risk}
R(\theta)  = \E\left[L(a(X),\theta) | \theta \right]
\end{align}
which is thus a function of theta.
The frequentist focus is therefore on \emph{repeatability}, i.e.
the generalization of the approach to different datasets that \emph{could} have been generated.
Choosing the parameters $\theta$ is thus based on optimizing for the best average performance over possible datasets,
with common approaches including cross-validation and minimax selection of $\theta$.  In some ways, this is a pessimistic
approach: the emphasis is on repeat performance and ensuring satisfactory performance for repeated use on different possible
datasets.  

In the Bayesian setting, $\theta$ is a random variable but $X$ is not.  Therefore one takes an expectation over $\theta$
to make predictions conditioned on the value of $X$ giving the \emph{posterior expected loss}~\cite{robert2007bayesian}
\begin{align}
\label{eq:bayes:bayes-est}
\varrho(\pi,X) = \E_{\pi(\theta)} [L(a(X),\theta) | X]
\end{align}
where $\pi(\theta)$ is a prior distribution on $\theta$.  Although $\varrho(\pi,X)$ is a function of the data, the Bayesian
approach takes the data as given (after all we have a particular dataset) and so for a given prior, the posterior
expected loss is a fixed value and, unlike in the frequentist case, further analysis is not required: the posterior
conveys all the information we need.  The focus of the Bayesian approach is on generalizing over all the possible values of
the parameters and using all the information at hand.  
This can be quite an optimistic approach as it does not consider the fact that, even for a given $\theta$,
other datasets \emph{might} have been generated.

We now introduce some shortfalls that originate from making these assumptions.  We emphasize though that we are only
scratching the surface of one of the most hotly debated issues in statistics and do not even come close to doing
the topic justice.  Our aim is thus less to provide an comprehensive explanation of the relative merits of the two approaches, but
more to make the reader aware that there are a vast array of complicated, and sometimes controversial,
issues associated with whether to use a Bayesian or
frequentist approach, most of which have no simple objective conclusion.


\subsection{Shortcomings of the Frequentist Approach}
\label{sec:bayes:religion:freq}

One of the key criticisms of the frequentist approach is that predictions depend on the experimental procedure and
can violate the \emph{likelihood principle}.  The likelihood principle states that, for a given model, 
the only information relevant
to the parameters $\theta$ is conveyed by the data is encoded through the likelihood function~\citep{robert2007bayesian}.  
In other words, the same data and 
the same model should always lead to the same inferences about $\theta$.  Though this sounds intuitively obvious, it is actually violated by
taking an expectation of $X$ in frequentist methods, as this introduces a dependency from the experimental procedure, which
is ancillary information to the data and the likelihood function.  

As a classic example, 
imagine that our data from flipping a coin is $3$ heads and $9$ tails.
In a frequentist setting we make different inferences about whether the coin is biased 
depending on whether our data originated from flipping the coin $12$ times and counting the number of heads, or if we 
flipped the coin until we got $3$ heads.  For example, at the $5\%$ level of a significance test we can reject the null
hypothesis that the coin is unbiased in the latter case, but not the former.  This is obviously somewhat problematic, but
can be used to argue both against frequentist methods and against the likelihood principle (and thus Bayesian methods).  Using it
to argue against frequentist methods, and in particular significance tests, is quite straightforward: the subjective
differences in our experiment should not affect our conclusions about whether the coin is fair or not.  We can also take things
further and make the results change for absurd reasons.  For example, imagine our experimenter had intended to flip until she got
$3$ heads but was then attacked and killed by a bear while the twelfth flip was in the air such that further flips would not
have been possible regardless of the outcome.  In the frequentist setting, this again changes our conclusions
about whether the coin is biased.  Clearly, it is ridiculous that the occurrence or lack of a bear attack during the experiment
should change our inferences, but that is need-to-know information for frequentist approaches, particularly if using p-values.

As we previously suggested though, one can also use this example to argue for frequentist methods because it actually
suggests the likelihood principle is incorrect.  Although p-values are a terribly abused tool whose misuse has had
severe detrimental impact on many applied communities~\citep{goodman1999toward,ioannidis2005most}, they are not incorrect if 
interpreted correctly
and are an extremely important part of the statistical arsenal when used appropriately.  If one very carefully considers
the definition of a p-value as being the probability that the observed event or one more extreme is observed if the
\emph{experiment is repeated}, we see that our bear attack does actually affect the outcome.  The chance of getting the same
or more extreme data from repeating the experiment of ``\textit{flip the coin until you get $3$ heads}'' is different to 
the chance of getting the same or a more extreme result from repeating the experiment 
``\textit{flip the coin until you get $3$ heads or make $12$ flips (at which point you will be killed by a bear)}''.  
As such, one can argue that the apparent absurd
changes in conclusions originate from misinterpreting the results and that, in fact, these changes actually demonstrate
that the likelihood principle is flawed, through showing why the experimental procedure actually matters because otherwise
we have no concept of repeatability.  Imagine instead the more practical scenario where a suspect researcher stops their experiment
early as it looks like the results are likely to support their hypothesis and they do not want to take the risk that if they
keep it running as long as they intended, then the results might no longer be so good.  Here the researcher has clearly
biased their results in a way that ostensibly violates the likelihood principle.

Whichever view you take, two things are relatively indisputable.  Firstly a number of a frequentist concepts, such as p-values,
are not compatible with the likelihood principle.  Secondly, frequentist methods are not always \emph{coherent} such that they can
return answers that are not consistent with each other, e.g. probabilities that do not sum to one.  For example, p-values
for a hypothesis and its complement do not, in general, sum to one.  

Another major criticism of the frequentist approach is that it takes a point estimate for $\theta$, rather than averaging
over different possible parameters.  This can be somewhat inefficient in the finite data case as it limits the information
stored in the model to that stored by the parameters and it is clearly wasteful for a model to make predictions based on
only one possible set of parameters when our parameter estimates are not known to be correct.  Part of the reason that this
is done is to actively avoid placing a prior distribution on the parameters, either because this prior distribution might
be ``wrong''\footnote{Whether a prior can be wrong, or what that even means, is a somewhat open question except in the
	case where it fails to put any probability mass (or density for continuous problems) on the ground truth value of
	the parameters.} or because, at a more philosophical level, it is not a random variable under the frequentist definition
of probability.  Some people thus object to placing a distribution over it at a fundamental level (we will see this objection
mirrored by Bayesians for the data in the next section).  For the Bayesian perspective (and a viewpoint we actively 
argue for elsewhere in the paper), this is itself also a weakness of the frequentist approach as incorporating prior
information is often essential for effective modeling.


\subsection{Shortcomings of the Bayesian Approach}
\label{sec:bayes:religion:bayes}

Unfortunately, the Bayesian approach is also not without its shortcomings.  We have already discussed one
key criticism in the last section in that the Bayesian approach relies on the likelihood principle which itself may not
be sound, or at the very least ill-suited for some statistical modeling problems.  More generally, it can be seen as
foolhardy to not consider other possible datasets that might have been generated.  Taking a very strict stance, then
even checking the performance of a Bayesian method on test data is fundamentally frequentist as we are assessing how
well our model generalizes to other data.  Pure Bayesianism, which is admittedly not usually carried out in practice, shuns
empiricism as this by definition is rooted in the concept of repeated trials which is not possible if the data is kept fixed.
The rationale typically given for
this is that we should use all the information available in the data and by calculating a frequentist risk we are throwing
some of this away.  For example, cross-validation approaches only ever use a subset of the data when training the model.
However, a common key aim of statistics is generalization and repeatability.  Pure Bayesian approaches include no
consideration for \emph{calibration} which means that, even if our likelihood model is correct, there is still no reason
that any probabilities or confidence intervals are correct.  This at odds with frequentist approaches, for which
we can sometimes demonstrate that quoted results are correct under repeated execution of the experiment if certain assumptions
hold true.

A related issue is that Bayesian approaches will often reach
spectacularly different conclusions for ostensibly inconsequential changes between datasets.\footnote{Though this is arguably more
of an issue with generative approaches than Bayesian methods in particular.}
At least when making the standard assumption of i.i.d.~data in Bayesian analysis, then likelihood terms are multiplicative 
and so typically when one adds more data, the relative probability of two parameters quickly diverge.  This divergence is
necessary for Bayesian methods to converge to the correct ground truth parameters for data distributed exactly as per the
model, but it also means any slight misspecifications in the likelihood model
become heavily emphasized very quickly.  As a consequence, Bayesian methods often chronically underestimate uncertainty 
in the parameters for large datasets (and often also surprisingly small datasets), because they do not account for
the unknown unknowns and ostensibly  inconsequential features of the likelihood model
lead to massively different conclusions about the relative probabilities of different parameters.
In terms of the posterior expected loss, this is often not much of a problem as the assumptions might be similarly
inconsequential for predictions.  However, if our aim is actually to learn 
about the parameters then this is quite worrying.  At the very least it shows why we should view posteriors with
a serious degree of skepticism (particularly in their uncertainty estimates), rather than taking them as ground truth.
Though techniques such as cross-validation can reduce sensitivity to model
misspecification, generative frequentist methods still often do not fare much better than Bayesian
methods for misspecified models (after all they produce no uncertainty estimates on $\theta$).
Discriminative methods (which are by proxy frequentist), on the other hand, do not have
an explicit model to specify in the same way and so are far less prone to the same issues.
Therefore, though  much of the criticisms of Bayesian modeling stem
from the use of a prior, issues with model misspecification are often much more severe and predominantly
shared with generative frequentist approaches~\citep{gelman2013not}.  It is, therefore, often the question of discriminative
vs generative machine learning that is most critical~\citep{breiman2001statistical}.

%
%Consider, for example, a binary
%classification problem where, as in common in the Bayesian framework, we will presume
%that our observations $y_{1:N}$ are independent such that our likelihood is a product of 
%likelihood terms for individual datapoints, i.e.
%$p(y_{1:N} | \theta) = \prod_{n=1}^{N} p(y_n | \theta)$.  We now presume that using
%hyperparameter\footnote{Mathematically is no hard distinction between a hyperparameter and a parameter, but the
%	former is often used to refer to ``higher-level'' parameters.  For example, we might have a prior over model parameters, with
%	the parameters of the prior itself referred to as hyperparameters.  Here we are implicitly presuming that there are some latent
%	variables that have been marginalized over to make the setup realistic, hence $\theta$ corresponding to a ``hyperparameter''.}
% $\theta_1$ classifies four billion of the five billion points in the dataset ``correctly''
%while using hyperparameter $\theta_2$, which has the same prior probability,
%classifies four billion and one thousand of data points ``correctly'', where by correct we mean that $p(y_n | \theta) = 0.8$
%(otherwise $p(y_n | \theta) = 0.2$).  We thus have that the difference in classification accuracies of the
%approaches is $0.00002\%$, but under our posterior $\theta_2$ is $(0.8/0.2)^{1000}\approx10^{602}$ times
%more probable than $\theta_1$.  Intuitively this is preposterous and no sensible gambler would ever take odds
%that $\theta_2$ is even twice as likely to give better performance on the a test dataset of the same size.  
%Though somewhat oversimplified, this example is not actually unrepresentative to behavior often experienced.  
%To explain the dichotomy between the theoretical correctness of Bayes' rule and our intuitions, we consider what
%happens if we let the universe regenerate the dataset.  We will draw datapoints independently
%and we presume that the probability of a new datapoint being correctly classified using $\theta_1$ is independent to the probability of
%it being correctly classified by $\theta_2$, and that these probabilities are equal to the empirical misclassification
%rates from our first dataset.  The probability that $\theta_1$ gives a better misclassification rate for the new dataset is
%almost $50\%$, even though the previous posterior suggested that $\theta_2$ was astronomically better than $\theta_1$.
%Furthermore, there is almost a $50\%$ chance that $\theta_1$ will have an equally astronomically higher
%posterior mass for our second dataset!  The problem here is that the Bayesian approach does not taken into 
%account that datasets are usually sampled from a population or possible samples that \emph{could} have been generated.
%Our analysis was perfectly reasonable under the assumption that the our dataset was fixed and absolute, but it
%lead to somewhat preposterous conclusions when we generalized to other possible data.
%In terms of the posterior expected loss, this is not much of a problem
%as for most sensible loss functions this will not substantially change.  However, if our aim is actually to learn 
%about $\theta$ then this is quite worrying.  At the very least it shows why we should view posteriors with
%a degree of skepticism, rather than taking them as ground truth.

Naturally one of the biggest concerns with Bayesian approaches is their use of a prior, with this being
perhaps the biggest difference to generative frequentist approaches.  The prior is typically a double-edged
sword.  On the one hand, it is necessary for combining existing knowledge and information from data 
in a principle manner, on the other hand, priors are inherently subjective and so all produced posteriors are
similarly subjective.  Given the same problem, different practitioners will use different priors and reach
potentially different conclusions.  In the Bayesian framework their is not such thing as incorrect prior 
and so there is equally no such thing as a correct posterior for a given likelihood (presuming finite data).
Consequently, ``all bets are off'' for repeated experimentation with Bayesian methods as their is no
quantification for how wrong our posterior predictive might be compared with the true generating
distribution.  This makes them somewhat inappropriate for tasks where the focus is on repeated use,
e.g. providing reliable confidence intervals for medical trials.  Such cases both predicate a need for
considering that there are many possible datasets that might occur and, ideally, an objective approach
that means the conclusions are independent to the whims of the analyst.

Another issue that comes up with the prior is that it is often easy to end up 
``using the data twice''~\citep{gelman2008objections}.  The Bayesian paradigm requires that the
prior is independent from the data and this means that there should not be any human induced
dependency.  In other words, it is necessary that the user sets their prior before
observing the data they condition on, otherwise the data will both influence their choice of prior
and be incorporated through the likelihood.  This is not a of problem with Bayesian 
methods per se, but it can be a common shortfall in their application.

Another practical issue with Bayesian methods is their computational complexity at both
train time and test time.  Firstly, using Bayesian methods requires one to carry out inference,
which, as we explain in Chapter~\ref{chp:inf}, is typically a challenging and computationally
expensive process, often prohibitively so.  Some frequentist approaches can also
be similarly expensive at train time, but others can be substantially cheaper, particularly
discriminative approaches.  Further, Bayesian methods tend to also be somewhat expensive
for making predictions with the posterior predictive itself being an integral.  Frequentist
methods tend to instead predict using point estimates for $\theta$, such that prediction is typically
much cheaper.

\subsection{Practical usage}
\label{sec:bayes:religion:correct}

Although Bayesianism and frequentism are both exact frameworks, their applications to real problems is not.
Both frameworks have their strengths and weaknesses and so perhaps the key question is not which framework
is correct, but when should we use each.  In
particular, the Bayesian approach is often essential when working with small datasets but where we have substantial
prior expertise.  On the other hand, a frequentist approach is essential to ensuring repeatability for large
datasets with little prior knowledge of note.  Bayesian and frequentist methods are also by no means mutually 
exclusive and effective modeling often requires elements of both
to be used concurrently.  From a theoretical perspective,
 averaging over both data and over parameters, leading to the \emph{Bayes risk}~\citep{robert2007bayesian}.   
For example, one could be Bayesian about the results from a cross-validation test.

We finish by noting a critical assumption made by both generative Bayesian and frequentist methods --
that there is some true underlying value for the parameters.  Because
all models are approximations of the real world, this is often a misguided and harmful assumption.  That this assumption
is made is clear in the frequentist setting, but is somewhat subtle for Bayesian approaches.
Bayesian methods allow for multiple hypothesis or parameter values, but our this originates from our own uncertainty
about which parameter or hypothesis is correct, thereby still implicitly assume that \emph{one} of them is correct.
Namely, in the limit of large data, Bayesian methods with finite numbers of parameters will collapse to a point estimate, 
corresponding to the ``true parameter values'' (assuming the model is correct)~\citep{robert2007bayesian}.
Consequently, a Bayesian approach does not fundamentally enrich the model space by averaging over parameters -- it
is still necessary that exactly one set of parameters lead to the data but we are not exactly sure which one~\citep{minka2000bayesian}.
Consider as an example, Bayesian modeling for decision trees~\cite{chipman1998bayesian,lakshminarayanan2013top}
compared to (discriminative) ensemble based approaches~\citep{breiman2001random,rainforth2015canonical}.
The Bayesian approaches explicitly assume that our data is generated by one single decision tree and so in the limit of
large data the relative probabilities of different trees in the posterior diverges and will collapse to a
single tree.\footnote{Technically we only get a single tree 
	if we limit the tree depth to ensure a finite parameterization.  Nonetheless, the argument still
	holds from a practical perspective even when trees are unbounded.}  The ensemble approaches, on the other hand, maintain
a full ensemble of trees in the limit of large data.  They are, in fact, a fundamentally more general model class
as they do not require the data to have been generated by a single tree.  Here the averaging over trees enriches the
model class itself, rather than just representing uncertainty about which of the trees in the ensemble is the correct 
one~\citep{domingos1997does}, typically leading to better performing algorithms for large datasets than Bayesian approaches.