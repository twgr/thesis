% !TEX root = ../main.tex

\section{Frequentist vs Bayesian -- an Atheist's View on Two Religions}
\label{sec:bayes:religions}

We have just introduced the Bayesian approach to generative modeling, but this is far from
the only possible approach.  In this section, we will briefly introduce the alternative, \emph{frequentist},
approach.
  Unfortunately, there is often a substantial, and sometimes acrimonious, divide between those
who advocate Bayesian or frequentist methods within the statistics and machine learning communities.  Many people
have very strong views one way or another in the debate and it is easy, certainly as a graduate student,
to be sufficiently isolated on one side of the divide to completely detract oneself from the considerations of the
other.  It sometimes feels that the beliefs of some advocates of each side are so deep-rooted and 
fundamental that this almost a religious, rather than scientific divide.  Given the fierceness of the debate,
the actual statistical differences between the approaches (rather than the well-known philosophical differences we touched
on in Section~\ref{sec:prob:prob} which are often dubiously used for justification) are often surprisingly poorly understood.
Our aim in this section is not to advocate the use of one approach over the other, but to (hopefully objectively)
highlight these statistical differences and demonstrate that \emph{both} approaches have flaws, such that ``head in the sand''
mentalities either way can be highly detrimental.  
We note that whereas Bayesian methods are always, at least in theory, generative~\citep[Section~14.1]{gelman2014bayesian},
frequentist methods can be either generative or discriminative. 
As we have already discussed differences between generative and
discriminative modeling in Section~\ref{sec:bayes:discrim} we will omit this difference from our subsequent discussion.

At their root, the difference between Bayesian and frequentist methods stem from distinct fundamental
assumptions: frequentist modeling presumes fixed parameters, Bayesian modeling assumes fixed data.  In many ways,
both of these assumptions are actually quite silly. Why assume fixed parameters when we do not have enough
information about from the data to be certain of the correct value?  Why ignore that fact that other data could have 
been generated by the underlying process for the same underlying true parameter?  Perhaps this is why people have
such strong views either way -- the arguments against both methods are very strong, so much so that advocates are
often, sometimes unintentionally, willing to overlook the shortcomings of their favored approach.  

To elucidate the
different assumptions further and start looking into why they are made, we will now step into a decision-theoretic
framework.  Let's presume that the universe gives us some data $X$ and some true parameter $\theta$, the former of which
we can access but the latter of which is unknown.  We can alternatively think about this as $X$ being some information
that we actually receive and $\theta$ as some underlying truth or oracle from which we could make perfect predictions.  There is
no need for $\theta$ to be some explicit finite parameterization.  Any machine learning approach will take this data as input and
return some artifact or decision, for example, predictions on test data.  Let's call this our action, or decision, $a(X)$
and further presume that for a given $X$, $a(X)$ is deterministic (if we allow our predictions to be probability
distributions this assumption is effectively equivalent to assuming we can solve any analysis required by our approach exactly).
Presuming that our analysis is not frivolous, there will be some loss function $L(a(X),\theta)$ associated with the action we take
and the true parameter $\theta$, even if this loss function is subjective or unknown.  At a high level, our aim is always to
minimize this loss, but what we mean by minimizing the loss changes between the Bayesian and frequentist settings.  

In the
frequentist setting, $X$ is a random variable but $\theta$ is not.  Therefore, one takes an expectation of possible data
that could have been generated to give the frequentist risk $R(\theta)$ which is thus a function of theta~\cite{vapnik1998statistical}
\begin{align}
\label{eq:bayes:freq-risk}
R(\theta)  = \E\left[L(a(X),\theta) | \theta \right].
\end{align}
The focus is therefore on \emph{repeatability}
the generalization of the approach to different datasets that \emph{could} have been generated.
Choosing the parameters $\theta$ is thus based on optimizing for the best average performance over possible datasets,
which common approaches including cross-validation and minimax selection of $\theta$.  In some ways, this is a pessimistic
approach and the emphasis is on repeat performance and ensuring satisfactory performance for repeated use on different possible
datasets.  

In the Bayesian setting, $\theta$ is a random variable but $X$ is not.  Therefore one takes an expectation over $\theta$
to make predictions conditioned on the value of $X$ giving the \emph{posterior expected loss}~\cite{robert2007bayesian}
\begin{align}
\label{eq:bayes:bayes-est}
\varrho(\pi,X) = \E_{\pi(\theta)} [L(a(X),\theta) | X]
\end{align}
where $\pi(\theta)$ is a prior distribution on $\theta$.  Although $\varrho(\pi,X)$ is a function of the data, the Bayesian
approach takes the data as given (after all we have a particular dataset) and so for a given prior, the posterior
expected loss is a fixed value and, unlike in the frequentist case, further analysis is not required: the posterior
conveys all the information required.  The focus of the Bayesian approach is generalizing over all the possible values of
the parameters and using all the information at hand.  
This can be quite an optimistic approach as it does not consider the fact that, even for a given $\theta$,
other datasets might have been generated.  

\subsection{Shortcomings of the Frequentist Approach}
\label{sec:bayes:religion:freq}

One of the key criticisms of the frequentist approach is that predictions depend on the experimental procedure and
can violate the \emph{likelihood principle}.  The likelihood principle is that, for a given model and data, the only information relevant
to the parameters $\theta$ is conveyed by the likelihood function.  In other words, the same data and the same model should
always lead to the same conclusions about $\theta$.  Though this sounds intuitively obvious, it is actually violated by
taking an expectation of $X$ in frequentist methods as this introduces a dependency from the experimental procedure, which
is ancillary information to the data and the likelihood function.  

As a classic example, 
imagine that our data from flipping a coin is $3$ heads and $9$ tails.
In a frequentist setting we make different inferences about whether the coin is biased 
depending on whether our data originated from flipping the coin $12$ times and counting the number of heads or if we 
flipped the coin until we got $3$ heads.  For example, at the $5\%$ level of a significance test we can reject the null
hypothesis that the coin is unbiased in the latter case but not the former case.  This is obviously somewhat problematic but
can be used to argue both against frequentist methods and the likelihood principle (and thus Bayesian methods).  Using it
to argue against frequentist methods, and in particular significance tests, is quite straightforward: the subjective
differences in our experiment should not affect our conclusions about whether the coin is fair or not.  We can also take things
further and make the results change for absurd reasons: if our experimenter had intended to flip until she got
$3$ heads but was then attacked and killed by a bear while the twelfth flip was in the air such that further flips would not
have been possible regardless of the outcome, this again changes our conclusions
about where the coin is biased.  Clearly, it is ridiculous that the occurrence or lack of a bear attack during the experiment
should change our inferences, but that is need-to-know information for frequentist approaches, particularly if using p-values.

As we previously suggested though, one can also use this example to again for frequentist methods because it actually
suggests the likelihood principle is incorrect.  Although p-values are a terribly abused tool whose misuse has had
severe detrimental impact on many applied communities~\citep{goodman1999toward,ioannidis2005most}, they are not incorrect if 
interpreted correctly
and are an extremely important part of the statistical arsenal when used appropriately.  If one very carefully considers
the definition of a p-value as being the probability that the observed event or one more extreme is observed if the
\emph{experiment is repeated}, we see that our bear attack does actually affect the outcome.  The chance of getting the same
or more extreme data from repeating the experiment of ``\textit{flip coins until you get $3$ heads or make $12$ flips (at which
	point you will be killed by a bear)}'' is different to the chance of getting the same or a more extreme result from
repeating the experiment ``\textit{flip coins until you get $3$ heads}''.  As such, one can argue that the apparent absurd
changes in conclusions originate from misinterpreting the results and that, in fact, these changes actually demonstrate
that the likelihood principle is flawed by demonstrating why the experimental procedure actually matters because otherwise,
we have no concept of repeatability.  Imagine instead the scenario where a suspect researcher stops their experiment
early as it looks like the results are likely to support their hypothesis and they do not want to take the risk that if they
keep it running as long as they intended, then the results might no longer be so good.  Here the researcher has clearly
biased their results in a way that ostensibly violates the likelihood principle.

Whichever view you take, two things are relatively indisputable.  Firstly a number of a frequentist concepts, such as p-values,
are not compatible with the likelihood principle.  Secondly, frequentist methods are not always coherent such that they can
return answers that are not consistent with each other or probabilities that do not sum to one.  For example, p-values
for a hypothesis and its complement do not in general sum to $1$.  

Another major criticism of the frequentist approach is that it takes a point estimate for $\theta$, rather than averaging
over different possible parameters.  This can be somewhat inefficient in the finite data case as it limits the information
stored in the model to that stored by the parameters and it is clearly wasteful for a model to make predictions based on
only one possible set of parameters when our parameter estimates are not known to be correct.  Part of the reason that this
is done is to actively avoid placing a prior distribution on the parameters, either because this prior distribution might
be ``wrong''\footnote{Whether a prior can be wrong, or what that even means, is a somewhat open question except in the
	case where it fails to put any probability mass (or density for continuous problems) on the ground truth value of
	the parameters.} or because at a more philosophical level, it is not a random variable under the frequentist definition
of probability and so some people object to placing a distribution over it at a fundamental level (we will see this objection
mirrored by Bayesians for the data in the next section).  For the Bayesian perspective (and a viewpoint we actively 
argue for elsewhere in the paper), this is itself also a weakness of the frequentist approach as incorporating prior
information is often essential for effective modeling.


\subsection{Shortcomings of the Bayesian Approach}
\label{sec:bayes:religion:bayes}

Now we have exposed some shortcomings of the frequentist approach, let's consider if the Bayesian approach does any better.
Unfortunately, we find that the Bayesian approach is also not without its shortcomings.  We have already discussed one
key criticism in the last section in that the Bayesian approach relies on the likelihood principle which itself may not
be sound, or at the very least ill-suited for some statistical modeling problems.  More generally, it can be seen as
foolhardy to not consider other possible datasets that might have been generated.  The rationale typically given for
this is that we should use all the information available in the data and by calculating a frequentist risk we are throwing
some of this away.  For example, cross-validation approaches only ever use a subset of the data when training the model.
However, a common key aim of statistics is generalization and repeatability.  Bayesian approaches will often reach
spectacularly different conclusions for purely random changes between different datasets.  Consider, for example, a binary
classification
problem where using parameters $\theta_1$ classifies four billion of five billion points in the dataset correctly 
while using parameters $\theta_2$
classifies four billion and one thousand of data points correctly.  This gives a difference in classification accuracy 
of $0.00002\%$ but differences in the posterior probabilities for $\theta_1$ and $\theta_2$ will generally
be huge.\todo{Make this concrete}
Now presume that we regenerate our data and that each new point is drawn i.i.d.,
 that the probability of it being correctly classified using $\theta_1$ is independent to the probability of
being correctly classified by $\theta_2$, and that these probabilities are equal to the empirical misclassification
rates.  The probability that $\theta_1$ gives a better misclassification rate for the new dataset is around
almost $50\%$, even though previous posterior suggested that $\theta_2$ was substantially better than $\theta_1$.
Furthermore, if $\theta_1$ does give a much better misclassification rate for the new dataset, it is likely 
to be assigned a substantially higher mass under then posterior that $\theta_2$, even though we know that 
the latter is actually asymptotically preferable.  In terms of the posterior expected loss, this is not much of a problem
as for most sensible loss functions this will not substantially change.  However, if our aim is actually to learn 
about $\theta$ then this is quite worrying.  At the very least it shows why we should view posteriors with
a degree of skepticism, rather than taking them as ground truth.

Another concern for Bayesian approaches is the choice of the prior \dots \todo[inline]{write me}

Lack of ability to take discriminant approach

Computational intensity, particularly in terms of prediction compared to using point estimate for $\theta$.

\subsection{Are there Alternatives?}
\label{sec:bayes:religion:correct}

\todo[inline]{Rewrite and expand the next paragraph}

We might now ask, is anything correct?  The answer here is probably no, but that this does not really matter.  All methods
have their strengths and weaknesses and provided we use approaches appropriate to the problem at hand, things short work out.
One thing that can be done and should probably be done more often is to use both Bayesian and frequentist approaches concurrently,
by averaging both over data and over parameters, leading to the \emph{Bayes risk}.   For example, one could be Bayesian 
about the results from a cross-validation test.  Such an approach is still inevitably not universal -- we still need to choose
a prior, 

We argue that the two positions that are completely indefensible are the complete dismissal of Bayesian
methods and the complete dismissal of frequentist methods.  We have demonstrated that there are shortfalls
in both and, in general, there will be some cases where one is preferable and some where the other is.  In
particular, the Bayesian approach is essential when working with small datasets but where we have substantial
prior expertise.  On the other hand, a frequentist approach is essential to ensuring repeatability for large
datasets with little prior knowledge of note.  
We will take a Bayesian approach for most of the rest of this thesis, but this is decision-based subject matter and
to give focus to the work, rather than a point of principle.

\todo[inline]{Missing points:
	
Bayesian models are slow in prediction

mean vs median

Both assume some true underlying $\theta$ - often inferior to say an ensemble approach, Issues of Bayesian model averaging.

Using the data twice because influences choice of prior}