% !TEX root = ../main.tex

\section{Bayesianism vs Frequentism}
\label{sec:bayes:religions}

We have just introduced the Bayesian approach to generative modeling, but this is far from
the only possible approach.  In this section, we will briefly introduce and compare the alternative, \emph{frequentist}, approach.
As a community, we have a come a long way from the absolutism of~\cite{feller1950introduction}, with most researchers 
appreciating that
both Bayesian and frequentist approaches are a necessary component of the general statistical method.
Nonetheless, the divide between those within 
the statistics and machine learning communities
who advocate, at least at the philosophical level, the use Bayesian or frequentist methods 
is at times surprisingly large.  Many researchers
have strong views one way or another and it is easy, certainly as a graduate student,
to be sufficiently isolated to develop a somewhat warped view of the overall picture.
%It sometimes feels that the beliefs of some advocates of each side are so deep-rooted and 
%fundamental that this almost a religious, rather than scientific divide.  
The actual statistical differences between the approaches are somewhat distinct to the 
well-known philosophical differences we touched
on in Section~\ref{sec:prob:prob}, even though the latter are often dubiously used for justification 
for the practical application of a particular approach. These statistical differences are arguably
less well-known, in general, by those in the early stages of their research careers without statistics
backgrounds.
Our aim in this section is not to advocate the use of one approach over the other, but to (hopefully objectively)
highlight these statistical differences and demonstrate that \emph{both} approaches have advantages and
drawbacks, such that ``head in the sand''
mentalities either way can be highly detrimental, with effective modeling often requiring us to draw
on both.
We note that whereas Bayesian methods are always, at least in theory, generative~\citep[Section~14.1]{gelman2014bayesian},
frequentist methods can be either generative or discriminative. 
As we have already discussed differences between generative and
discriminative modeling in Section~\ref{sec:bayes:discrim}, we will mostly omit this difference from our subsequent discussion.

At their root, the statistical differences between Bayesian and frequentist methods\footnote{At least in their decision-theoretic
frameworks.  It is somewhat inevitable that delineation here and later will be a simplification on what is, in truth, not a 
clear-cut divide~\citep{gelman2011induction}.}
stem from distinct fundamental
assumptions: frequentist modeling presumes fixed parameters, Bayesian modeling assumes fixed data~\citep{jordan2009you}.  In many ways,
both of these assumptions are somewhat dubious. Why assume fixed parameters when we do not have enough
information from the data to be certain of the correct value?  Why ignore that fact that other data could have 
been generated by the same underlying true parameters?  However,
making such assumptions can sometimes be unavoidable for carrying out particular analyses.
%Perhaps this is why people have
%such strong views either way -- the arguments against both methods are very strong, so much so that advocates are
%often, sometimes unintentionally, willing to overlook the shortcomings of their favored approach.  

To elucidate the
different assumptions further and start looking into why they are made, we will now step into a decision-theoretic
framework.  Let's presume that the universe gives us some data $X$ and some true parameter $\theta$, the former of which
we can access, but the latter of which is unknown.  We can alternatively think in terms of $X$ being some information
that we actually receive and $\theta$ being some underlying truth or oracle from which we could make optimal predictions, noting 
that there is
no need for $\theta$ to be some explicit finite parameterization.  Any machine learning approach will take the data as input and
return some artifact or decision, for example, predictions for previously unseen inputs.  
Let's call this process the decision rule $\delta$, which we presume, for the sake of argument, to be 
deterministic for a giving dataset, producing decisions $\delta(X)$.\footnote{If we allow our predictions to be probability
distributions this assumption is effectively equivalent to assuming we can solve any analysis required by our approach exactly.}
Presuming that our analysis is not frivolous, there will be some loss function $L(\delta(X),\theta)$ associated with the action we take
and the true parameter $\theta$, even if this loss function is subjective or unknown.  At a high level, our aim is always to
minimize this loss, but what we mean by minimizing the loss changes between the Bayesian and frequentist settings.  

In the
frequentist setting, $X$ is a random variable but $\theta$ is not.  Therefore, one takes an expectation over possible data
that could have been generated, giving the frequentist risk~\citep{vapnik1998statistical}
\begin{align}
\label{eq:bayes:freq-risk}
R(\theta,\delta)  = \E\left[L(\delta(X),\theta) | \theta \right]
\end{align}
which is thus a function of theta and our decision rule.
The frequentist focus is therefore on \emph{repeatability}, i.e.
the generalization of the approach to different datasets that \emph{could} have been generated.
Choosing the parameters $\theta$ is thus based on optimizing for the best average performance over all possible datasets.

In the Bayesian setting, $\theta$ is a random variable but $X$ is fixed: the focus of the Bayesian approach 
is on generalizing over possible values of
the parameters and using all the information at hand.  Therefore one takes an expectation over $\theta$
to make predictions conditioned on the value of $X$, giving the \emph{posterior expected loss}~\citep{robert2007bayesian}
\begin{align}
\label{eq:bayes:bayes-est}
\varrho(\pi,\delta(X) | X) = \E_{\pi(\theta | X)} [L(\delta(X),\theta) | X],
\end{align}
where $\pi(\theta | X)$ is our posterior distribution on $\theta$.  Although $\varrho(\pi,\delta(X) | X)$ 
is a function of the data, the Bayesian
approach takes the data as given (after all we have a particular dataset) and so for a given prior and decision rule, the posterior
expected loss is a fixed value and, unlike in the frequentist case, further assumptions are not required to
calculate the optimal decision rule $\delta$.  To see this, we can consider calculating the 
\emph{Bayes risk}~\citep{robert2007bayesian}, 
also known as the \emph{integrated risk}, which averages over both data and parameters
\begin{align}
	\label{eq:bayes:bayes-risk}
r(\pi,\delta) = \E \left[\E_{\pi(\theta | X)} [L(\delta(X),\theta) | X]\right] = 
\E_{\pi(\theta | X)} \left[\E\left[L(\delta(X),\theta) | \theta \right] \right].
\end{align}
Here we have noted that we could have equivalently taken the expectation of the frequentist
risk over the posterior, such that, despite the name, the Bayes risk is neither wholly Bayesian
nor frequentist.  It is now straightforward to show (see e.g.~\citep{robert2007bayesian}) that the
decision function which minimizes $r(\pi,\delta)$ is obtained by, for each possible dataset $X\in\mX$,
choosing the decision that minimizes the posterior expected loss, i.e. $\delta(X) = \argmin_{\delta(X)} \varrho(\pi,\delta(X) | X)$.
By comparison, because the frequentist risk is still a function of the parameters,
 further work is required to define the optimal decision rule, e.g. by taking a \emph{minimax}
approach~\citep{vapnik1998statistical}.
We now see that the Bayesian approach can be relatively optimistic, as it is constrained to choose
decisions that optimize the expected loss,
whereas the frequentist approach allows, for example, $\delta$ to be chosen in a manner that
optimizes for the worst case $\theta$.

We now introduce some shortfalls that originate from taking each approach.  We emphasize that we are only
scratching the surface of one of the most hotly debated issues in statistics and do not even come close to doing
the topic justice.  Our aim is less to provide a comprehensive explanation of the relative merits of the two approaches, but
more to make the reader aware that there are a vast array of complicated, and sometimes controversial,
issues associated with whether to use a Bayesian or
frequentist approach, most of which have no simple objective conclusion.


\subsection{Shortcomings of the Frequentist Approach}
\label{sec:bayes:religion:freq}

One of the key criticisms of the frequentist approach is that predictions depend on the experimental procedure and
can violate the \emph{likelihood principle}.  The likelihood principle states that, for a given model, 
the only information relevant
to the parameters $\theta$ conveyed by the data is encoded through the likelihood function~\citep{robert2007bayesian}.  
In other words, the same data and 
the same model should always lead to the same inferences about $\theta$.  Though this sounds intuitively obvious, it is actually violated by
taking an expectation of $X$ in frequentist methods, as this introduces a dependency from the experimental procedure.

As a classic example, 
imagine that our data from flipping a coin is $3$ heads and $9$ tails.
In a frequentist setting, we make different inferences about whether the coin is biased 
depending on whether our data originated from flipping the coin $12$ times and counting the number of heads, or if we 
flipped the coin until we got $3$ heads.  For example, at the $5\%$ level of a \emph{significance test}, we can reject the \emph{null
hypothesis} that the coin is unbiased in the latter case, but not the former.  This is obviously somewhat problematic, but it
can be used to argue both for and against frequentist methods.  Using it
to argue against frequentist methods, and in particular significance tests, is quite straightforward: the subjective
differences in our experiment should not affect our conclusions about whether the coin is fair or not.  We can also take things
further and make the results change for absurd reasons.  For example, imagine our experimenter had intended to flip until she got
$3$ heads, but was then attacked and killed by a bear while the twelfth flip was in the air, such that further flips would not
have been possible regardless of the outcome.  In the frequentist setting, this again changes our conclusions
about whether the coin is biased.  Clearly, it is ridiculous that the occurrence or lack of a bear attack during the experiment
should change our inferences, but that is need-to-know information for frequentist approaches.

As we previously suggested though, one can also use this example to argue for frequentist methods as one can argue that it actually
suggests the likelihood principle is incorrect.  Although significance tests are a terribly abused tool whose misuse has had
severe detrimental impact on many applied communities~\citep{goodman1999toward,ioannidis2005most}, they are not incorrect,
and extremely useful, if 
interpreted correctly.  If one very carefully considers
the definition of a \emph{p-value} as being the probability that a given, or more extreme, event is observed if the
\emph{experiment is repeated}, we see that our bear attack does actually affect the outcome.  Namely, the chance of getting the same
or more extreme data from repeating the experiment of ``\textit{flip the coin until you get $3$ heads}'' is different to 
the chance of getting the same or a more extreme result from repeating the experiment 
``\textit{flip the coin until you get $3$ heads or make $12$ flips (at which point you will be killed by a bear)}''.  
As such, one can argue that the apparently absurd
changes in conclusions originate from misinterpreting the results and that, in fact, these changes actually demonstrate
that the likelihood principle is flawed because, without a notion of an experimental procedure, 
we have no concept of repeatability.  Imagine instead the more practical scenario where a suspect researcher stops their experiment
early as it looks like the results are likely to support their hypothesis and they do not want to take the risk that if they
keep it running as long as they intended, then the results might no longer be so good.  Here the researcher has clearly
biased their results in a way that ostensibly violates the likelihood principle.

Whichever view you take, two things are relatively indisputable.  Firstly a number of a frequentist concepts, such as p-values,
are not compatible with the likelihood principle.  Secondly, frequentist methods are not always \emph{coherent}, such that they can
return answers that are not consistent with each other, e.g. probabilities that do not sum to one.  
%For example, p-values
%for a hypothesis and its complement do not, in general, sum to one.  

Another major criticism of the frequentist approach is that it takes a point estimate for $\theta$, rather than averaging
over different possible parameters.  This can be somewhat inefficient in the finite data case, as it limits the information
gathered from the learning process to that encoded by the calculated point estimate for $\theta$, which is then wasteful
when making predictions.  Part of the reason that this
is done is to actively avoid placing a prior distribution on the parameters, either because this prior distribution might
be ``wrong''\footnote{Whether a prior can be wrong, or what that even means, is a somewhat open question except in the
	case where it fails to put any probability mass (or density for continuous problems) on the ground truth value of
	the parameters.} or because, at a more philosophical level, they are not random variables under the frequentist definition
of probability.  Some people thus object to placing a distribution over them at a fundamental level (we will see this objection
mirrored by Bayesians for the data in the next section).  For the Bayesian perspective (and a viewpoint we actively 
argue for elsewhere in the paper), this is itself also a weakness of the frequentist approach as incorporating prior
information is often essential for effective modeling.


\subsection{Shortcomings of the Bayesian Approach}
\label{sec:bayes:religion:bayes}

Unfortunately, the Bayesian approach is also not without its shortcomings.  We have already discussed one
key criticism in the last section in that the Bayesian approach relies on the likelihood principle which itself may not
be sound, or at the very least ill-suited for some statistical modeling problems.  More generally, it can be seen as
foolhardy to not consider other possible datasets that might have been generated.  Taking a very strict stance, then
even checking the performance of a Bayesian method on test data is fundamentally frequentist, as we are assessing how
well our model generalizes to other data.  Pure Bayesianism, which is admittedly not usually carried out in practice, shuns
empiricism as empiricism, by definition, is rooted in the concept of repeated trials which is not possible if the data is kept fixed.
The rationale typically given for
this is that we should use all the information available in the data and by calculating a frequentist risk we are throwing
some of this away.  For example, cross-validation approaches only ever use a subset of the data when training the model.
However, a common key aim of statistics is generalization and repeatability.  Pure Bayesian approaches include no
consideration for \emph{calibration}, which means that, even if our likelihood model is correct, there is still no reason
that any probabilities or confidence intervals must be also.  This at odds with frequentist approaches, for which
we can often derive absolute guarantees.

A related issue is that Bayesian approaches will often reach
spectacularly different conclusions for ostensibly inconsequential changes between datasets.\footnote{Though this is arguably more
of an issue with generative approaches than Bayesian methods in particular.}
At least when making the standard assumption of i.i.d.~data in Bayesian analysis, then likelihood terms are multiplicative 
and so typically when one adds more data, the relative probabilities of two parameters quickly diverge.  This divergence is
necessary for Bayesian methods to converge to the correct ground truth parameters for data distributed exactly as per the
model, but it also means any slight misspecifications in the likelihood model
become heavily emphasized very quickly.  As a consequence, Bayesian methods can chronically underestimate uncertainty 
in the parameters, particularly for large datasets, because they do not account for
the \emph{unknown unknowns}.  This means that ostensibly  inconsequential features of the likelihood model
can lead to massively different conclusions about the relative probabilities of different parameters.
In terms of the posterior expected loss, this is often not much of a problem as the assumptions might be similarly
inconsequential for predictions.  However, if our aim is actually to learn 
about the parameters themselves then this is quite worrying.  At the very least it shows why we should view posteriors with
a serious degree of skepticism (particularly in their uncertainty estimates), rather than taking them as ground truth.

Though techniques such as cross-validation can reduce sensitivity to model
misspecification, generative frequentist methods still often do not fare much better than Bayesian
methods for misspecified models (after all they produce no uncertainty estimates on $\theta$).
Discriminative methods, on the other hand, do not have
an explicit model to specify in the same way and so are far less prone to the same issues.
Therefore, though  much of the criticisms of Bayesian modeling stem
from the use of a prior, issues with model (i.e. likelihood) misspecification are often much more severe and predominantly
shared with generative frequentist approaches~\citep{gelman2013not}.  It is, therefore, often the question of discriminative
vs generative machine learning that is most critical~\citep{breiman2001statistical}.

%
%Consider, for example, a binary
%classification problem where, as in common in the Bayesian framework, we will presume
%that our observations $y_{1:N}$ are independent such that our likelihood is a product of 
%likelihood terms for individual datapoints, i.e.
%$p(y_{1:N} | \theta) = \prod_{n=1}^{N} p(y_n | \theta)$.  We now presume that using
%hyperparameter\footnote{Mathematically is no hard distinction between a hyperparameter and a parameter, but the
%	former is often used to refer to ``higher-level'' parameters.  For example, we might have a prior over model parameters, with
%	the parameters of the prior itself referred to as hyperparameters.  Here we are implicitly presuming that there are some latent
%	variables that have been marginalized over to make the setup realistic, hence $\theta$ corresponding to a ``hyperparameter''.}
% $\theta_1$ classifies four billion of the five billion points in the dataset ``correctly''
%while using hyperparameter $\theta_2$, which has the same prior probability,
%classifies four billion and one thousand of data points ``correctly'', where by correct we mean that $p(y_n | \theta) = 0.8$
%(otherwise $p(y_n | \theta) = 0.2$).  We thus have that the difference in classification accuracies of the
%approaches is $0.00002\%$, but under our posterior $\theta_2$ is $(0.8/0.2)^{1000}\approx10^{602}$ times
%more probable than $\theta_1$.  Intuitively this is preposterous and no sensible gambler would ever take odds
%that $\theta_2$ is even twice as likely to give better performance on the a test dataset of the same size.  
%Though somewhat oversimplified, this example is not actually unrepresentative to behavior often experienced.  
%To explain the dichotomy between the theoretical correctness of Bayes' rule and our intuitions, we consider what
%happens if we let the universe regenerate the dataset.  We will draw datapoints independently
%and we presume that the probability of a new datapoint being correctly classified using $\theta_1$ is independent to the probability of
%it being correctly classified by $\theta_2$, and that these probabilities are equal to the empirical misclassification
%rates from our first dataset.  The probability that $\theta_1$ gives a better misclassification rate for the new dataset is
%almost $50\%$, even though the previous posterior suggested that $\theta_2$ was astronomically better than $\theta_1$.
%Furthermore, there is almost a $50\%$ chance that $\theta_1$ will have an equally astronomically higher
%posterior mass for our second dataset!  The problem here is that the Bayesian approach does not taken into 
%account that datasets are usually sampled from a population or possible samples that \emph{could} have been generated.
%Our analysis was perfectly reasonable under the assumption that the our dataset was fixed and absolute, but it
%lead to somewhat preposterous conclusions when we generalized to other possible data.
%In terms of the posterior expected loss, this is not much of a problem
%as for most sensible loss functions this will not substantially change.  However, if our aim is actually to learn 
%about $\theta$ then this is quite worrying.  At the very least it shows why we should view posteriors with
%a degree of skepticism, rather than taking them as ground truth.

Naturally one of the biggest concerns with Bayesian approaches is their use of a prior, with this being
one of the biggest differences to generative frequentist approaches.  The prior is typically a double-edged
sword.  On the one hand, it is necessary for combining existing knowledge and information from data 
in a principled manner, on the other hand, priors are inherently subjective and so all produced posteriors are
similarly subjective.  Given the same problem, different practitioners will use different priors and reach
potentially different conclusions.  In the Bayesian framework, there is no such thing
as a correct posterior for a given likelihood (presuming finite data).
Consequently, ``all bets are off'' for repeated experimentation with Bayesian methods as there is no
quantification for how wrong our posterior predictive might be compared with the true generating
distribution.  This can mean they  are somewhat inappropriate for tasks where the focus is on repeated use,
e.g. providing reliable confidence intervals for medical trials, though many Bayesian methods retain
good frequentist properties.  Such cases both predicate a need for
considering that there are many possible datasets that might occur and, ideally, an objective approach
that means the conclusions are independent of the whims of the analyst.

In particular, there is no (objective) Bayesian alternative to frequentist
\emph{falsification} methods such as the aforementioned significance tests.\footnote{This is not to say one cannot 
	perform falsification as part of Bayesian modeling, in fact avoiding doing so would
	be ridiculous, but that providing objective statistical guarantees to this falsification requires the use of
	frequentist methods.  For example, even the essential research process of informally
	rejecting and improving models undermines Bayesian coherence~\citep{gelman2011induction}.  On the other hand, the
	process of peer review effectively
	invalidates frequentist calibration by ensuring some studies never make it to the public domain.}
  Both Bayesian and frequentist methods require assumptions
and neither can ever truly prove that a particular model or prediction is correct, but frequentist methods do
allow one to indisputably \emph{disprove} hypotheses to within some confidence interval.  The real power
of this is realized when we consider disproving \emph{null hypotheses}, e.g. the hypothesis that an
output is independent of a potential driving factor.  This is why significance tests are so widely
used through the sciences as, although they cannot be used to prove
a model is correct (as much as people might try), they can certainly be used to show 
particular assumptions or models are wrong.

The use of a prior in Bayesian modeling can also be problematic because it is often easy to end up 
``using the data twice''~\citep{gelman2008objections}.  The Bayesian paradigm requires that the
prior is independent from the data and this means that there should not be any human-induced
dependency.  In other words, it is necessary that the user sets their prior before
observing the data they condition on, otherwise, the data will both influence their choice of prior
and be incorporated through the likelihood.  This is not a problem with Bayesian 
methods per se, but it can be a common shortfall in their application.

Another practical issue with Bayesian methods is their computational complexity at both
train time and test time.  Firstly, using Bayesian methods requires one to carry out inference,
which, as we explain in Chapter~\ref{chp:inf}, is typically a challenging and computationally
expensive process, often prohibitively so.  Some frequentist approaches can also
be similarly expensive at train time, but others can be substantially cheaper, particularly
discriminative approaches.  Further, Bayesian methods tend to also be somewhat expensive
for making predictions with the posterior predictive itself being an integral.  Frequentist
methods tend to instead predict using point estimates for $\theta$, such that prediction is typically
much cheaper.

\subsection{Practical Usage}
\label{sec:bayes:religion:correct}

Although Bayesianism and frequentism are both exact frameworks, their application to real problems is not.
Both frameworks have their strengths and weaknesses and so perhaps the key question is not which framework
is correct, but when should we use each.  In
particular, the Bayesian approach is often essential when working with small datasets but where we have substantial
prior expertise.  On the other hand, a frequentist approach is essential to providing guarantees and ensuring repeatability.  
Bayesian and frequentist methods are also by no means mutually 
exclusive and effective modeling often requires elements of both
to be used concurrently.  
For example, one could be Bayesian about the results from a cross-validation test or look to calculate
frequentist guarantees for a Bayesian model.
In essence, Bayesian and frequentist analysis have different aims -- Bayesianism is about updating
subjective beliefs and frequentism is about creating long run, or repeated application, guarantees.
We often we care about both.  It is also worth noting that a number of Bayesian methods exhibit
good frequentist properties, see e.g.~\cite{mcallester2013pac} and the references therein.

We finish by noting a critical assumption made by both Bayesian and generative frequentist methods --
that there is some true underlying value for the parameters.  Because
all models are approximations of the real world, this is often a misguided and harmful assumption.  That this assumption
is made is clear in the frequentist setting, but is somewhat subtle for Bayesian approaches.
Bayesian methods allow for multiple hypotheses or parameter values, but this originates from our own uncertainty
about which parameter or hypothesis is correct, thereby still implicitly assuming that \emph{one} of them is correct.
Namely, as we showed with the Bernstein-Von Mises theorem,  in the limit of large data, Bayesian methods with finite numbers of parameters will collapse to a point estimate, 
corresponding to the ``true parameter values'' (assuming the model is correct).
Consequently, a Bayesian approach does not fundamentally enrich the model space by averaging over parameters -- it
is still necessary that exactly one set of parameters lead to the data, but we are not exactly sure which one~\citep{minka2000bayesian}.

Consider as an example, Bayesian modeling for decision trees~\citep{chipman1998bayesian,lakshminarayanan2013top}
compared to (discriminative) ensemble-based approaches~\citep{breiman2001random,rainforth2015canonical}.
The Bayesian approaches explicitly assume that our data is generated by one single decision tree and so in the limit of
large data the relative probabilities of different trees in the posterior diverge and will collapse to a
single tree.\footnote{Technically we only get a single tree 
	if we limit the tree depth to ensure a finite parameterization.  Nonetheless, the argument still
	holds from a practical perspective even when trees are unbounded.}  The ensemble approaches, on the other hand, maintain
a full ensemble of trees in the limit of large data.  They are, in fact, a fundamentally more general model class
as they do not require the data to have been generated by a single tree.  Here the averaging over trees enriches the
model class itself, rather than just representing uncertainty about which of the trees in the ensemble is the correct 
one~\citep{domingos1997does}, typically leading to better performing algorithms for large datasets than Bayesian approaches.