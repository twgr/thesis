% !TEX root = ../main.tex

\section{Optimization in Probabilistic Machine Learning}
\label{sec:opt:intro}

In general, all optimization problems can be expressed in the form
\begin{align}
	\begin{split}
	\label{eq:opt:funcMax}
	\theta^* &= \argmax_{\theta \in \vartheta} f\left(\theta\right) \\
	\subto & \: g_i(\theta) = c_i \quad \forall i \in \{1,\dots,n_e\} \\
		 & h_i(\theta) \ge d_i \quad \forall i \in \{1,\dots,n_i\}
	\end{split}
\end{align}
where $f \colon \vartheta \rightarrow \real$ is the target function with input
variables $\theta$, $\vartheta$ is the space of permissible solutions, 
$g_i(\theta) = c_i$ are equality constraints, $h_i(\theta) \ge d_i$ are
inequality constraints, and $\theta^*$ is the optimum value of $\theta$.
We note that careful definition of $\vartheta$ can remove the need to
define constraints explicitly, a fact which we exploit in Chapter~\ref{chp:bopp},
while minimization problems can be dealt with by maximizing $-f\left(\theta\right)$
instead of $f\left(\theta\right)$.

A large range of problems are covered by~\eqref{eq:opt:funcMax}.  For example, 
there are arbitrary different types $\vartheta$ might correspond to from the set of real
values to the setup of race car.  There is also a wide range of information that might
be known about $f$ -- we might have access to the exact function and all its derivatives,
or it might be that the only way to evaluate it is to conduct an expensive real-world 
experiment with stochastic results at a given value of $\theta$.  Naturally this leads 
to a plethora of different techniques to suit the different problems at hand such
as dynamic programming~\citep{bellman2013dynamic}, evolutionary 
algorithms~\citep{back1996evolutionary}, combinatorial optimization 
methods~\citep{papadimitriou1982combinatorial}, gradient-based 
methods~\citep{boyd2004convex}, and Monte Carlo based methods~\citep{robert2004monte}.
Such optimization algorithms can be split into two key categories:
local optimization approaches and global optimization approaches.  
The latter strictly aims to solve the true target as laid out in~\eqref{eq:opt:funcMax}, but
is in often infeasible in practice.
The former instead only tries to find a point where there is no better point in its immediate vicinity. 
Local optimization is generally
used either when the problem is known or expected to be convex (such that there is only
one such local optimum), or when global optimization is infeasible and a local
optimum is expected to still for a good approximate solution, as is typically the case in,
for example, neural network training.

In a probabilistic machine learning context, common optimization problems 
include maximum likelihood (ML) estimation,
maximum a posterior (MAP) estimation, maximum marginal likelihood (MML) estimation, 
marginal maximum a posteriori (MMAP) estimation,
and risk minimization.  
ML estimation looks to find the most probable set of parameters
for a given distribution, namely given data $Y$ they look to find
\begin{align}
\label{eq:opt:max-lik}
\theta^* &= \argmax_{\theta \in \vartheta} p(Y|\theta).
\end{align}
Many classical statistical and machine learning techniques on based on 
ML estimation as it is intuitively the most probable
set of parameters given data and a model.  However, this can be prone to overfitting and
so it typically requires some form of regularization~\citep{hastie01statisticallearning}.

MAP estimation corresponds to maximizing the posterior probability, which is equivalent
to extending ML estimation to also include a prior term, noting that $p(Y)$ is a constant
\begin{align}
\label{eq:opt:map}
\theta^* &= \argmax_{\theta \in \vartheta} p(\theta|Y) = \argmax_{\theta \in \vartheta} p(Y|\theta) p(\theta).	
\end{align}
This provides regularization compared to ML estimation (indeed many priors and
ML regularization methods are exactly equivalent~\citep{bishop2006pattern}),
but still has a number of drawbacks compared to full inference when the latter is
possible.  For example, the position of the MAP estimate is dependent of the
parametrization of the problem (see Section~\ref{sec:prob:measure}).  Using a MAP estimate also, of course,
incorporates less information into the predictive distribution than using a
fully Bayesian approach.  Nonetheless, MAP estimation is still an important
tool of Bayesian machine learning, necessary when a single estimate or decision is required,
or when inference is infeasible.  Note that when $\vartheta$ is bounded, 
then ML estimation is recovered from MAP estimation by using
a uniform prior.

MML estimation is often referred to using a number of different names such as expectation
maximization, type-II maximum likelihood estimation, hyperparameter optimization, and
empirical Bayes.  Despite the multitude of names, the aim of all these methods is the 
same: to maximize the likelihood of a model
marginalized over some latent variables $X$, namely
\begin{align}
	\label{eq:opt:MML}
	\theta^* &= \argmax_{\theta \in \vartheta} p(Y|\theta) 
	= \argmax_{\theta \in \vartheta} \E \left[p(Y,X|\theta) | Y, \theta\right]
\end{align}
where the expectation is with respect to $p(X | \theta)$.  Common examples of MML
problems include parameter tuning, decision making, model learning, and policy search~\citep{deisenroth2013survey}.

MMAP estimation varies from MML in the same manner as MAP from ML: the inclusion
of a prior on the target parameters.  Specifically it aims to optimize
\begin{align}
\label{eq:opt:MMAP}
\theta^* &= \argmax_{\theta \in \vartheta} p(\theta | Y)  = \argmax_{\theta \in \vartheta} p(Y,\theta) 
= \argmax_{\theta \in \vartheta} \E \left[p(Y,X|\theta) p(\theta) | Y, \theta\right].
\end{align}
As with MML and MMAP estimation, risk minimization problems consider the optimization
of an expectation.  The key difference is in the fact that they are a \textit{minimization} rather
than a \textit{maximization}, namely they look to find
\begin{align}
\label{eq:opt:riskmin}
\theta^* &= \argmin_{\theta \in \vartheta}  \E \left[L(X,Y;\theta)\right]
\end{align}
for some loss function $L$ which is parametrized by $\theta$ and is typically positive only.
Depending on the context, $Y$ can either be taken as fixed, or the expectation can be over
both $X$ and $Y$.  The latter corresponds to the classical risk taken in the frequentist view
of supervised learning.
At first, the change from
maximization to minimization might seem trivial.  However,
the fact that probabilities are positive only means that the two problems behave very
differently.  For example, imagine the problem of designing a bridge and let 
$Y=\emptyset$, $X$ be the loadings the bridge will see in its lifetime, and $\theta$
the design.  Even though the probability of failure will generally be very low, the
loss in the case where the bridge collapses is very large.  Therefore the expected loss
is dominated by rare adversarial events and the desire is to choose a ``safe'' value for
$\theta$.  Such a problem cannot be encoded effectively as a MML or MMAP estimation
problem where we are maximizing the probability of the bridge not collapsing,
as this expected probability cannot be dominated by the rare adversarial loading cases.
In essence, expectations over probabilities are ``optimistic'' and by construction
focus on the more probable cases.

In the rest of this chapter, we will consider in more depth a particular approach
for carrying out such optimizations known as Bayesian optimization (BO).
%two approaches that have been used
%extensively through machine learning to solve optimization problems such as those 
%discussed: stochastic gradient ascent and Bayesian optimization.  As we will see, these
%represent quite starkly different approaches and are thus suited to very different problems.