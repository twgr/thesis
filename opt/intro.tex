% !TEX root = ../main.tex

\section{Introduction}
\label{sec:opt:intro}

From designing engineering components to managing portfolios and tuning control systems,
optimization is used ubiquitously throughout engineering and the sciences.   Whereas inference
dealt with cases were we desire to calculate an expectation or approximate a probability distribution
given data, optimization considers cases where only a single output is desired.  For example, in
a decision problem only a single action can be taken, meaning a single optimal action must be
found.  It it is also often used in machine learning as a tractable alternative when full inference
is infeasible~\cite{murphy2012machine}.

In general, all optimization problems can be expressed in the form
\begin{align}
	\begin{split}
	\label{eq:opt:funcMax}
	\theta^* &= \argmax_{\theta \in \vartheta} f\left(\theta\right) \\
	\subto & \: g_i(\theta) = c_i \quad \forall i \in \{1,\dots,n_e\} \\
		 & h_i(\theta) \ge d_i \quad \forall i \in \{1,\dots,n_i\}
	\end{split}
\end{align}
where $f \colon \vartheta \rightarrow \real$ is the target function with input
variables $\theta$, $\vartheta$ is the space of permissible solutions, 
$g_i(\theta) = c_i$ are equality constraints, $h_i(\theta) \ge d_i$ are
inequality constraints, and $\theta^*$ is the optimum value of $\theta$.
We note that careful definition of $\vartheta$ can remove the need to
define constraints explicitly, a fact which we exploit in Chapter~\ref{chp:bopp},
while minimization problems can be dealt with by maximizing $-f\left(\theta\right)$.

A large range of problems covered by~\eqref{eq:opt:funcMax}.  For example, 
there are arbitrary different types $\vartheta$ might correspond to from the set of real
values to the setup of race car.  There is also a wide range of information that might
be known about $f$ - we might have access to the exact function and all its derivatives,
or it might be that the only way to evaluate it is to conduct an expensive real world 
experiment with stochastic results at a given value of $\theta$.  Naturally this leads 
to a plethora of different techniques to suit the different problems at hand such
as dynamic programming~\citep{bellman2013dynamic}, evolutionary 
algorithms~\citep{back1996evolutionary}, combinatorial optimization 
methods~\citep{papadimitriou1982combinatorial}, gradient based 
methods~\citep{boyd2004convex}, and Monte Carlo based methods~\citep{robert2004monte}.
Such optimization algorithms can be split into two key categories:
local optimization approaches and global optimization approaches.  
The latter strictly aims to solved the true target as laid out in~\eqref{eq:opt:funcMax}, but
is in often infeasible in practice.
The former instead only tries to find a point where there is no better point in its immediate vicinity. 
Local optimization is generally
used either when the problem is known or expected to be convex (such that there is only
one such local optimum), or when global optimization is infeasible and a local
optimum is expected to still for a good approximate solution.  

In a probabilistic machine learning context, common optimization problems 
include maximum likelihood (ML) estimation,
maximum a posterior (MAP) estimation, maximum marginal likelihood (MML) estimation, 
marginal maximum a posteriori (MMAP) estimation,
and risk minimization.  

ML estimation looks to find the most probable set of parameters
for a given distribution, namely given data $Y$ they look to find
\begin{align}
\label{eq:opt:max-lik}
\theta^* &= \argmax_{\theta \in \vartheta} p(Y|\theta).
\end{align}
Many classical statistical and machine learning techniques on based on 
ML estimation~\cite{hastie01statisticallearning} as it is intuitively the most probable
set of parameters given data and a model.  However, this can be prone to overfitting and
so it is typically requires the inclusion of regularization terms.

MAP estimation extends ML estimation to also include a prior term
\begin{align}
\label{eq:opt:map}
\theta^* &= \argmax_{\theta \in \vartheta} p(Y|\theta) p(\theta).	
\end{align}
This provides regularization compared to ML estimation (indeed many priors and
ML regularization methods are exactly equivalent~\cite{bishop2006pattern}),
but still has a number of drawbacks compared to full inference when the latter is
possible.  For example, the position of the MAP estimate is dependent of the
parametrization of the problem: if some mapping to input variables is
provided $\phi = \eta (\theta)$, this changes the position of the optimum, i.e.
in general $\phi^* \neq \eta (\theta^*)$.  Using a MAP estimate also of course
incorporates less information into the predictive distribution than using a
fully Bayesian approach.  Nonetheless, MAP estimation is still an important
tool of Bayesian machine learning, necessary when a single estimate or decision is required,
or when inference is infeasible.  Note that when $\vartheta$ is bounded or takes
on only finite values, then ML estimation is recovered from MAP estimation by using
a uniform prior.

MML estimation is often referred to using a number of different names such as expectation
maximization, type-II maximum likelihood estimation, hyperparameter optimization, and
empirical Bayes.  Despite the multitude of names, which originate from different application
areas, the aim of all these methods is the same: to maximize the likelihood of a model
marginalized over some latent variables $X$, namely
\begin{align}
	\label{eq:opt:MML}
	\theta^* &= \argmax_{\theta \in \vartheta} p(Y|\theta) 
	= \argmax_{\theta \in \vartheta} \E \left[p(Y|X,\theta) | Y, \theta\right]
\end{align}
where the above expectation is with respect to $p(X | \theta)$.  Common examples of MML
problems include parameter tuning, decision making, and model learning.

MMAP estimation varies from MML in the same manner as MAP from ML: the inclusion
of a prior on the target parameters.  Specifically it aims to optimize
\begin{align}
\label{eq:opt:MMAP}
\theta^* &= \argmax_{\theta \in \vartheta} p(Y,\theta) 
= \argmax_{\theta \in \vartheta} \E \left[p(Y|X,\theta) p(\theta) | Y, \theta\right].
\end{align}
As with MAP estimation compared with ML estimation, 
MMAP estimation problems extend MML by allowing for prior information or regularization to be
included.

As with MML and MMAP estimation, risk minimization problems consider the optimization
of an expectation.  The key difference is in the fact that the are a \textit{minimization} rather
than a \textit{maximization}, namely they look to find
\begin{align}
\label{eq:opt:riskmin}
\theta^* &= \argmin_{\theta \in \vartheta}  \E \left[L(X,Y;\theta)\right]
\end{align}
for some loss function $L$ which is parametrized by $\theta$ and is typically positive only.
Depending on the context, $Y$ can either be taken as fixed, or the expectation can be over
both $X$ and $Y$.  The later corresponds to the classical risk taken in the frequentist view
of supervised.
At first the change from
maximization to minimization might seem trivial.  However,
the fact that probabilities are positive only means that the two problems behave very
differently.  For example, imagine the problem of designing a bridge and let 
$Y=\emptyset$, $X$ be the loadings the bridge will see in its lifetime, and $\theta$
the design.  Even though the probability of failure will generally be very low, the
loss in the case where the bridge collapses is very large.  Therefore the expected loss
is dominated by rare adversarial events and the desire is to choose a ``safe'' value for
$\theta$.  Such a problem cannot be encoded effectively as a MML or MMAP estimation
problem where we maximizing the probability of the bridge not collapsing,
as this expected probability cannot be dominated by the rare adversarial loading cases.
In essence, expectations over probabilities are ``optimistic'' and by construction
focus on the more probable cases.

In the rest of this chapter we will consider in more depth two approaches that have been used
extensively through machine learning to solve optimization problems such as those 
discussed: stochastic gradient ascent and Bayesian optimization.  As we will see, these
represent quite starkly different approaches and are thus suited to very different problems.