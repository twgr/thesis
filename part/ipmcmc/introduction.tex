% !TEX root = ../main.tex

% Intro, contribution statement and related work
\subsection{Introduction}
\label{sec:ipmcmc:intro}
%In this paper we propose the interacting particle Markov chain Monte Carlo (\ipmcmc), a particle Markov chain Monte Carlo (\pmcmc) method that is suitable to execution on distributed and multi-core computing architectures and present empirical results suggesting significant improved mixing rates relative to both non-interacting \pg nodes and a single \pg node with an identical computational budget. 

%In this paper we present a Markov chain Monte Carlo (\mcmc) method that employs a pool of particle Markov chain Monte Carlo (\pmcmc) and sequential Monte Carlo (\smc) samplers. 


In this paper we propose the interacting particle Markov chain Monte Carlo (\ipmcmc) sampler. In \ipmcmc we run a pool of \csmc and unconditional \smc algorithms as parallel processes that we refer to as nodes. After each run of this pool, we apply successive Gibbs updates to the indexes of the \csmc nodes, such that the indices of the \csmc nodes changes. Hence, the nodes from which retained particles are sampled can change from one MCMC iteration to the next. This lets us trade off exploration (\smc) and exploitation (\csmc) to achieve improved mixing of the Markov chains. Crucially, the pool provides numerous candidate indices at each Gibbs update, giving a significantly higher probability that an entirely new retained particle will be ``switched in'' than in non-interacting alternatives.

This interaction requires only minimal communication; each node must report an estimate of the marginal likelihood and receive a new role (\smc or \csmc) for the next sweep. This means that \ipmcmc is embarrassingly parallel and can be run in a distributed manner on multiple computers.

%We introduce the interacting particle Markov chain Monte Carlo (\ipmcmc) method, a new type of 
%high-dimensional \tom{The phrase high-dimensional seems a little out of place here to me.  I understand what you are saying but it is maybe a bit premature to make the point}
%Monte Carlo integration method enabling efficient 

We prove that \ipmcmc is a partially collapsed Gibbs sampler on the extended space containing the particle sets for all nodes. In the special case where \ipmcmc uses only \emph{one} \csmc node, it can in fact be seen as a non-trivial and unstudied instance of the $\alpha$-\smc-based \citep{whiteley2016} \pmcmc method introduced by \citet{huggins2015}. However, with \ipmcmc we extend this further to allow for an arbitrary number of \csmc and standard \smc algorithms with interaction. Our experimental evaluation shows that \ipmcmc outperforms both independent \pg samplers as well as a single \pg sampler with the same number of particles run longer to give a matching computational budget.

An implementation of iPMCMC is provided in the probabilistic programming system \emph{Anglican}\footnote{\angurl} \citep{wood2014new}, whilst illustrative MATLAB code, similar to that used for the experiments, is also provided\footnote{\myurl}.

%Our key contribution is the \ipmcmc method, %with a highly efficient off-the-shelf \brooks{what do you mean by ``off the shelf''?} \mcmc method, the interacting particle Markov chain Monte Carlo (\ipmcmc), that also can make systematic use of distributed or multi-core computing architectures. 

%\tom{Should we maybe add the longer run chains to results to that we can make this point clearly?}
%\fredrik{Is the last sentence ok?}