% !TEX root = ../main.tex

\chapter{Introduction}
\label{chp:intro}

\input{intro/rise.tex}

\section{Thesis Aims and Layout}

This thesis contains a mixture of pedagogical material, literature review, and 
cutting-edge research.
Although there will inevitably be gaps, the aim is to take
the reader from the fundamentals of probability and machine learning,
all the way through to highly advanced topics such as designing automatic
mixed inference-optimization engines for probabilistic programming languages.
In addition to making the work is as self-contained as is realistically possible, there are
three key rationales for doing this.  Firstly, it is our aim to make the work 
as accessible as possible and hopefully provide a useful guide to those new to the field.
Probabilistic programming, and machine learning more generally, draws
researchers from an exceptionally diverse pool of backgrounds and what is rudimentary
to, for example, the statistics community is often bewildering to the programming
languages community and vice-versa. Secondly, because probabilistic programing, and 
in particular the advanced content of this thesis, goes beyond conventional notions of
Bayesian modeling, a thorough understanding of the core concepts is essential.
Thirdly, it is the authors' 
opinion that because machine learning moves so quickly, or perhaps simply because
of the nature of research itself, one tends to become so fixated on a small area of
research that the big picture becomes lost.  For good research it is important to not
only question ones contribution to a particular sub-field, but also the justification for
that subfield itself.  With the current hype surrounding machine learning and, in particular,
deep learning\footnote{Pronounced \emph{neu-ral net-works}}, returning to these
fundamental principles and core concepts is more important than ever so that we know 
what the right approach for a particular problem is and realistically know what we can
hope to achieve with machine learning.

The breakdown between the pedagogical and research elements of this thesis are as follows.
Chapters~\ref{chp:prob},~\ref{chp:bayes}, and~\ref{chp:inf} are all explicitly pedagogical
and cover fundamentals that should be understood in depth by anybody researching or
applying Bayesian modeling and arguably machine learning more generally.  Though
Chapters~\ref{chp:prob} is very much covering the absolute basics,
even seasoned researchers should hopefully find nuggets of unseen or forgotten information 
in Chapters~\ref{chp:bayes}, and~\ref{chp:inf}.  Chapter~\ref{chp:probprog} provides an
 introduction to
probabilistic programming from the perspective of writing models.  Though its earlier
sections are still predominantly pedagogical (albeit for what is still a relatively niche field),
much of the core notation for the rest of the thesis is provided in its latter sections, some of
which is somewhat distinct to that conventionally used in the literature.
Chapter~\ref{chp:part} considers a more advanced class of Bayesian inference methods,
namely sequential Monte Carlo~\citep{doucet2009tutorial} and particle Markov chain Monte
Carlo methods~\citep{andrieu2010particle},
building up to our recent contribution to the literature~\citep{rainforth2016interacting}.
Chapter~\ref{chp:probprog} though not explicitly new research, still constitutes
relatively advanced and cutting-edge material in the form of the intricate workings
of a probabilistic programming system back-end.   Chapter~\ref{chp:opt} is a return
to slightly more pedagogical material in the form of an introduction to Bayesian optimization.
Chapters~\ref{chp:bopp},~\ref{chp:nest}, and~\ref{chp:design} all outline new
contributions to the literature both as presented in recent publications of
ours~\citep{rainforth2016nips,rainforth2017pitfalls,vincent2017darc} and newly introduced
material for this thesis.