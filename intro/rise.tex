% !TEX root = ../main.tex

How come a dog is able to catch a
frisbee in mid-air? How come a batsman can instinctively predict the flight of a cricket ball
moving at over $100$km/h sufficiently accurately and quickly to hit it with the bat when there
is not even time to consciously make a prediction?  Clearly, neither can be based on a
deep explicit knowledge of the laws of physics or some hard-coded model for the movement
of objects; we are not even born with the knowledge that unsupported objects will not 
fall down~\citep{baillargeon2002acquisition}.  The only reasonable explanation for these
abilities is that the batsmen and the dog have \emph{learned from experience}.  We do not have
all the knowledge we require to survive from birth, but we are born with the
ability to learn and adapt, making observations about the world around us and using these
to refine our cognitive models for everything from the laws of physics to social interaction.
Classically the scientific method has relied on human interpretation of the world to formulate
explicit models to explain our internal intuitions, which we then test
through experimentation.  However, even as a whole scientific society, our models
are often terribly inferior to the subconscious models of animals and children, such as for most
tasks revolving around social interaction.  This leads one to ask, is there something fundamentally
wrong with this hand-crafted modeling approach?  Is there another approach that better mimics the
way humans themselves learn?

\emph{Machine learning} is an appealing alternative, and often complementary,
approach that focuses on constructing algorithms and systems that can adapt, or learn, from data in order
to make predictions that have not been explicitly programmed.
This is exciting not only because of the potential it brings
to automate and improve a wide array of computational tasks, but because it allows us to design 
systems capable of going beyond the boundaries of human understanding, reasoning
about and making predictions for tasks we cannot solve directly ourselves.
As a field, machine learning is very wide ranging,
straddling computer science, statistics, engineering, and beyond. 
It is perhaps most closely related to the field of computational statistics, differing predominantly
in its emphasis on prediction rather than understanding.  Despite the current hype around the
field, most of the core ideas have existed for some
time, often under the guise of pattern recognition, artificial intelligence, or computational 
statistics.  Nonetheless, the explosion in the availability of data and in computational processing power in recent
years has led to a surge of interest in machine learning by academia and industry alike, particularly 
in its application to real world problems.  This interest alone is enough to forgive the hype as the
spotlight is not only driving the machine learning community itself forward, but helping identify
huge numbers of applications where existing techniques can be transferred to fantastic effect.
From autonomous vehicles~\citep{lefevre2014survey}, to speech recognition \citep{jurafsky2014speech},
and designing new drugs \citep{burbidge2001drug}, machine learning is rapidly becoming a 
crucial component in many technological and scientific advancements.  

%From autonomous vehicles~\citep{lefevre2014survey}, to speech recognition \citep{jurafsky2014speech},
%and designing new drugs \citep{burbidge2001drug}, machine learning is rapidly becoming a 
%crucial component in many technological and scientific advancements.  
%At its heart, it is the study of algorithms which can adapt, or learn, from data in order
%to make predictions that have not been explicitly programmed by the user.  It is a wide ranging field,
%straddling computer science, statistics, engineering, and beyond. Though it has existed for some
%time, often under the guise of pattern recognition, artificial intelligence, or computational 
%statistics, the explosion in the availability of data and in computational processing power in recent
%years has led to a surge of interest in machine learning by academia and industry alike, particularly 
%in its application to real world problems.  This is exciting not only because of the potential it brings
%to automate and improve a wide array of computational tasks, but because it allows us to design 
%systems capable of going beyond the boundaries of human understanding and prediction, reasoning
%about and making predictions for tasks we cannot solve directly ourselves.

In many machine learning applications, it is essential to use a principled \emph{probabilistic} 
approach \cite{ghahramani2015probabilistic}, incorporating uncertainty and utilizing all the information at hand, 
particularly when data is scarce.  The \emph{Bayesian paradigm} provides an excellent basis upon which to do this: an area 
specialist constructs a probabilistic model for data generation, conditions this on the actual observations received, 
and, using Bayes' rule, receives an updated model incorporating this information.  This
allows information from both existing expertise and data to be combined in a statistically
rigorous fashion.  As such, it allows us to use machine learning to complement the conventional
scientific approach, rather than directly replacing it: we can construct models in a similar way to
that which is already done, but then improve and refine these models using data.

Unfortunately, there are two key stumbling blocks that often make it difficult for this idealized
view of the Bayesian machine learning approach to be realized in practice.  Firstly, a process 
known as \emph{Bayesian inference} is
required to solve the specified problems.  This is typically a challenging task, closely 
related to integration, which is often computationally intensive to solve.  Furthermore, it often
requires significant statistical expertise to implement effectively, creating a substantial barrier to
entry.  
Secondly, it can be challenging to specify models that are true to the assumptions the user
wishes to make and the prior information available.  It can again require statistical expertise to abstract
application specific knowledge to a valid statistical model.  Furthermore, assumptions are often made in the interest
of the tractability of inference, rather than the fidelity of the model.  Perhaps because of these drawbacks, there is often
a reliance on off-the-shelf solutions, even when these models are somewhat inappropriate for the task
at hand.

%Similar scenarios of limited data are prevalent through all areas of research; medical trials, materials testing, drug discovery, design of robotic movement and engineering design, to name but a few.  Indeed, the prevalence of such problems is so large, that the problem in solving them is arguably more one of expertise than appropriate techniques.  Bayesian approaches offer a powerful means to mathematically formalize these problems, but the required expertise to effectively solve the resulting inference can be prohibitive, or at least problematic and time consuming, particularly for those from less mathematical backgrounds.

Probabilistic programming systems (PPS) \citep{goodman2008church} are an attempt to
overcome this dichotomy between the Bayesian ideal and common practice.  Their core philosophy 
is to decouple model specification and inference, the former corresponding to the user-specified 
program code, composing of a generative model and statements for conditioning on data, and the 
latter to an inference engine capable of operating on arbitrary programs.  This abstraction barrier 
allows users with domain specific knowledge to write models naturally, as if they were writing a 
simulator, without worrying about the inference, which becomes the job of the developer. Informally 
one can think of PPS as operating as inverse probability engines, outputting the conditional 
probability distribution implied by the generative model coupled with the observed data.
Removing the need for users to worry about the required inference significantly reduces the 
burden of developing new models, and makes effective statistical methods accessible to 
non-experts.  From a developer's perspective, the abstraction can also aid in the design 
and testing of new inference algorithms.  Furthermore, the availability of the target source 
code, with known semantics, opens up many opportunities for new methods that would
not otherwise be possible.

The underlying theme of this thesis is improving and extending probabilistic programs.  However,
doing this will require us to make advancements in a number of different research areas such
as particle Markov chain Monte Carlo methods~\citep{andrieu2010particle,rainforth2016interacting},
Bayesian optimization~\citep{movckus1975bayesian,rainforth2016bayesian}, and \mc
fundamentals~\cite{metropolis1949monte,rainforth2016pitfalls}.  Our aim is to both improve
the performance of PPS by improving the underlying inference engines, increasing
efficiency and the range of models for which inference can be tractably provided for, and to increase
the scope of problems which can be covered by PPS more generally.  In particular, we
will extend PPS beyond the standard Bayesian inference setting to more general frameworks
such as marginal maximum a posteriori estimation and nested estimation.  This opens up a number
of fascinating opportunities, such as the prospect of constructing systems for automating the
design of adaptive experiments and for carrying out principled, probabilistic, engineering simulations
that explicitly incorporate the uncertainty in the task at hand.

\todo[inline]{Build on this a bit so that is a full motivation section.  In particular, could do a better
	job of linking it back to the real world.  Could also do more of a linking of my work to the aims, 
	rather than just probabilistic programming in general.}