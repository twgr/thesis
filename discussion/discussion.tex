% !TEX root = ../main.tex

\chapter{Challenges, Criticisms, and Future Directions}
\label{chp:discussion}

\begin{itemize}
	\item Relationships with ABC
	\item Reinvention of priors in deep learning by generating data
	\item Is amortization just a learning a different decomposition of the joint 
	$p(\theta | \mathcal{D}_1)p(\mathcal{D}_2|\theta)$ that shifts more to the
	prior to make inference easier?  Does it actually implicitly define a different
	joint distribution that is otherwise difficult to express or is it just proposal
	adaptation as Tuan Anh's paper says.
\end{itemize}