% !TEX root =  ../main.tex

\section{Implications For Nesting Probabilistic Programs}
\label{sec:nest:imp}

Given our results, it is now natural to ask: what are the implications for nesting probabilistic programming queries?
In particular, when is doing
this valid and are there any precautions we can take to avoid problems?  Before
we get to these question though, we first need to explicitly define what we
mean by a nested query.  To this end, we distinguish between nested calling
structures (such as the example given in Figure~\ref{fig:probprog:schell}) and
queries that do not represent a single estimation, which we will refer
to as nested queries. Our motivation for defining only the latter
as nested is that cases of the former can always be represented as a single
query, because they still represent a single expectation and define models
for which the unnormalized target distribution~\eqref{eq:probprog:universal-cond}
 can be evaluated exactly.  Therefore, although these models are
of clear importance, they are not a fundamentally different problem class
to standard Bayesian inference problems and so we assert that existing \mc
converge results directly apply.  However, it can be deceptively difficult to establish
whether a model requires query nesting or just nested calling.
For example, the Schelling coordination example we gave in Figure~\ref{fig:probprog:schell}
is clearly an example of a nested calling structure as it is explicitly a single query,
while the original version of this problem~\cite[Figure 6]{stuhlmuller2014reasoning}
is actually an (equivalent!) example of a true nested query.  It turns out that this
constitutes one of our special cases that can be reformulated to a single estimate (namely
no random variables are passed \emph{to} the nested query).
In the rest of this section we will delineate possible ways one might want
to nested probabilistic program queries and
assert their respective correctness (or lack thereof) and required conditions.
Namely, we will consider \emph{sampling} from the conditional distribution
of another query, \emph{observing} another query, and using \emph{estimates as variables}
in another query.  We will further identify some special cases which can be rearranged to single estimators or
provide Monte Carlo convergence rates even when expressed as nested queries.

\subsection{Nested Sampling}
\label{sec:nest:imp:sampling}

One of the clearest ways one might want to nest queries is by sampling from the conditional
distribution of one query inside another query.  Due to the ``sample then check'' setup of
Church, all of the examples given in~\cite{stuhlmuller2014reasoning} are by proxy this
type of query nesting,\footnote{Even though the nested calls are made inside the conditioning
	predicate for these examples, the semantics of the language is to sample these
	and then check if the predicate holds as a hard constraint.  This is equivalently to doing a nested
	sampling in the program and then evaluating a deterministic predicate using our PPL notation.}
as is any usage of the \conditional predicate in Anglican.\footnote{This is because 
	\lsi{conditional} is only currently supported for use
with \lsi{sample} statements and inference algorithms that do not required the density to be evaluated.}
Such problems fall under a more general framework of \emph{nested inference}~\cite{mantadelis2011nesting} or equivalently inference for so-called
\emph{doubly intractable} distributions~\citep{murray2006mcmc} (or multiply intractable
	distributions for repeated nesting~\citep{stuhlmuller2014reasoning}).
	 The key feature of these problems is that they include
terms with unknown, \emph{parameter dependent}, normalization constants.  For nested probabilistic programming
queries, this is manifested in \emph{conditional normalization} within a query.
Consider as an example the following model using a nested calling structure
\vspace{-15pt}
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize,multicols=2,frame=none]
(defm inner [y D]
 (let [z (sample (gamma y 1))]
  (observe (normal z y) D)
  z))

(defquery outer [D]
 (let [y (sample (beta 2 3))
       z (inner y D)]
  (* y z)))
\end{lstlisting}
\vspace{-25pt}
compared to a following nested query model
\vspace{-15pt}
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize,multicols=2,frame=none]
(defquery inner [y D]
 (let [z (sample (gamma y 1))]
  (observe (normal z y) D)
  z))
  
(defquery outer [D]
 (let [y (sample (beta 2 3))
       dist (conditional inner)
       z (sample (dist y D))]
  (* y z)))
\end{lstlisting}
\vspace{-15pt}
where we have replaced the \defm with a query nesting.  The unnormalized distribution on
traces for the first model is straightforwardly given by
\[
\gamma_1(z,y,D) = p(y)p(z|y)p(D|y,z) = \textsc{Beta}(y;2,3) \textsc{Gamma}(z;y,1) \mathcal{N}(D;z,y^2),
\]
for which we can clearly use conventional Monte Carlo inference schemes.  The second model
is only subtly, but critically different in that $p(z|y)p(D|y,z)$ is conditionally normalized 
before being used in the outer query
\[
\gamma_2(z,y,D) = p(y)p(z|y,D) = p(y)\frac{p(z|y)p(D|y,z)}{\int p(z|y)p(D|y,z)dz} = p(y)\frac{p(z|y)p(D|y,z)}{p(D|y)} \neq \gamma_1(z,y,D).
\]
The key point here is that the partial normalization constant $p(D|y)$ depends on $y$ and so 
$\gamma_2(z,y,D)$ is doubly intractable because we cannot evaluate our unnormalized target
distribution exactly.  By normalizing $p(z|y)p(D|y,z)$ within the outer query, 
 \conditional has changed the distribution defined by the program.  Another interesting way of looking at this
 is that wrapping \clj{inner} in \conditional has ``protected'' $y$ from the conditioning 
 in \clj{inner} -- noting
 that $\gamma_1 (z,y,D) \propto p(y|D)p(z|y,D)$ -- such that the \observe statement only effects the probability
 of $z$ given $y$ and not the probability of $y$.
 
 Unfortunately the inverse function $f(x) = 1/x$ constitutes
a nonlinear mapping and so expectations with respect to nested queries defined in this way using
\conditional will generally require nested estimation schemes unless we can find a means of
sampling from $p(z|y,D)$ exactly with finite computational budget.
The obvious question is therefore what behavior do we need for \conditional, or query nesting through sampling
more generally, in order to ensure consistency?  

One possible approach would be to attempt to
use an exact sampling method for sampling from \conditional. Presuming we cannot
do this analytically~\citep{cornish2017efficient}, the most obvious way to generate such exact
samples is using rejection sampling (see Section~\ref{sec:inf:foundation:rejection}), as done in
Church.\footnote{Note, though, that this
is not the only way to generate exact samples from an unnormalized target distribution (see e.g. \citep{craiu2011perfection}).}
However, even this opens up a minefield of theoretical
and practical questions.  For example, because the acceptance rate of an inner query might depend on the
value of the input variable, it would presumably not be valid to generate all the variables at once
and then only accept the sample if every condition is satisfied.
Church instead takes an approach whereby no sample ever returns until it passes its local acceptance criterion.
Although doing this for soft conditioning in a automated way might be insurmountably problematic (because of the
need to choose a valid $C$ in~\eqref{eq:inf:rej-acc-criteria}), it does open up the intriguing prospect
of providing a standard Monte Carlo convergence rate for a set of inherently nested problems.  This is because although the performance clearly
gets exponentially worse with increased nesting (as the probability of a successful return is the
product of an increasing number of expected acceptance rates), this is a change in the constant factor of the
computation, not its scaling with the number of samples taken.  This is perhaps easiest to see by noting that 
generating a single exact sample of the distribution
can be done in finite time using rejection sampling (and thus still converges with a fixed $N_1,\dots,N_D$), 
whereas in general NMC we had that $N_1,\dots,N_D$ need to increase as $N_0$ increases for convergence.
If correct, this assertion would be a somewhat startling result: it suggests that Monte Carlo 
estimates made using nested rejection sampling have a fundamentally different
convergence rate for nested inference problems (though not nested estimation problems more generally) than, say,
nested self-normalized importance sampling (SNIS).  Amongst other things, this might lead, 
perhaps through combination with 
the work of~\cite{moller2006efficient}, to a contradiction of the, ostensibly very reasonable, conjecture
made by~\cite{murray2004bayesian} that there are no generic tractable MCMC schemes for doubly intractable distributions.
It is beyond the scope of the work to analyze this hypothesis more formally, or the rabbit hole of
nested exact sampling methods more generally, but we highlight that this constitutes a potentially fascinating line of inquiry.

%From a practical perspective, it becomes much easier to write programs which will never terminate
%as, using the notation from our previous example, then for all possible $y$, there must be some $z$
%for which $p(D|y,z)\neq0$ (or for which the condition holds true in the context of Church).

%WRONG!
%Imagine we take
%a rejection sampling approach whereby no sample ever returns until
%it passes the acceptance criterion (which seems to be the approach taken by Church).  One clear issues this throws up is that it might be the case
%that the inner query has no valid outputs for a particular input.  Using the notation from our earlier example, we
%might have that $p(z|y,D) = 0, \forall z$ for a particular value of $y$, e.g. if $p(D|y,z) = \mathbb{I}(|y|<1 \cap |z|<1)$
%then any $|y|\ge1$ outside this range has zero probability.  
%Our outer distribution here $\gamma_2(z,y,D)$
%is still well defined, it just happens to be zero unless $|y|<1$.  Thus our depth-first rejection 
%strategy could get permanently stuck for a fundamentally computable distribution.  On the other hand,
%we could instead 

Without access to exact sampling methods, it is straightforward to see 
that asymptotic bias (i.e. bias even if we take
an infinite number of samples from the outer query) is, in general, unavoidable for
query nesting if each call of \conditional only carries out finite computation.  
This follows from our general NMC results and
the fact that normalization constitutes a nonlinear mapping.
For example, imagine we
use SNIS to generate outputs from \conditional
with a fixed number of internal samples proposed for each given as output.  As we showed in 
Section~\ref{sec:inf:foundation:importance:self-norm}, expectations with respect to these samples will be
biased, which we can think of in the NMC context as occurring because the inversion of the unbiased marginal likelihood estimate
induces a bias.  In addition to the possibility that we might be able to get around this
problem using exact sampling as discussed in the last paragraph, it might also be possible to
use debiasing techniques to remove the particular bias that is generated, see e.g.~\cite{glynn2014exact,jacob2017smoothing}.

One special case where consistency can be maintained, even without rearrangement,
 is if the inner query takes no 
random variables as inputs, as is the case for
the Schelling coordinate example in~\cite[Figure 6]{stuhlmuller2014reasoning}.  
A condition for this is that we use a method that provides an unbiased estimate of 
the marginal likelihood (e.g. importance sampling or SMC) and weight the trace appropriately
with the product of this marginal likelihood estimate and the internal self-normalized weight
of the returned sample.  
If this conditioned is maintained, we can demonstrate the validity of the approach by noting that
the normalization constant of the nested query is not parameter dependent, i.e. $p(D|y)=p(D)$ 
using the notation from our earlier example.  It can therefore 
be factored into that of the outer query as the nested query now defines the same distribution 
as if conditional query were replaced with a \defm.  We can therefore actually view inference 
on such problems as taking a pseudo-marginal approach, provided that all samples returned from
conditional are properly weighted~\citep{naessethLS2015nested} as previously described.
The correctness of this approach then clearly follows from a combination of
Theorems~\ref{the:finite-res} and \ref{the:prod}.
However, Anglican's current use of \conditional does not
satisfy our requirement as it only returns unweighted samples.\footnote{Unfortunately, this
	means that, though still practically potentially useful, Anglican's current usage of {\footnotesize \lsi{conditional}}
	for nesting never provides consistent estimates unless the inner query contains no conditioning, 
	in which
	case its behavior is equivalent to \lsi{defm} anyway.  It should be noted, though, that no claims 
	to the contrary have ever been made, with \lsi{conditional} still very much a developmental feature
	of the language.}
Therefore, for Anglican to provide consistent estimates in such situations, we would need to
change the probabilistic semantics of \conditional, such that it
both produces a sample and simultaneously weights the trace using the unnormalized weight of the returned particle in the inner query.  In other words, it would need to operate in a similar fashion
to the following simplified code
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize,frame=none]
(defn conditional-sample [inner-query inputs inf-alg M inf-opts]
 (let [samp (nth M (doquery inf-alg inner-query inputs inf-opts))]
  (observe (factor) (get-log-weight samp))
  (:result samp)))
\end{lstlisting}
\vspace{-10pt}
where we first run inference with budget \clj{M} to produce a sample, then weight the program trace
using the (unnormalized) sample weight, and finally return the value of the sample.
Remember though that this only addresses the special case where \clj{input} is not a random variable
and \clj{inf-alg} is an inference algorithm producing properly weighted samples.

Another special case is problems where the variables passed to the inner query can only take on, say $C$, finite
possible values. As
explained in Section~\ref{sec:linear_case}, such problems can always be rearranged to
$C$ separate estimators such that the standard Monte Carlo error rate can be achieved.
For repeated nesting, this rearrangement can be recursively applied until one achieves
a complete set of non-nested estimators.  This special case is one which requires rearrangement
or a specialist consideration by the language back-end (as done by \cite{stuhlmuller2012dynamic,stuhlmuller2014reasoning,cornish2017efficient}),
with na\"{i}ve implementations leading to NMC convergence rates.  It emphasizes that
nested problems with continuous variables are a fundamentally different problem
class to those with only discrete bounded variables (or only continuous variables at the bottom layer)
and we must therefore be wary of presuming that results from the discrete case carry over.
Nonetheless, in the same way that it is often necessary to use Monte Carlo rather than enumeration
for inference in discrete problems because there are too many possible outcomes to enumerate, 
it will still often be necessary from a practical perspective
to use NMC for discrete problems. Ideally one should then do this in a manner that identifies and exploits the fact that
the inner estimator is often called multiple times with the same inputs such that the samples
from these calling can be combined (e.g. as done by~\cite{stuhlmuller2012dynamic}).

\subsection{Nested Observation}
\label{sec:nest:imp:obs}

An alternative way one might wish to nest queries is through nested observation, whereby we
observe the output of another query rather than sampling from it.  For hard-constraints such
as used in Church, this is equivalent to query nesting through sampling.  However, for soft
constraints it can be substantially different.  The distinction is the same as that between
\sample and \observe~-- we are observing an outcome of a program, rather than sampling
from it.  As \observe returns no outputs, the mathematical interpretation of observing 
the output of another query is that we are weighting the outer program trace by the
partition function estimate of the inner query.  Presuming that our partition function estimates
are unbiased (which they will be if we use, say, SMC for the inner estimator), then
this corresponds to a particular instance of the products of expectations special case
covered by Theorem~\ref{the:prod}.  It, therefore, immediately follows that if
the outer estimator uses importance sampling, then such an approach is consistent.  More
generally, this case corresponds to a pseudo-marginal approach where the samples
from the inner estimator are never returned.  We can, therefore, use existing results
to demonstrate the consistency of the approach for other choices of outer estimator.
For example, consistency when using SMC in the outer estimator follows directly
from a combination of Theorem~\ref{the:prod} and the results of~\cite{naessethLS2015nested}.
It should also be possible to construct consistent approaches using MCMC samplers
at the top level as per~\cite{andrieu2009pseudo}, but this would require more
careful consideration.

In addition to the statistical complications just discussed,
there are some significant semantic complications associated with observing
another query: there are actually two ways we might want to ``observe'' a nested query.
The first is simply to run the nested query unmodified and factor the trace probability
by the marginal likelihood estimate.  This is a perfectly reasonable thing to want to do,
allowing us to, for example, construct a PMMH sampler (see
Section~\ref{sec:part:pmcmc:global}).  In fact, this was exactly what we did alongside
some manual code transformations to
construct the PMMH benchmarks for BOPP in Section~\ref{sec:bopp:exp}.  However,
this is not really observing the output of the inner query: it is instead a means of
replacing an analytic likelihood term with an unbiased estimate.  The other way we might thus
want to observe a query is to estimate the likelihood of it producing a particular output.
Unfortunately, in general, this will be infeasible (at least without code analysis) 
because the density of the output variables is affected by deterministic computations in the query
(see discussion of program outputs in Section~\ref{sec:probprog:models:first}).  
This is identical to the
problem we encountered for BOPP, where we could not optimize with respect to arbitrary
variables in the program because of not being able to calculate their density.
This now presents an interesting possibility -- we could use exactly the same
marginal transformation as in BOPP, but then use this to estimate the partition
function with certain variables ``clamped''.  We can actually think of BOPP as being
a nested estimation scheme where the outer estimator is an optimizer.  Therefore, we
could use the same framework as BOPP but replace the BO optimizer
with an estimation scheme, i.e. the outer query.  In fact, if desired, we could even 
maintain the same usage of the GP surrogate in BOPP to, for example, carry out estimation
using Bayesian quadrature~\citep{osborne2012active} or the scheme laid out
by~\citep{gutmann2016bayesian}.

\begin{figure}[t]
			\centering	
			\vspace{-10pt}		
			\rule{\linewidth}{0.4pt}
			\vspace{-28pt}		
	\begin{lstlisting}[basicstyle=\ttfamily\footnotesize,multicols=2,frame=none]
(defm prior [] (normal 0 1))
(defm lik [theta d] (normal theta d))

(defquery inner-q [y d]
 (let [theta (sample (prior))]
  (observe (lik theta d) y)))

(defn inner-E [y d M]
 (->> (doquery :importance 
         inner-q [y d])
      (take M)
      log-marginal))
      
(with-primitive-procedures [inner-E]
 (defquery outer-q [d M]
  (let [theta (sample (prior))
        y (sample (lik theta d))
        log-lik (observe* 
                  (lik theta d) y)
        log-marg (inner-E y d M)]
   (- log-lik log-marg))))

(defn outer-E [d M N]
 (->> (doquery :importance 
         outer-q [d M])
      (take N)
      (map :result)
      mean))
	\end{lstlisting}
	\vspace{-20pt}		
	\rule{\linewidth}{0.4pt}
	%\vspace{5pt}
	\caption{Anglican code for Bayesian experimental design corresponding to the estimator
		given in~\eqref{eq:exp-des-nmc}.  By changing the definitions of \clj{prior}
		 and \clj{lik}, this code can be used as a
		 NMC estimator (consistent as $N,M\rightarrow\infty$) for any static
		 Bayesian experimental design problem.
		 Here \lsi{observe*} is a function for returning the log likelihood (it does not
		 effect the trace probability), \lsi{log-marginal} produces a marginal likelihood estimated
		 from a collection of weighted samples,
		 and \lsi{->>} successively applies a series of functions calls,
		 using the result of one as the last input the next.  When \lsi{outer-E} is invoked,
		 this runs importance sampling on \lsi{outer-q}, which, in additional to carrying out
		 its own computation, calls \lsi{inner-E}.  This in turn invokes another inference over
		 \lsi{inner-q}, such that a \mc estimate using \lsi{M} samples is constructed for
		 each sample of \lsi{outer-q}.  Thus \lsi{log-marg} is \mc estimate itself.
		 The final return is the (weighted) empirical mean for
		 the outputs of \lsi{outer-q}.
	\label{fig:nest:exp}}
\vspace{-5pt}
\end{figure}

\subsection{Estimates as Variables}
\label{sec:nest:imp:expt}

The third and final way we consider using nested estimation in probabilistic programming
is to use expectation estimates as first class variables.  In our previous examples,
nonlinear usage of expectations only ever manifested through inversion of the normalization
constant.  These methods are therefore clearly insufficient for expressing more general
nested estimation problems as would be required by, for example, experimental design.
Anglican does not explicitly support such behavior inside pure Anglican code, but 
one can still enforce such
behavior by either calling \doquery inside a \defdist deceleration or in a \defn function
passed to a query using \clj{with-primitive-procedures}.  An example of this is shown
in Figure~\ref{fig:nest:exp} corresponding to a program for carrying out Bayesian experimental
design (see Chapter~\ref{chp:design}).  The validity of this ``estimates as variables'' 
approach is effectively
equivalent to that of NMC more generally (as this effectively allows any nested estimation
problem to be defined) and needs to be considered on a case-by-case basis as per
the general guidelines we have provided throughout this chapter.

%We have shown that it is theoretically possible for a nested Monte Carlo scheme to yield a
%consistent estimator, and have quantified the convergence error associated with doing so.
%However, we have also revealed a number of pitfalls that can arise if nesting is applied
%na\"{i}vely, such as the resulting estimator becoming necessarily biased, requiring additional
%assumptions on $f$, being unlikely to converge unless the number of samples used in the inner
%estimator is driven to infinity,
%and is likely to converge at a significantly slower rate than un-nested Monte
%Carlo. These results have implications for applications ranging from experimental design
%to probabilistic programming, and serve both as an invitation for further inquiry and a
%caveat against careless use.

%We have shown that although consistent nested inference is still possible, it is inherently biased, requires Lipshitz continuity, and has a convergence rate that reduces exponentially in the nesting depth.  
%These results have implications for many applications such as experimental design and probabilistic programming.
%For the latter, it shows that when there is only a linear dependence on the nested query, the problem can be unravelled to a single inference, but that otherwise there are additional severe restrictions on the problems that can be solved and the performance that can be achieved.
%When there is only a linear dependence of the outer integration on the nested expectation, the problem can be unravelled to a single inference, therefore, although nested queries in probabilistic programs do increase the scope of models which can be defined, these models have continuity restrictions and only permit \mc inference at prohibitively slower convergence rates than ordinary inference.
