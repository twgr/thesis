% !TEX root =  ../main.tex

\subsection{Proof of Theorem~\ref{the:finite-res} - Convergence Rate for Finite Realisations of $y$}
\label{sec:app:finite-res}

%\begin{theorem}
%	If $f$ is Lipschitz continuous, then the mean squared error of 
%	\[
%	I_N = \sum_{c=1}^C (\hat{P}_N)_c \, (\hat{f}_N)_c
%	\]
%	as an estimator for $I$ converges at rate $O(1/N)$.
%\end{theorem}
\thefiniteres*

\begin{proof}
	Denote
	$P_c = P(y = y_c)$ and
	$f_c = f(y_c, \gamma(y_c))$
	noting that as the $y_c$ are fixed values, so are $P_c$ and $f_c$.
	Then, Minkowski's inequality allows us to bound the mean squared error as
	\begin{eqnarray*}
		\E\left[(I_N - I)^2\right] = \norm{I_N - I}_2^2
		\leq \left(\sum_{c=1}^C W_c \right)^2 \quad \text{where} \quad W_c := \norm{(\hat{P}_N)_c \, (\hat{f}_N)_c - P_c \, f_c}_2.
	\end{eqnarray*}
	Moreover, again by Minkowski, we have $W_c \leq U_c + V_c$
	where 
	\[
	U_c = \norm{(\hat{P}_N)_c \, (\hat{f}_N)_c - (\hat{P}_N)_c \, f_c}_2, \quad
	V_c = \norm{(\hat{P}_N)_c \, f_c - P_c \, f_c}_2.
	\]
	Factoring out $(\hat{P}_N)_c$ in $U_c$ and noting that each $y_n$ and $z_{n,c}$ are sampled independently gives
	\begin{eqnarray*}
		U_c = \sqrt{\E\left[(\hat{P}_N)_c^2 \, \left((\hat{f}_N)_c - f_c\right)^2\right]} 
		= \sqrt{\E\left[(\hat{P}_N)_c^2\right]} \sqrt{\E\left[\left((\hat{f}_N)_c - f_c\right)^2\right]}.
	\end{eqnarray*}
	Using Minkowski's
	inequality, we may write the first right-hand term as
	\begin{eqnarray*}
		\sqrt{\E\left[(\hat{P}_N)_c^2\right]} = \norm{(\hat{P}_N)_c}_2
		&\leq& \frac{1}{N} \sum_{n=1}^N \norm{\mathbbm{1}(y_n = y_c)}_2 \\
		&=&\frac{1}{N} \sum_{n=1}^N \E \left[{\mathbbm{1}(y_n = y_c)}^2\right] 
		= \frac{1}{N} \sum_{n=1}^N P_c
		= P_c.
	\end{eqnarray*}
	For the second term, note that by Lipschitz continuity, we have for some constant $K >
	0$
	\begin{eqnarray*}
		\sqrt{\E\left[\left((\hat{f}_N)_c - f_c\right)^2\right]} = \norm{(\hat{f}_N)_c - f_c}_2 &\leq& 
		K \, \norm{\frac{1}{N} \sum_{n=1}^N \phi(y_c, z_{n,c}) - \gamma(y_c)}_2  \\	
		&=& K \cdot O(1/\sqrt{N}) 
		= O(1/\sqrt{N}),
	\end{eqnarray*}
	since $\frac{1}{N} \sum_{n=1}^N \phi(y_c, z_{n,c})$ is a Monte Carlo estimator for
	$\gamma(y_c)$. Altogether then, we have that
	\[
	U_c = P_c \cdot O(1 / \sqrt{N}) = O(1 / \sqrt{N}).
	\]
	We can also factor out $f_c$ in $V_c$ to obtain
	\[
	V_c = |f_c| \cdot \norm{(\hat{P}_N)_c - P_c}_2 = |f_c| \cdot O(1/\sqrt{N}) = O(1/\sqrt{N}),
	\]
	since $(\hat{P}_N)_c$ is a Monte Carlo estimator for $P_c$.
	Now by noting that 
	$(A+B)^2 \le 2(A^2+B^2)$
	for any $A, B \in \mathbb{R}$, an inductive arguments shows that 
	\[
	\left(\sum_{\ell=1}^L A_\ell\right)^2 \leq 2^{\lceil \log_2 L \rceil} \sum_{\ell=1}^L A_\ell^2
	\]
	for all $A_1, \cdots, A_L \in \mathbb{R}$.  We can now show that
	our asymptotic bounds for $U_c$ and $V_c$ entail that our overall mean squared
	error satisfies
	\begin{eqnarray}
	\E\left[(I_N - I)^2\right] &\leq& 2^{\lceil \log_2 C \rceil} \sum_{c=1}^C W_c^2 \label{eq:square-inequal-1} \notag
	\leq 2^{\lceil \log_2 C \rceil} \sum_{c=1}^C (U_c + V_c)^2 \notag
	\leq 2^{\lceil \log_2 C \rceil + 1} \sum_{c=1}^C U_c^2 + V_c^2 \label{eq:square-inequal-2} \notag \\
	&=& 2^{\lceil \log_2 C \rceil + 1} \sum_{c=1}^C O(1/N) + O(1/N) \notag
	= O(1/N), \notag
	\end{eqnarray}
	as desired.
\end{proof}