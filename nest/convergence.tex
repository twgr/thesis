% !TEX root =  main.tex

\section{Convergence of Nested Monte Carlo}
\label{sec:convergence}

Since we cannot always unravel the target estimation using one of the special cases in the previous section, we must resort to NMC (or another nested estimation scheme) in order to compute $I$ in general. 
Our aim here is to show that
approximating $I \approx I_{N,M}$ is in principle possible, at least when $f$ is
well-behaved. In particular, we establish a convergence rate of 
the mean squared error of $I_{N,M}$ and prove a form of almost sure convergence to
$I$.  We further generalize our convergence rate to apply to the case of multiple
levels of estimator nesting.

\begin{wrapfigure}{r}{0.42\textwidth}
	%\vspace{-12pt}
	\centering 
	\resizebox{.4\textwidth}{!}{
		\input{nest/Figures/algorithm.tex}
	}
	\caption{Convergence representation \label{fig:conv-rep}}
	\vspace{-10pt}
\end{wrapfigure}

%\subsection{Strong Consistency}

Before providing a formal examination of the convergence of NMC, we first provide more
intuition about how we might expect to construct a convergent NMC estimator.  Consider the
diagram shown in Figure~\ref{fig:conv-rep}, and suppose that we want our error to be
less than some arbitrary $\varepsilon$.  Assume that $f$ is sufficiently smooth 
that we can choose $M$ large enough to make
$\left|I-\E\left[f(y_n,(\hat{\gamma}_M)_n)\right]\right| < \varepsilon$
(we will characterize the exact requirements for this later).  For this fixed
$M$, we have a standard \mc estimator on an extended space $y,z_1,\dots,z_M$ such that each
sample constitutes one of the red boxes.  As we take $N\rightarrow \infty$, i.e. taking
all the samples in the green box, this estimator converges such that $I_{N,M}
\to \E\left[f(y_n,(\hat{\gamma}_M)_n)\right]$ as $N \to \infty$ for fixed $M$.  As we can
make $\varepsilon$ arbitrarily small, we can also achieve an arbitrarily small error.

Convergence bounds for NMC have previously been shown by~\citet{hong2009estimating} in the 
financial statistics literature.  Under the assumption that $\hat{\gamma}$ is Gaussian distributed
(which is often reasonable due to the central limit theorem) and that $f$ is thrice differentiable
other than at some finite number of points, they have shown that it is possible to achieve a
converge rate of $O(1/N+1/M^2)$.  We now show that these assumptions can be relaxed to only requiring
$f$ to be Lipschitz continuous, at the expense of weakening this bound to $O(1/N+1/M)$.
\vspace{-5pt}
\begin{restatable}{theorem}{theRate} \label{the:Rate}
	If $f$ is Lipschitz continuous and $f(y_n, \gamma(y_n)), \phi(y_n, z_{n,m}) \in
	L^2$, the mean squared error of $I_{N,M}$ converges to $0$ at rate $O\left(1/N +
	1/M\right)$.
\end{restatable}
\vspace{-12pt}
\begin{proof}
The Theorem follows as a special case of Theorem~\ref{the:Repeat} which
provides an exact form for this bound and a tighter bound
that holds when $f$ is continuously differentiable.  See also~\cite{rainforth2017pitfalls}
for a more accessible proof of this particular result.
\end{proof}
\vspace{-12pt}
\begin{remark}
This result can be carried over to cases where there are finite numbers of points for which $f$ is
not Lipschitz continuous by decomposing the estimator into separate truncated functions as
per~\cite{hong2009estimating}.
\end{remark}
\vspace{-8pt}
\noindent Inspection of the convergence rate above shows that, given a total number of samples
$T=MN$, our bound is tightest when $N\propto M$, with a
corresponding rate $O(1/\sqrt{T})$ (see~\cite{rainforth2017pitfalls}). 
When the additional assumptions of~\citet{hong2009estimating}
apply, this rate can be lowered to $O(1/T^{2/3})$ by setting $N \propto M^2$.  
We note that
the result of Theorem~\ref{the:Rate} was recently independently derived by~\citet{fort2016mcmc}
in work published shortly after our own~\citep{rainforth2016pitfalls}.
\citet{fort2016mcmc} also show that a $O(1/N+1/M^2)$ convergence rate can be achieved under
weaker assumptions than those of~\citet{hong2009estimating}, namely that $f$ is continuously differentiable,
and that these results hold when the $y_n$ are generated by a valid Markov chain, instead of being i.i.d..

We next consider the question of what is the minimal requirement on $f$ to ensures some form of
convergence? For a fixed single $y_1$, we
have that $(\hat{\gamma}_M)_1=\frac{1}{M}\sum_{m=1}^{M} \phi(y_1,z_{1,m})\rightarrow\gamma(y_1)$ 
almost surely as $M \rightarrow \infty$, because the left-hand side is a \mc estimator. If $f$ is continuous
around $y_1$, this also implies $f(y_1,(\hat{\gamma}_M)_1) \rightarrow
f(y_1,\gamma(y_1))$.  Our candidate requirement is that this holds in
expectation, i.e. that it holds when we incorporate the effect of the outer estimator.
More precisely, we define $(\epsilon_M)_n = \left|f(y_n, (\hat{\gamma}_M)_n) -
f(y_n,\gamma(y_n))\right|$ and require that $\E\left[(\epsilon_M)_1\right] \to 0$ as $M
\to \infty$ (noting that $(\epsilon_M)_n$ are i.i.d. and so
$\E\left[(\epsilon_M)_1\right] = \E\left[(\epsilon_M)_n\right], \forall n\in\N$). Informally, this ``expected continuity''
requirement is weaker than uniform continuity (and much weaker than Lipschitz continuity)
because it does allow (potentially infinitely
many) discontinuities in $f$.  More formally we have the following result.
\vspace{-8pt}
\begin{restatable}{theorem}{theConsistent} \label{the:Consistent}
	For $n \in \N$, let $
	(\epsilon_M)_n = \left|f(y_n, (\hat{\gamma}_M)_n) - f(y_n, \gamma(y_n))\right|.$
  Assume that $\E\left[(\epsilon_M)_1\right] \to 0$  as $M \to \infty$. Denote by $\Omega$
  the sample space of our underlying probability space, so that $I_{\tau_\delta(M),M}$ can
  be thought of as a map from $\Omega$ to $\mathbb{R}$. Then, for every $\delta > 0$,
  there exists a measurable $A_\delta \subseteq \Omega$ with $\mathbb{P}(A_\delta) <
  \delta$, and a function $\tau_\delta : \N \to \N$ such that, for all $\omega\not\in
  A_\delta$,
  \vspace{-4pt}
	\[ 
		I_{\tau_\delta(M),M}(\omega) \to I\quad\mbox{as}\quad M \to \infty.
	\]
\end{restatable}
\vspace{-14pt}
\begin{proof}
	See~\citep{rainforth2017pitfalls}.
\end{proof}
\noindent As well as providing proof of a different form of convergence to any existing results, this
result is particularly important because many, if not most, functions are not Lipschitz
continuous due to the their behavior in the limits.  For example, even the function $f(y,\gamma(y)) = \left(\gamma(y)\right)^2$
is not Lipschitz continuous because the derivative is unbounded as $\left|\gamma(y)\right|\rightarrow\infty$.
On the other hand, the vast majority of problems involving this $f$ will satisfy $\E\left[(\epsilon_M)_1\right] \to 0$.
%\begin{theorem} \label{the:Consistent}
%  For $n \in \N$, let 
%  \[
%          (\epsilon_M)_n = \left|f(y_n, (\hat{\gamma}_M)_n) - f(y_n, \gamma(y_n))\right|.
%  \]
%  If~~$\E\left[(\epsilon_M)_1\right] \to 0$  as $M \to \infty$, then
%  there exists a $\tau : \N \to \N$ such that $I_{\tau(M),M} \asto I$ as $M \to \infty$.
%\end{theorem}
%%\begin{proof}
%%  See Section~\ref{sec:app:conv-proof} in the Appendices. \todo{Add discussion
%%  characterising some instances of when $\epsilon_M \to 0$}
%%\end{proof}

%An important point to note is that as this convergence is in $M$, it suggests (and is reinforced by the convergence rate) 
%that in order to achieve convergence for most $f$,
%we should increase not just the number of samples in the outer estimator 
%but also the number of samples in the inner estimator.
%Theorem~\ref{the:Rate} gives an intuitive reason for why this should be the case;
%it says that if $M$ is fixed, the bias on each inner term will remain non-zero even when
%$N$ tends to $\infty$.
%
%\subsection{Convergence Rate}
%We now refine this by also establishing the convergence rate as below.
%
%

We next consider the case of multiple levels of nesting.
To formalize what we mean by arbitrary nesting, we first assume some fixed integral depth
$D > 0$, and real-valued functions $f_0, \cdots, f_D$.
We then define
\begin{align*}
  \gamma_D\left(y^{(0:D-1)}\right) &= \E \left[f_D\left(y^{(0:D)}\right) \middle| y^{(0:D-1)}\right] \quad \text{and} \\
  \gamma_k(y^{(0:k-1)}) &= \E \left[f_k\left(y^{(0:k)}, \gamma_{k+1}\left(y^{(0:k)}\right)\right) \middle| y^{(0:k-1)}\right],
\end{align*}
for $0 \leq k < D$, where $y^{(k)} \sim p\left(y^{(k)}|y^{(0:k-1)}\right)$. 
Note that our single nested case corresponds to the setting of $D=1$, $f_0 = f$, $f_1 = \phi$, $y^{(0)}=y$,
$y^{(1)}=z$, $\gamma_0 = I$, and $\gamma_1 = \gamma$. Our goal is to
estimate $\gamma_0 = \E \left[f_0\left(y^{(0)},
\gamma_1\left(y^{(0)}\right)\right)\right]$. To do so we will use the following NMC scheme:
\begin{align*}
  I_D\left(y^{(0:D-1)}\right) &= \frac{1}{N_D} \sum_{n=1}^{N_D} f_D\left(y^{(0:D-1)}, y^{(D)}_n\right) \quad \text{and} \\
  I_k\left(y^{(0:k-1)}\right) &= \frac{1}{N_k} \sum_{n=1}^{N_k} f_k\left(y^{(0:k-1)}, y^{(k)}_n, I_{k+1}\left(y^{(0:k-1)}, y^{(k)}_n\right)\right)
\end{align*}
for $0 \leq k \le D-1$, where each $y^{(k)}_n \sim p\left(y^{(k)}|y^{(0:k-1)}\right)$ is drawn
independently. Note thus that there are multiple values of $y^{(k)}_n$ for each possible $y^{(0:k-1)}$
and that $I_k\left(y^{(0:k-1)}\right)$ is still a random variable given  $y^{(0:k-1)}$.

We are now ready to provide our general result for the convergence bounds that applies to cases of
repeated nesting, provides constant factors (rather than just using big $O$ notation), and
shows how the bound can be improved if the additional assumption of continuous differentiability holds.
We note that for the particular case of a single nesting without continuous differentiability,
then the bound is exact is the sense that, in addition to providing the required constant factors,
there are no higher-order terms that are only dominated asymptotically.
%\begin{restatable}{theorem}{theRepeat} \label{the:Repeat}
%  Assume that $f_0, \cdots, f_D$ are all Lipschitz continuous, and 
%  that 
%  \[
%          f_k\left(y^{(0:k)}, \gamma_{k+1}\left(y^{(0:k)}\right)\right), I_k\left(y^{(0:k-1)}\right) \in L^2 
%  \]
%  for $0 \leq k \leq D$. Then, the mean squared error $\E \left[\left(\gamma_0 - I_0\right)^2\right]$ 
%  converges to $0$ at rate $O\left(\sum_{k=0}^D
%  \frac{1}{N_k}\right)$.
%\end{restatable}
\vspace{-5pt}
\begin{restatable}{theorem}{theRepeat}
	\label{the:Repeat} %\label{the:biggie}
	If $f_0, \cdots, f_D$ are all Lipschitz continuous with associated Lipschitz 
	constants
	\[K_k = \sup_{y^{(0:k)}} \left| \frac{\partial f_k\left(y^{(0:k)},\gamma_{k+1}(y^{(0:k)})\right)}{\partial \gamma_{k+1}}\right|, \quad \forall k
	\in 0,\dots,D-1
	\]
	and if
	\[
	\varsigma_{k}^2  
	=\E \left[\left(f_k\left(y^{(0:k)},\gamma_{k+1}
	\left(y^{(0:k)}\right) \right)-\gamma_k\left(y^{(0:k-1)}\right)\right)^2\right] < \infty \quad \forall k\in 0,\dots,D
	\]
	then the mean squared error $\E \left[\left(I_0 - \gamma_0\right)^2\right] $ converges to $0$ with rate
	\begin{align}
	\label{eq:bound-lip}
	\E \left[\left(I_0 - \gamma_0\right)^2\right] \le
	\frac{\varsigma_{0}^2}{N_0} +
	\sum_{k=1}^{D} \left(\prod_{\ell=0}^{k-1} K_{\ell}^2\right)
	\frac{\varsigma_{k}^2}{N_{k}}+ O(\epsilon)
	\end{align}
	where $O(\epsilon)$ represents terms that become dominated as $N_0,\dots,N_D
	\rightarrow \infty$.
	
	If $f_0, \cdots, f_D$ are also continuously differentiable with first derivative bounds
	$K_0, \dots, K_D$ and second derivative bounds 
	$C_k = \sup_{y^{(0:k)}} \left|\frac{\partial^2 f_k\left(y^{(0:k)},\gamma_{k+1}(y^{(0:k)})\right)}{\partial \gamma^2_{k+1}}\right|, \forall k
	\in 0,\dots,D-1$, then this mean square error bound can be tightened to
	\begin{align}
	\label{eq:bound-cont}
	\E \left[\left(I_0 - \gamma_0\right)^2\right] \le 
	\frac{\varsigma_0^2}{N_0}
	+\frac{1}{4}\left(
	\frac{C_0 \varsigma_{1}^2}{N_{1}}
	+\sum_{k=0}^{D-2}  \left(\prod_{d=0}^{k} K_{d}\right)
	\frac{C_{k+1} \varsigma^2_{k+2}}{N_{k+2}}
	\right)^2 + O(\epsilon).
	\end{align}
	In the case of a single nesting we can further characterize $O(\epsilon)$ to give
	\begin{align}
	\E \left[\left(I_0 - \gamma_0\right)^2\right]  &\le \frac{\varsigma^2_0}{N_0}+\frac{4 K_{0}^2 \varsigma_1^2}{N_0 N_{1}}
	+\frac{2 K_{0}\varsigma_{0} \varsigma_1}{N_{0} \sqrt{N_1}}+\frac{K_0 ^2 \varsigma_1^2}{N_1} \\
	\E \left[\left(I_0 - \gamma_0\right)^2\right]  &\le \frac{\varsigma^2_0}{N_0}+\frac{4 K_{0}^2 \varsigma_1^2}{N_0 N_{1}}
	+\frac{2 K_{0}\varsigma_{0} \varsigma_1}{N_{0} \sqrt{N_1}}+\frac{C_0 ^2 \varsigma_1^4}{4 N_1^2}
	+ O\left(\frac{1}{N_1^{3}}\right).
	\end{align}
	for when the continuous differentiability assumption does not hold and 
	holds respectively.
\end{restatable}
\vspace{-12pt}
\begin{proof}
	See~\citep{rainforth2017pitfalls}.
\end{proof}
\noindent At a high level, the key points of these results are a convergence rate of $O(\sum_{k=0}^{D} 1/N_k)$
when only Lipschitz continuity holds and $O(1/N_0 +(\sum_{k=1}^{D} 1/N_k)^2)$ when
all the $f_k$ are also continuously differentiable.  
As estimation requires drawing $T = \prod_{k=0}^{D} N_k$ samples, 
the convergence rate will rapidly diminish with repeated nesting.  More precisely then the optimal convergence rates,
as shown~\cite{rainforth2017pitfalls}, are
$O(T^{\frac{-1}{D}})$ and $O(T^{\frac{-2}{D+1}})$ respectively for the two cases (note the single nesting case 
is $D=2$), both of which
imply that exponentially more samples are required to achieve a given error as $D$ is increased.
