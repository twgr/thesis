% !TEX root =  bopp.tex

\subsection{Automatic and Adaptive Domain Scaling}
\label{sec:domain}

Domain scaling, by mapping to a common space, is crucial for BOPP to operate in the required black-box fashion as it allows a general purpose and problem independent hyperprior to be placed on the GP hyperparameters.  BOPP therefore employs an affine scaling to a $[-1,1]$ hypercube for both the inputs and outputs of the GP.  To initialize scaling for the input variables, we sample directly from the generative model defined by the program. %\footnote{Note that Anglican's ability to include statements for conditioning on generated variables, for example to truncate distributions, means this does not always represent $p(\theta)$ and is only a prior in a more abstracted sense.}
This is achieved using a second transformed program, \qprior, which removes all conditioning, i.e. \observe statements, and returns $\theta$.  This transformation also introduces code to terminate execution of the query once all $\theta$ are sampled, in order to avoid unnecessary computation.  As \observe statements return \lsi{nil}, this transformation trivially preserves the generative model of the program, but the probability of the execution changes.  Simulating from the generative model does not require inference or calling potentially expensive likelihood functions and is therefore computationally inexpensive.   By running inference on \qmarg~given a small number of these samples as arguments, a rough initial characterization of output scaling can also be achieved.  If points are observed that fall outside the hypercube under the initial scaling, the domain scaling is appropriately updated\footnote{An important exception is that the output mapping to the bottom of the hypercube remains fixed such that low likelihood new points are not incorporated.  This ensures stability when considering unbounded problems.} so that the target for the GP remains the $[-1,1]$ hypercube.

%Our second transformation generates a representation of the prior, \qprior, by removing all conditioning, i.e. \observe, statements, whilst preserving the generative model.  Though Anglican's ability to include statements for conditioning on generated variables, for example to truncate distributions, means this new program will not always represent $p(\theta)$ exactly, it none-the-less provides a useful tool for establishing a rough initialization for domain scaling as detailed in Section \ref{sec:domain}.

\subsection{Unbounded Bayesian Optimization via Non-Stationary Mean Function Adaptation}
\label{sec:unbounded}

Unlike standard BO implementations, BOPP is not provided with external constraints and we therefore develop a scheme for operating on targets with potentially unbounded support.  Our method exploits the knowledge that the target function is a probability density, implying that the area that must be searched in practice to find the optimum is finite, by defining a non-stationary prior mean function.  This takes the form of a bump function that is constant within a region of interest, but decays rapidly outside.  Specifically we define this bump function in the transformed space as
\begin{align}
\label{eq:BUMP}
\mu_{\mathrm{prior}}\left(r;r_e,r_{\mathrm{\infty}}\right) = \begin{cases} 0  \hfill & \mathrm{if} \; r \leq r_{\mathrm{e}} \\ 
\log \left(\frac{r-r_{\mathrm{e}}}{r_{\mathrm{\infty}}-r_{\mathrm{e}}}\right)+\frac{r-r_{\mathrm{e}}}{r_{\mathrm{\infty}}-r_{\mathrm{e}}} & \mathrm{otherwise} \end{cases}
\end{align}
where $r$ is the radius from the origin, $r_e$ is the maximum radius of any point generated in the initial scaling or subsequent evaluations, and $r_{\mathrm{\infty}}$ is a parameter set to $1.5 r_{e}$ by default.  Consequently, the acquisition function also decays and new points are never suggested arbitrarily far away.  Adaptation of the scaling will automatically update this mean function appropriately, learning a region of interest that matches that of the true problem, without complicating the optimization by over-extending this region.  We note that our method shares similarity with the recent work of Shahriari et al \citep{shahriari2016unbounded}, but overcomes the sensitivity of their method upon a user-specified bounding box representing soft constraints, by initializing automatically and adapting as more data is observed.