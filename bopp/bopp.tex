% !TEX root = ../main.tex
%
%\begin{savequote}[8cm]
%	Everyone by now presumably knows about the danger of premature optimization. 
%	I think we should be just as worried about premature design - 
%	designing too early what a program should do.
%	\qauthor{--- Paul Graham}
%\end{savequote}

\chapter{Automating Learning -- Bayesian Optimization for Probabilistic Programs}
\label{chp:bopp}

\input{bopp/abstract}

\section{Motivation} 
\label{sec:IntroductionBOPP}

\input{bopp/introduction}

\section{Related Work} 
\label{sec:bopp:related}

Reasonable consideration has been given before to solving maximum likelihood and marginal 
a posteriori (MAP) problems with PPSs and many systems provide some form of appropriate
estimation scheme (see e.g.~\citep{goodman_book_2014,carpenter2015stan,salvatier2016probabilistic}).  One simple
approach is to apply an annealing to the \observe densities (for maximum likelihood estimation)
or both the \sample and \observe densities (for MAP estimation) in
a conventional MCMC sampler such as LMH (see Section~\ref{sec:proginf:str:lmh}).
This results in a simulated annealing algorithm~\citep{aarts1988simulated} for general purpose programs.
An alternative approach, introduced by~\cite{tolpin-socs-2015}
instead uses ideas from Monte Carlo tree search to construct a general purpose MAP estimator.
However, all of these approaches do not permit the more challenging scenario
where the target is to optimize a marginal distribution.  They are, therefore, not suitable for combined
inference and optimization problems.

One approach that does consider optimization of marginal probabilities of probabilistic
programs is given by~\cite{vandemeent2016black}, which thus perhaps presents the closest
approach to our own work.  The aim of~\cite{vandemeent2016black} is, on the surface, somewhat
different to our own as they look to automate policy search problems~\citep{deisenroth2013survey}
using probabilistic programs.  However, policy search is a particular instance of MML estimation
and so falls within our general problem class.  Moreover, their approach of maximizing an ELBO~\citep{blei2016variational}
using stochastic gradient ascent~\citep{robbins1951stochastic} is, in principle, substantially
more general than policy search problems and constitutes a general MML estimation scheme in its own right.
However, the approach has a number of restrictions and assumptions stemming from its variational 
inference roots.  Perhaps the most critical for our purposes is that gradients are estimated using
importance sampling and thus will generally become increasingly noisy as the dimensionality of the nuisance variables (i.e. those we wish to marginalize over) increases.
The approach also requires mean field approximations
to be made, the appropriateness of which will vary from problem to problem, while, as with other
stochastic gradient ascent approaches, a large number of optimization iterations is typically required
to reach convergence, which can be prohibitive for some problems.  BOPP, on the other hand, requires
very few assumptions to be made and is free to use more advanced inference approaches, for example
SMC, for estimating the marginal likelihood.  One consequence of this is that it can scale to far
higher dimensions in the nuisance variables.  For example, our HMM model in Section~\ref{sec:bopp:exp}
marginalizes over a $\tilde5000$ dimensional space.

Another interesting alternative approach,
developed since publication of BOPP, involves taking derivatives \emph{through} an SMC
sweep~\citep{le2017auto,naesseth2017variational,maddison2017filtering}. 
More precisely, these methods allow the derivative of the marginal likelihood estimate (or more typically
its logarithm) to be calculated during an SMC sweep, for example by using automatic differentiation
on the calculation of the original estimate~\citep{le2017auto}.  This can be used for MML or MMAP estimation
of global parameters by using these gradients as input to a stochastic gradient ascent scheme.
A key difference of this approach, to say~\cite{vandemeent2016black}, is that using SMC instead of importance
sampling means that substantially lower variance gradient estimates can be achieved.
Though, to the best of our knowledge, no such approach is currently implemented in a PPS, doing so is 
in theory perfectly possible given a system supporting automatic differentiation.  
In~\cite{le2017auto}, we also show how this approach can be extended to perform simultaneous model
and proposal adaptation and further to an amortized inference setting, whereby we learn an inference
artifact that returns a proposal at run time.  This means that, when the model of interest comprises of
a deep neural network, then the method can be viewed as extending so-called auto-encoding methods~\citep{kingma2014auto,burda2015importance}
from their limited importance sampling setting, to a more powerful SMC framework.

\section{Problem Formulation}
\label{sec:problem}

\input{bopp/problem-formulation.tex}

\section{Bayesian Program Optimization}
\label{sec:bopp}

\input{bopp/optimization}
%
%\subsection{Problem Independent Gaussian Process Hyperprior}
%\label{sec:app:hyperprior}
%
%\input{bopp/gp.tex}

\section{Experiments}
\label{sec:bopp:exp}

\input{bopp/experiments.tex}

%
%\subsection{Full Details for House Heating Experiment}
%\label{sec:app:heating}
%
%\input{bopp/house-heating.tex}

\section{Discussion}
\label{sec:disc}

\input{bopp/discussion}

%\section{Program Transformations in Detail}
%\label{sec:program-transformations}
%\input{bopp/program-transformations}

%which in the interest of clarity will our focus.  Other variations of COI, such as risk minimization and type-$\RN{2}$ maximum likelihood can be achieved by for switching between minimization and maximization, and, using Bayes' rule, removing the prior component on $\theta$.  These cases are also covered by BOPP and are discussed in the SM.
%To carry out global optimization, it is necessary for the target to diminish away from a region of interest.  This is implicitly satisfied by \eqref{eq:MMAP} as $p(Y, \theta)$ is a probability distribution.  
%We note that as finite bounds are equivalent to placing a uniform prior over the space of permissible solutions, this permits parameter optimization and standard BO (where $X=\emptyset$) as special cases\footnote{In some cases this may also require a mapping of the target to unnormalized probability distribution.  For example by noting that $\argmax_{\theta \in \mathcal{S} \subset \vartheta} f\left(\theta\right) = \argmax_{\theta \in \vartheta} \mathrm{Uniform}\left(\theta \in \mathcal{S}\right) \exp(f\left(\theta\right))$.}.
%Although $X$ may be dynamically typed, we assume that $\theta$ is statically determinable.  This is because optimization, unlike inference, is not in general well defined on a variable defined relative to a mixed measure as some values may be infinity more probable than others.  We emphasise though that different $\phi$ may be of different type (i.e. some continuous and some discrete) and the type need not need be known prior to execution - the distribution from which $\phi$ is sampled may itself be dynamic provided the density is defined with respect to the same measure for all possible program traces.  We also apply the restriction that each target variable $\phi \in \theta$ is the direct output of a \sample statement.  As all variables in an Anglican query are either the result of a sample statement or deterministically calculable from previously invoked sample statements, this concession does not restrict the space of models in practise.  Further discussion on the rational behind these restrictions is provided in the supplementary material.