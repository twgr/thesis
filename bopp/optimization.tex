% !TEX root =  bopp.tex

In addition to the syntax introduced in the previous section, there are five main components to BOPP:
\begin{itemize}
	\setlength\itemsep{0.1em}
	\item[-] A program transformation, \texttt{q}$\rightarrow$\qmarg, allowing estimation of the evidence $p(Y,\theta)$ at a fixed $\theta$.
	\item[-] A high-performance, GP based, BO implementation for actively sampling $\theta$.
	\item[-] A program transformation, \texttt{q}$\rightarrow$\qprior,  used for automatic and adaptive domain scaling, such that a problem-independent hyperprior can be placed over the GP hyperparameters.
	\item[-] An adaptive non-stationary mean function to support unbounded optimization.
	\item[-] A program transformation, \texttt{q}$\rightarrow$\qacq, and annealing maximum likelihood estimation method to optimize the acquisition function subject the implicit constraints imposed by the generative model.
\end{itemize}
Together these allow BOPP to perform online MMAP estimation for arbitrary programs in a manner that is black-box from the user's perspective - requiring only the definition of the target program in the same way as existing PPS and identifying which variables to optimize.  The BO component of BOPP is both probabilistic programming and language independent, and is provided as a stand-alone package.\footnote{Code available at~\href{http://www.github.com/probprog/deodorant/}{\url{http://www.github.com/probprog/deodorant/}}}  It requires as input only a target function, a sampler to establish rough input scaling, and a problem specific optimizer for the acquisition function that imposes the problem constraints.  %We first provide a high-level overview of the algorithm before separately explaining these components.

%BOPP provides online MMAP estimation for arbitrary programs in a manner that is black-box from the user's perspective - requiring only the definition of the target program in the same way as existing PPS and identifying which variables to optimize. It has three main components: a series of program transformations, inference schemes for evaluating these transformed programs, and a BO scheme that uses them to provide the required MMAP estimation.  Implementation of the transformations is naturally language specific, but the required techniques can be applied to any system with general-purpose languages for model specification and which provides the required inference schemes.  Given functions for evaluating these transformed programs, the BO scheme for MMAP estimation can be abstracted from probabilistic programming and is provided as its own separate package\footnote{\url{http:\\www.bitbucket.org\twgr\bo-mapp}}.  This package requires three things: a target function which provides estimates of the marginal $p(Y,\theta)$, a sampler for cheaply generating a rough representation of the input scaling, and an optimizer for the acquisition function that imposes the constraints of the problem.

Figure \ref{fig:bopp_overview} provides a high level overview of the algorithm invoked when \doopt is called on a query \texttt{q} that defines a distribution $p\left(Y, a, \theta , b\right)$.  We wish to optimize $\theta$ whilst marginalizing out $a$ and $b$, as indicated by the the second input to \texttt{q}. In summary, BOPP performs iterative optimization in 5 steps

\begin{figure}[t]
	\centering
	\includegraphics[width=\textwidth]{"bopp_overview_figure"}
	%\vspace{20pt}
	\caption{
		\label{fig:bopp_overview}
		Overview of the BOPP algorithm, description given in main text. \texttt{p-a}, \texttt{p-$\theta$}, \texttt{p-b} and \texttt{lik} all represent distribution object constructors. \factor is a special distribution constructor that assigns probability $p(y) = y$, in this case $y = \zeta(\theta)$.}
\end{figure}

%where \footnote{Anglican's \defdist construct makes it possible to specify arbitrary code, including code external to Anglican, to be called when \sample or \observe is invoked, e.g. as used to define \abcl in Figure \ref{fig:house-heating-code}.}, and which 

\begin{itemize}
	\item[-] Step 1 (blue arrows) generates unweighted samples from the transformed prior program \texttt{q-prior} (\emph{top center}), constructed by removing all conditioning. This initializes the domain scaling for $\theta$.
	\item[-] Step 2 (red arrows) evaluates the marginal $p(Y,\theta)$ at a small number of the generated $\hth$ by performing inference on the marginal program \qmarg~ (\emph{middle centre}), which returns samples from the distribution $p\left(a,b | Y, \theta\right)$ along with an estimate of $p(Y, \theta)$.  The evaluated points (\emph{middle right}) provide an initial domain scaling of the outputs and starting points for the BO surrogate.
	\item[-] Step 3 (black arrow) fits a mixture of GPs posterior \cite{rasmussen2006gaussian} to the scaled data (\emph{bottom centre}) using a problem independent hyperprior. The solid blue line and shaded area show the posterior mean and $\pm2$ standard deviations respectively. The new estimate of the optimum $\hth^*$ is the value for which the mean estimate is largest, with $\hat{u}^*$ equal to the corresponding mean value.    
	\item[-] Step 4 (purple arrows) constructs an acquisition function $\zeta \colon \vartheta \rightarrow \real^+$ (\emph{bottom left}) using the GP posterior.  This is optimized, giving the next point to evaluate $\hth_{\mathrm{next}}$, by performing annealed importance sampling on a transformed program \texttt{q-acq} (\emph{middle left}) in which all \observe statements are removed and replaced with a single \observe assigning probability $\zeta(\theta)$ to the execution. % A non-stationary prior mean function for the GP, the AF is penalized away from a region of interest, allowing unbounded optimization.  
	%The AF is optimized by performing annealed importance sampling on a transformed program \texttt{q-acq} (\emph{middle left}) in which all \observe statements are removed and replaced with a single \observe that assigns probability $\zeta(\theta)$ to the execution. 
	\item[-] Step 5 (green arrow) evaluates $\hth_{\mathrm{next}}$ using \qmarg~and continues to step 3.
\end{itemize}

\subsection{Program Transformation to Generate the Target}
\label{sec:transform}
\input{transform}

%\subsection{Marginal Maximum A Posteriori Estimation}
%\input{optquery}

\subsection{Bayesian Optimization of the Marginal}
\label{sec:BOPP}

\input{bopp-for-ml}
\label{sec:bopp-for-ml}

\input{domainscaling}

\input{optacqfunc}