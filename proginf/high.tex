% !TEX root = ../main.tex

\section{A High-Level Introduction to General Purpose Inference}
\label{sec:proginf:high}

Before getting into the nitty-gritty of designing a compiler and inference engine for a universal PPL, we
first consider at a high-level how we might hope to do inference in such systems.  The simplest
thing we could do is self normalized importance sampling (SNIS) using the generative model specified by the \sample
statements as a proposal.  This strategy is sometimes known as \emph{likelihood weighting} in the probabilistic
programming literature and simple involves directly sampling from the forward model an accumulating
weights from the \observe conditioning statements in a sequential importance sampling fashion (see 
Section~\ref{sec:part:smc:sis}).
In Section~\ref{sec:probprog:models:general}, we specified with~\eqref{eq:probprog:universal-cond}
the unnormalized conditional distribution of the program using execution traces generated
by the program.  If we sample from the generative model then it is clear that all samples will satisfy
$\mB(x_{1:n_x},\lambda)=1$ while our the support of our proposal will clearly cover that of
the posterior provided the conditioning density terms are always finite.  This is also a proposal
we can always sample from because our \observe terms will only effect the probability of a trace, not
the variables it generates.  Using the notation from Section~\ref{sec:probprog:models:general} 
this approach leads to the following SNIS characterization of the posterior
\begin{align}
p(x_{1:n_x} | \lambda) &\approx \hat{p}(x_{1:n_x} | \lambda) := \sum_{n=1}^{N} \nw_n \delta_{\hxnx^n} (x_{1:n_x})
\quad \text{where}  \\
\hxnx^n &\sim \prod_{j=1}^{n_x} f_{a_j}(x_j | \phi_j); \quad \nw_n = \frac{w_n}{\sum_{n=1}^{N} w_n}; \quad
w_n = \prod_{k=1}^{\hat{n}_{y}^n} g_{\hat{b}_{k}^n}(\hat{y}_{k}^n | \hat{\psi}_{k}^n);
\end{align}
$\delta_{\hxnx^n} (\cdot)$ is a delta function centered at $\hxnx^n$; $\phi_j$ and $a_j$ are deterministic functions of $\hxnx^n$ 
and $\lambda$; and $\hat{n}_{y}^n$, $\hat{b}_{k}^n$, $\hat{y}_{k}^n$, and $\hat{\psi}_{k}^n$ are random variables
that are deterministic functions of $\hxnx^n$ and $\lambda$.  Here we are sampling $\hxnx^n$ from
the generative model defined by the program which is equivalent to a running the forward program
ignoring all of the \observe statements.  Our unnormalized likelihood weights $w_n$ correspond to
product of all the probabilities accumulated from the \observe terms.  We remind the reader that each 
$x_j$ corresponds to the direct output of a sample statement, not an explicit variable in the program, 
with the program variables and outputs being deterministic functions of $x_{1:n_x}$ (see 
Section~\ref{sec:probprog:anglican:models}). In addition to providing a 
characterization of the conditional distribution through $\hat{p}(x_{1:n_x} | \lambda)$, we can also
use our samples to construct consistent estimates from the outputs of program $\Omega$, remembering
that these are a deterministic function of $\xnx$ and so an expectation over $\Omega$ can be
represented as an expectation over $\xnx$.
This convergence follows in the standard way for SNIS provided our program terminates with probability $1$.

In practice, we will not sample a full $\hxnx^n$ before evaluating its weight in one go -- we will use
sequential importance sampling strategy whereby we sample directly from any \sample statement as we encounter it
and accumulate weights from \observe statements as we go (see Figure~\ref{fig:probprog:poisson} in
Section~\ref{sec:probprog:models:general}).
Another way of viewing this is that we will repeatedly run our query as if it were an ordinary
program, except for the fact that we accumulate a weight as a side effect that is then returned with
the produced sample.  Such a guess an check strategy will obviously be ineffective except in very low
dimensions.  However, it will both be a go-to strategy for the most extreme possible problems that
fail to satisfy any of the required assumptions for more advanced methods and a basis for more complex 
inference strategies.  For example, as we will shown in Section~\ref{sec:proginf:str}, we can
convert this to an SMC strategy, provided $n_y$ is fixed, by running a number of separate executions
in parallel and performing resampling at each of the \observe statements.  Alternatively, we can
develop component-wise MCMC strategies by proposing changes to individual $x_j$ and then
re-evaluating the weights.  However, doing so will require our inference strategy to have more
control that simply running the program forwards in its entirety and this will require both a compilation
and an interface for inference, which will be the focus of the next two sections.