% !TEX root = ../main.tex

\section{Inference Strategies}
\label{sec:proginf:str}

%Before jumping into describing some specific inference algorithms for
%Anglican, we first link our compilation back to the execution-based 
%definition of the conditional distribution specified by a program given in 
%Section~\ref{sec:probprog:models:general} and consider what classes of inference
%algorithms might be able to operate in such settings. 

As we previously alluded to, the simplest inference strategy we can carry out is importance
sampling.  This is in fact so simple in a current framework that it uses the default behavior
of all the checkpoints, while the \anginfer function only involves constructing a lazy infinite
sequence of the output from independent calls of \clj{exec} on the full program.
Keeping the default behavior for the \sample implicitly means that our
inference will use a bootstrap proposal (i.e. the generative model is taken as the proposal).
Though not technically required, this is still a highly convenient choice of proposal as
amongst other things, this ensures that we can always sample from the proposal and
that the proposal is valid in terms of its tail behavior (presuming the conditional probabilities
are bounded).

\subsection{MCMC Strategies}
\label{sec:proginf:str:lmh}

We have already explained why it might be difficult to construct a global MH proposal for
our program due to the difficulties in varying dimensionality and unknown variable supports.
One solution to this is to try and use compilation to establish this support so that
a valid proposal can be specified.  However, doing this in a general manner is a somewhat
challenging and remains an open problem in the field.  Another, more immediately viable,
approach is to use a proposal that looks to update particular $x_j$ in the trace in a
component-wise MH manner (see Section~\ref{sec:inf:foundation:gibbs}), potentially rerunning
the rest of the program after that choice if necessary.  In the context of PPSs such
approaches are generally known as either single-site MH, random database sampling, or lightweight MH (LMH), and were
originally suggested by~\citep{wingate2011lightweight}.\footnote{The MH acceptance ratio in the original version of this work
	is incorrect so we point the reader to the following updated version
	\url{https://stuhlmueller.org/papers/lightweight-mcmc-aistats2011.pdf}.} There are two key factors
that make such approaches viable.  Firstly, if we update a term $x_j$ in our trace, there is
no need to update any of the factors in our trace probability that occur before the sampling
$x_j$, while we may be able to avoid evaluating many of those after as well by establishing
conditional independence.  Secondly, the support for a particular $x_j$ given
$x_{1:j-1}$ is typically known (as 
we have access to the appropriate distribution object) and so it is generally possible to specify
an appropriate proposal for an individual $x_j$.  Even if that transpired not to be possible, 
simply using the prior as the proposal is still often reasonable as individual $x_j$ terms will typically
be low dimensional.  However, a key downside of this approach is that changing a particular $x_j$
might lead to an invalid trace and checking for this might require revaluation of the entire rest of the
trace, making it an $O(n_x+n_y)$ operation.  Perhaps even more problematically, if $n_x$ is not fixed
then, unless we make adjustments to the algorithm, the approach will not produce a 
valid Markov chain.  Similarly, as we showed in Section~\ref{sec:inf:foundation:gibbs},
there are models for which component-wise MH approaches lead to reducible Markov chains
that no longer admit the correct target.  The emphasis on branching in universal PPSs further
means that the chance of falling into this category of invalid models is relatively high.  As
noted by~\citep{kiselyov2016problems}), this is a rather serious issue missed (or at least not
acknowledged) by~\cite{wingate2011lightweight} and various follow-up implementations.
See also issues highlighted by~\citep{hur2015provably} in other implementations.
The problem is not shared by all implementations though, with the Anglican LMH method, amongst
others, not suffering from the issue.
More generally, there are means of potentially overcoming both the reducibility and computational
issues of LMH as we will discuss later, but
they still represent noticeable practical and theoretical hurdles.

As a simple illustrative approach that avoids some of the theoretical pitfalls associated
with LMH approaches, consider an MH sampler whose proposal simply chooses one
of the \sample statements, $m$, uniformly at random from $\{1,\dots,n_x\}$, proposes
 a new $x_m'$ using a local reversible
MH kernel for that \sample statement $\kappa(x_m' | x_{m})$ (which can always be independently sampling
from $f_m(x_m|\eta_m)$ if necessary) and then reruns the entire rest of the program
from that point~\citep{wood2014new}.  This equates to using the 
proposal\footnote{We omit our trace validity term because it always holds when sampling from the generative model.}
\begin{align}
q(x_{1:n_x'}' | x_{1:n_x}) &= q(m|x_{1:n_x}) q(x_m' | x_{m}, m) q(x_{m+1:n_x}' | x_{1:m-1}, m, x_m') \mathbb{I}(x_{1:m-1}'=x_{1:m-1}) \nonumber \\
&=\frac{1}{n_x} \kappa(x_m' | x_{m}) \mathbb{I}(x_{1:m-1}'=x_{1:m-1})  \prod_{j=m+1}^{n_x'} f_{a_{j}'} (x_{j}' | \eta_{j}')
\end{align}
which in turn gives an acceptance probability of
\begin{align}
P(\text{Accept}) &= \min\left(1, \frac{\gamma(x_{1:n_x'}',\lambda) q(x_{1:n_x} | x_{1:n_x'}') }
{\gamma(x_{1:n_x},\lambda) q(x_{1:n_x'}' | x_{1:n_x}) }\right) \nonumber\\
&= \min\left(1, \frac{\kappa(x_m | x_{m}') f_{a_m} (x_m' | \eta_m)} {\kappa(x_m' | x_{m}) f_{a_m} (x_m | \eta_m)} \;
\frac{n_x}{n_x'} \; \frac{\prod_{k=k_0(x_{1:m})+1}^{n_y'} g_{b_{k}'} (y_{k}' | \psi_{k}')}
{\prod_{k=k_0(x_{1:m})+1}^{n_y} g_{b_{k}} (y_{k} | \psi_{k}) }\right) \label{eq:proginf:lmh-A}
\end{align}
where $k_0(x_{1:m})$ is equal to the number of \observe statements encountered by the partial
trace $x_{1:m}$.  Here we have used the fact that the trace probability factors for terms 
before $x_m$ cancel
exactly, while the \sample terms for $m+1$ onwards cancel between the target and the proposal.
We can intuitively break down the terms in~\eqref{eq:proginf:lmh-A} by first noting that the first
ratio of terms, $\frac{\kappa(x_m | x_{m}') f_{a_m} (x_m' | \eta_m)} {\kappa(x_m' | x_{m}) f_{a_m} (x_m' | \eta_m)}$, is what we would get from
doing MH targeting $f_{a_m}(x_m|\eta_m)$.  The next ratio of terms,
$n_x / n_x'$ reflects the fact that if our new trace is longer it is less likely that we would have chosen the
point $m$ to resample and vice versa.  The final ratio of terms reflects the ratio of the likelihood weight
for the new generated section of the trace over the existing section of the trace.

This approach avoids the reducibility issues because it
includes as a possible step generating a completely new trace from the generative model.  However,
it will clearly be heavily limited in practical performance because each iteration
is effectively using importance sampling in $n_x-m$ dimensions such that, once the sampler has burnt in,
its acceptance rate will typically decrease dramatically with dimension.  Each iteration is also $O(n_x+n_y)$ as
the complete new trace needs to be proposed each time.  To get around these issues, we would like
to have some concept of whether we can update $x_m$ without invalidating the trace.  This can sometimes
be done using code analysis or compilation to establish the Markov blanket of $x_m$
\citep{yang2014generating,mansinghka2014venture,ritchie2016c3}, thereby providing a representation of which
terms will be unaffected by an update.  One needs to be
careful, however, not to reintroduce reducibility issues and, in general, correctness of LMH schemes is
far from trivial and perhaps requires further consideration in the literature.

Even if we do not have a convenient means of extract Markov blankets, it will still generally be desirable
to reuse $x_{m+1:n_x}$ if it produces a valid trace, i.e. if $\mB ([x_{1:m-1},x_m',x_{m+1:n_x}],\lambda)=1$, and reducibility
issues can be guarded against, in order to avoid the terrible scaling in the acceptance rate 
of~\ref{eq:proginf:lmh-A}  with $n_x-m$.  One way to do this would be to propose
a new $x_m'$ in the same way but then deterministically run the program forward with the old
$x_{m+1:n_x}$ to evaluate whether the trace is valid, noting that if it is, it must also be the case that any 
extension or reduction of the trace, i.e. increasing or decreasing
$n_x$, is invalid as the program is deterministic given $\xnx$.  If $x_m \rightarrow x_m'$ gives
a valid trace, then $\gamma([x_{1:m-1},x_m',x_{m+1:n_x}],\lambda)$ will be well defined (though not necessarily
non-zero) and we can use the update as a proposal without needing to regenerate $x_{m+1:n_x}$.  Note though
that as the $a_j$, $b_k$, $\eta_j$, and $\psi_k$ terms downstream of $m$ can change, the 
probability of the new trace still needs recalculating.  In this scenario, our acceptance ratio becomes
\begin{align}
  P(\text{Accept}) &= \min\left(1, \frac{\gamma(x_{1:n_x}',\lambda) \kappa(x_m | x_{m}')  }
  {\gamma(x_{1:n_x},\lambda) \kappa(x_m' | x_{m})  }\right)
\end{align}
where all terms in $\gamma(x_{1:n_x}',\lambda)$ (i.e. both \sample and \observe) will need
to be evaluated for $j\ge m$, as will $\gamma(x_{1:n_x},\lambda)$ if these are not already known. 

If, on the other hand, our trace becomes invalid
at any point (noting that we can evaluate the validity of sub traces as we rerun them), we can resort to
resampling the trace anew from the required point onwards.  Note that we can do this validity evaluation
and regeneration during the same single forward pass through trace and that whether we do regeneration
is deterministic for given $\{x_m,x_m'\}$ pair (and thus does not affect the MH acceptance ratio by symmetry).
If we regenerate from \sample $\ell$ onwards then we now have
\begin{align}
P(\text{Accept}) &= \min\left(1, \frac{\gamma(x_{1:n_x}',\lambda) \kappa(x_m | x_{m}') n_x \prod_{j=\ell}^{n_x} f_{a_{j}} (x_j | \eta_j)}
{\gamma(x_{1:n_x},\lambda) \kappa(x_m' | x_{m})  n_x' \prod_{j=\ell}^{n_x'} f_{a_{j}'} (x_j' | \eta_j')}\right).
\end{align}
We can further overcome reducibility issues with such a scheme by forcing a fresh trace generation
not only when we realize the trace is invalid, but also when we find a term in our trace with probability zero.
Once this occurs, it is clear the that full trace probability must be zero, even if it produces a valid path.  We
could just immediately reject the trace, but we would not the reducibility issues.  By instead
regenerating the rest of the trace in this scenario, then at each iteration we can propose any value of $x_1$ that has non-zero marginal
mass under $\gamma(x_{1:n_x},\lambda)$ regardless of our current state, so we clearly have mixing on $x_1$.
For a given value of $x_1$, we can similarly propose any value of $x_2$ with non-zero density under the
marginal conditional distribution of $x_2|x_1$.  As we have mixing of $x_1$, this now implies we have
mixing on $x_{1:2}$ as well.  By induction, our method now leads to mixing on all of $x_{1:n_x}$, which
coupled with detailed balance provides an informal demonstration of the consistency of the method.

We can refine this process further by using the concept of a database.  To do this we mark each \sample
statement in the trace with a unique identifier that is common to all traces that evaluate the same \sample
statement at the same point, i.e. points in the trace for which both the \sample number $j$ and 
the lexical \sample identifier $a_j$ are the same.  Our database can be used to store previous samples and
deterministically return them when revisiting a \sample statement with the same identifier if this old
value still constitutes a valid sample with non-zero probability, regenerating it if not.  
If we use $\mathbb{D}(j)=0$ to denote terms taken from
the database and $\mathbb{D}(j)=1$ to indicate terms that are redrawn, then our acceptance ratio now becomes
\begin{align}
P(\text{Accept}) &= \min\left(1, \frac{\gamma(x_{1:n_x}',\lambda) \kappa(x_m | x_{m}') n_x \prod_{j=m+1}^{n_x} 
	\left(f_{a_{j}} (x_j | \eta_j)\right)^{\mathbb{D}(j)}}
{\gamma(x_{1:n_x},\lambda) \kappa(x_m' | x_{m})  n_x' \prod_{j=m+1}^{n_x'} \left(f_{a_{j}'} (x_j' | \eta_j')\right)^{\mathbb{D}(j)}}\right).
\end{align}
Note the importance of points in the database being defined by both $j$ and $a_j$ -- if we hit the same sample
lexical sample statement at a different point in the program, we always need to redraw it.
Care is also needed to ensure superfluous terms are removed from the database at each iteration -- it should
contain only terms from the current trace.  

Note, however, that our identification scheme for points in the database, namely points for which both $j$ and $a_j$ are the same
have the same point in the database, is
potentially stronger than necessary and there may be more useful addressing schemes.  Remembering back
to Section~\ref{sec:probprog:models:general}, each of the \sample and \observe statements are exchangeable up to
the required inputs being in scope.  Now consider the case where the update $x_m'\leftarrow x_m$ triggers an
inconsequential extra \sample to be invoked between points $j$ and $j+1$, with everything else staying the same.
Under our current system then all points after $j$ would need to be resampled.  However, our program would
define the same distribution if this superfluous \sample statement came at the end of the program, in which case
our method would mean that $x_{j+1:n_x}$ no longer necessarily need updating.  From a more practical perspective,
we can consider using more useful naming strategies for our database entries that exploit this exchangeability such
that $x_{j+2}'$ could, for example, inherit from $x_{j+1}$~\citep{wingate2011lightweight}.  
%Though this should
%intuitively not effect the convergence of estimates based on the program output, the formal proof is not immediately
%trivial and is perhaps missing from the literature.

We finish the section by briefly discussing some choices in the proposal.  The simplest choice for
$\kappa(x_m'|x_m)$ is just to redraw a new value from the prior.  In this case then all the 
$\frac{\kappa(x_m | x_{m}') f_{a_m} (x_m' | \eta_m)} {\kappa(x_m' | x_{m}) f_{a_m} (x_m' | \eta_m)}$ terms
will cancel in our acceptance ratios and giving a trivially valid proposal, other than the aforementioned 
reducibility issues.  We will refer to this strategy simply
as LMH in the rest of the thesis.  For models with significant prior-posterior mismatch, such an approach could be
slow to mix though as it does not allow for any locality in the moves (i.e. $\kappa(x_m'|x_m)$ is independent of $x_m$).
As suggested by, for example,~\cite{le2015rmh}, one can sometimes achieve improvements by instead use the type of the 
distribution object associated with $x_m$ to automatically construct a valid random walk proposal that allows for 
improved hill climbing
behavior on the individual updates.  We will refer to this method as RMH elsewhere in the thesis.\footnote{Note that
	the Anglican RMH implementation uses a mixture proposal where it samples from the prior, as per LMH, half the time
	and from the random walk proposal the other half.}  Thus far, we have
presumed that which \sample statement to update at each iteration is selected uniformly at random.  This is not
actually a necessary assumption, with~\cite{tolpin2015output} demonstrating that one can construct an adaptive LMH (ALMH)
that adaptively updates proposal probabilities for which term in the trace to update.

\subsection{Particle Based Inference Strategies}
\label{sec:proginf:str:part}

\subsubsection{Sequential Monte Carlo}
\label{sec:proginf:str:part:smc}
\vspace{-5pt}
Going from importance sampling to SMC in our framework is remarkably simple
from an implementation perspective~\citep{wood2014new,paige2014compilation}.  The behavior of the \sample and \clj{result} 
checkpoints is kept as per the default.  
The \observe checkpoints are redefined
to carry out the same operations, but return a record rather
than a thunk, returning control to the \anginfer function.  This means that 
calling \clj{exec} for the SMC checkpoint setup will run the program up to and including
the next \observe statement.  Consequently, if we run multiple threads of \clj{exec} at
once, each corresponding to a separate particle, these will all stop exactly when
the next resampling point is required for SMC.  Thus all the \anginfer function needs
to do for SMC, other than some bookkeeping,
is alternate between mapping an \clj{exec} call across all of the particles and
performing resampling steps (remembering to reset the internal weights for the traces to
be the same).  The marginal likelihood estimate can also be calculated in
the standard way, so the required lazy infinite sequence
of output samples can be produced by running independent SMC sweeps and setting the weights
to the product of the sweep marginal likelihood and local sample weight.\footnote{In practice, Anglican
	resamples after the last observation, so the
	local sample weights are all actually the same.}

From a theoretical perspective, running SMC in Anglican
requires us to make one small model assumption -- that the number of observations $n_y$ is fixed.  In
practice, this assumption is usually satisfied, particularly if their are no observations of
internally sampled variables.  Violations are caught at run time.  Given
a fixed $n_y$, we can define the series of targets for SMC as being the distributions
induced by running the program up to the $t^{\text{th}}$ \observe statement, namely
\begin{align}
\label{eq:proginf:smc-targ}
\gamma_t(x_{1:n_x}, \lambda) = \begin{cases}
\prod_{j=1}^{n_x} 
f_{a_j}(x_j | \eta_j)
\prod_{k=1}^{t}
g_{b_k}(y_k | \psi_k) \;\; \text{if} \;\; \mathcal{B}_t(x_{1:n_x},\lambda)=1 \\
0 \quad \text{otherwise}
\end{cases}
\end{align}
where $\mathcal{B}_t(x_{1:n_x},\lambda)$ is a function establishing the validity of the 
partial program trace.  More formally, we can define $\mathcal{B}_t(x_{1:n_x},\lambda)$ as
being a function indicating validity of a trace for transformation of the original program
that terminates after making its $t^{\text{th}}$ observe.
It may be that executions corresponding to different particles have not gone through the 
same \sample and \observe statements at any particular point, but this not a problem, from a theoretical perspective,
as provided that $n_y$ is fixed,~\eqref{eq:proginf:smc-targ} still defines an appropriate
series of targets for SMC inference.
Although changing the position of the \observe statements in
our program does not change the final distribution targeted by running SMC, we note that it can change
the intermediate target distributions, by adjusting at what point during the series of targets
the \sample statements are introduced.  Consequently, changing the position of the \observe
statements can have a dramatic effect on the practical performance of the inference, e.g.
placing all the \observe statements just before the program returns will cause the algorithm
to reduce to basic importance sampling.  The earlier the \observe statements are in the program,
or more precisely the later variables are sampled relative to the \observe statements, the
better inference will performance as less information is lost in
the resampling.  Tricks such as lazily sampling variables (such that \sample statements
are only invoked when needed) can, therefore, lead to substantial
performance gains.

\subsubsection{Particle Gibbs}
\label{sec:proginf:str:part:pgibbs}

Provided one does not try to support the special treatment of global variables that particle
Gibbs allows (i.e. restricting to the iterated CSMC case), extending SMC to particle Gibbs
is relatively straightforward in our framework.  From a theoretical perspective, the algorithm
extends from the SMC case in the same way as outside of the probabilistic programming framework.
From a practical perspective there are two distinct challenges. Firstly, resampling for CSMC
sweeps does not maintain the target distribution in the same way as SMC~\citep{holenstein2009particle} and so one has
to be careful that there is no possibility for gratuitous resampling or missing a required resampling step (e.g. 
we cannot use the adaptive resampling discussed in Section~\ref{sec:part:smc:prat:ad-re}).
At first this would make it
seem like one would need to take care not to resample after the $n_y^{\mathrm{th}}$ observation.
However, resampling and choosing the retained particle uniformly at random from the
present particles turns out to be identical to sampling the retained particle in proportion to weight and
so this is, in fact, not a problem.  Secondly, we need a method of storing and retrieving the
state of the retained particle in a manner that allows other particles to inherit from it.  Given our
inference methods do not use a stack, this is done by storing the raw $x_{1:n_x}$ samples within
\angstate for each particle and then retrieving them for the retained particle.  The retained
particle is thus re-run in a deterministic fashion from a stored $x_{1:n_x}$, regenerating
all the variables in the program and the continuation.  This is achieved by
editing the \sample checkpoint so that it deterministically retrieves the $x_j$ for the 
retained particle, but samples normally for other particles.  For all particles it is also stores the current
$x_j$ in case this particle becomes the retained particle at the next sweep.
Provided one is running a reasonably large number of particles the extra computational cost for
re-running the retained particle is negligible, but the need to store all the $\{\hxnx^n\}_{n=1}^{N}$
can noticeably increase the memory requirements.  Other than these minor complications, the
extension from SMC to particles Gibbs in the probabilistic programming setting is identical to
that for conventional settings as disucssed in Section~\ref{sec:part:pmcmc:pgibbs}.

\subsubsection{Interacting PMCMC}
\label{sec:proginf:str:part:ipmcmc}

Given the particle Gibbs and SMC implementations, iPMCMC can be implemented
relatively simply by using those implementations for the CSMC and SMC sweeps respectively.
All checkpoint implementations are inherited from particle Gibbs and the extension does
not have any distinct challenges unique to probabilistic programming.  We note though that
the Anglican implementation of iPMCMC exploits the algorithmic ability for parallelization 
by creating a pool of threads to distribute computation of the different nodes.   Consequently,
the implementation has substantial computational benefits over that of say particle Gibbs, in addition
to the improved per-sample performance demonstrated in Section~\ref{sec:part:ipmcmc}.

\subsection{Other Methods}
\label{sec:proginf:str:part:other}

Though it would be impractical to do justice to all of the methods one can use for general
purpose in universal PPSs, we briefly lay out another of other algorithms of particular note.

The particle cascade~\citep{paige2014asynchronous} is an asynchronous and anytime
	variant of SMC that can provide improved in-sweep parallelization compared to SMC
	and an alternative to PMCMC methods for overcoming memory restrictions in
	SMC.  It has the crucial advantage over PMCMC methods of maintaining an unbiased estimate
	of the marginal likelihood, but it can suffer instability issues in the number of particles produced
	by the schedule strategy during a sweep. It also does not permit any distinct treatment of global
	variables.

Particle Gibbs with ancestor sampling (PGAS)~\citep{lindstenJS2014,vandemeent_aistats_2015}
	looks to alleviate degeneracy for the particle Gibbs algorithm by performing ancestor sampling
	for the retained particle.  This can substantially improve mixing for the early samples, but can
	result in very large computational overheads, sometimes requiring $O(Nn_y^2)$ computation
	(compared to $O(Nn_y)$).  This can be partly 
	mitigated by using database techniques to avoid unnecessary
	repeated computation~\citep{vandemeent_aistats_2015}, but is still typically substantially more computationally intensive than
	standard particle Gibbs.  
	%Nonetheless, this extra computation is sometimes justified for challenging
	%problems with severe degeneracy.

Some black-box variational methods~\citep{ranganath_aistats_2014} 
	have been developed for general purpose PPSs~\citep{kucukelbir2015automatic} including Anglican~\citep{vandemeent2016black,paige2017thesis}.  Though these can suffer from difficulties
	in finding effective addressing schemes and or high variance of the ELBO gradients, they can still
	form an effective approximate approach for certain problems.

\subsection{Which Anglican Inference Algorithm Should I Use?}
\label{sec:proginf:str:which}

The question of which inference algorithm one should use is a critical, but somewhat subjective and
problem dependent question.  We now provide some (inevitably biased) practical recommendations for which
inference algorithms to use in Anglican in its current form.  These
should be taken as a rough starting point rather than clear cut truth.

Firstly, except for very low dimensional problems, importance sampling should
be avoid as it is almost universally worse than, e.g. SMC.  Similarly, there is very little reason to use PIMH
instead of SMC or other PMCMC methods, while RMH or ALMH should generally be preferable to vanilla LMH.
The relative performance of the MCMC
based and particle based (including PMCMC) methods will depend on the amount of structure that can
be exploited in the system by the respective approaches.  The particle based approaches will generally be substantially
preferable when the \observe statements are interlaced with the \sample statements so that the intermediate
information can be exploited.  However, if all the variables need to be sampled before making observations, or if
all the variables are completely independent of one another given the data, SMC will reduce to importance sampling,
while the MCMC methods can still take advantage of hill-climbing effects.

If using SMC or PMCMC based methods, then, as per Section~\ref{sec:part:smc:prat:part}, the most important
thing is not the exact algorithm, but ensuring that sufficient particles are used.  Because PMCMC methods
in Anglican do not use separate updates for global parameters, using enough particles
can be even more important than in conventional settings.  Presume that your computational budget is $M$
particles and your memory budget is $N$ particles.  Our recommendation is that if $M<N$, you should look
to run a single SMC sweep with $M$ particles -- all the more advanced algorithms are mostly setup to avoid
issues that are easily solved by running more particles when you can.  For larger values of $M$, then you should
keep the number of particles as high as possible (ideally $N$).  Our recommendation here is to use iPMCMC
as the go-to algorithm as this will rarely be noticeably worse than the other particle based approaches and sometimes
substantially better, both in terms of per-sample performance and its support for parallelization.
Two possible exceptions to this are that PGAS and the particle cascade can sometimes still be better
when $M\gg N$, with the later also supporting effective parallelization of computation.  
For example, PGAS can be very effective in models where there are extreme restrictions on
the number of particles that can be run, e.g. due to calling an expensive external simulator.
