% !TEX root = ../main.tex

\section{Differing Approaches}
\label{sec:probprog:two}

PPSs allow users to define probabilistic models 
using a domain-specific programming language or modeling library. A probabilistic program implicitly 
defines a distribution on random variables, whilst the system back-end implements 
general-purpose inference methods.  As such, rather than being a clearly defined particular algorithm,
probabilistic programming is more of an umbrella term that covers a spectrum of 
different approaches, varying from inference toolboxes, through to languages that allow
arbitrary probabilistic code to be written, even that which might not correspond to a valid
model.  Often there is a trade-off between efficiency and expressivity: the more restricted
one makes the language, the more those restrictions can be exploited to improve the efficiency
of the inference.  This leads itself two distinct philosophies when developing a language: 
start with a particular inference algorithm and then design a language around making it as
easy as possible to write models for which that inference algorithm is suitable, or start with a
general as possible a language to allow any possible model to be written and then try to construct
inference engines that are capable of work in such a general framework.  Both approaches 
have their merits and drawbacks, with the distinction typically coming down to the intended use.
We will now elucidate each approach more precisely.  

\subsection{Inference Driven Systems}
\label{sec:probprog:two:inf}

Though there is a plethora of bespoke inference algorithms design for particular models, the vast majority of these are based around
a relatively small number of foundational methods such as importance sampling, sequential Monte Carlo,
Metropolis-Hastings, Gibbs sampling, message passing, and variational inference (see Chapter~\ref{chp:inf}
for an introduction to Bayesian inference).
The extensive use of these core inference approaches throughout Bayesian statistics and machine
learning means that it makes clear sense to write packages for automating them and to provide packages
that make it easy for the user to define an appropriate graphical models for which the inference can be automated.
This both improves efficiency of modeling and reduces barriers to effective Bayesian modeling by reducing the
required inference expertise for users.  This inference-first philosophy is taken by a number of successful PPSs
and inference toolboxes (the distinguishing line between which can be a little blurry) such as, amongst many others,
\begin{itemize}
	\item BUGS (Bayesian inference Using Gibbs Sampling) \citep{spiegelhalter1996bugs} and its 
	extensions~\citep{lunn2000winbugs,plummer2003jags,todeschini2014biips}
	allows finite DAGs to be specified using declarative code or pictorially using a graphical user
	interface.  These are converted to a form that is suitable for inference, the exact nature of which
	depends on the implementation.
	\item Infer.Net \citep{minka_software_2010} is modeling language for defining, and automating approximate inference in,
	both DAGs and Markov random fields, using predominantly message-passing algorithms. Distributions
	are predominantly, though not exclusively, restricted to be exponential families.  Branching (i.e. \texttt{if} statements) 
	is allowed, but requires enumeration of all possible paths.
	\item LibBi \citep{murray2013bayesian} is a package for doing Bayesian inference for state-space models,
	using particle-based inference methods (see Chapter~\ref{chp:part}).  There is a strong focus on scalable
	computation, providing support for multi-core architectures and graphics processing units.
	\item PyMC3 \citep{salvatier2016probabilistic} is a python framework for carrying out MCMC and variational
	inference, using Theano~\citep{bergstra2010theano} to calculate the gradients required by some inference methods.
	\item Stan \citep{carpenter2015stan} in a PPS with interfaces to many difference languages with a
	focus on performing Hamiltonian Monte Carlo inference~\citep{duane1987hybrid,hoffman2014no}, though
	other inference methods such as variational inference are provided as well~\citep{kucukelbir2015automatic}.
	As with PyMC3, automatic differentiation is used to calculate required gradients.  The need to take
	derivatives, means that there is limited support for discrete variables or branching.
\end{itemize}
These systems do not allow users to write models that would be difficult (at least for
an expert) to code without a PPS -- in general they all can be thought of as defining a graphical model
or sometimes factor graph -- but they offer substantial utility through ease of model exposition and
automating inference.

\subsection{Flexibility Driven Systems}
\label{sec:probprog:two:general}

Our focus will instead be on systems such as Church \citep{goodman_uai_2008}, Venture \citep{mansinghka2014venture}, 
WebPPL \citep{goodman_book_2014}, and Anglican \citep{wood2014new}, which employ a general-purpose programming 
language for model specification. In these systems, the set of random variables is dynamically typed, such that it is possible 
to write programs in which this set differs from execution to execution.  This allows an unspecified number of random 
variables and incorporation of arbitrary black box deterministic functions. The price for this expressivity is that inference 
methods must be formulated in such a manner that they are applicable to models where the density function is intractable 
and can only be evaluated during forwards simulation of the program. 

\todo[inline]{write me, better section title}
