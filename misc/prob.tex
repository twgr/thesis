% !TEX root = ../main.tex

\chapter{A Brief Introduction to Probability Theory}
\label{chp:prob}

Before going into the main content of the paper, we first provide a quick primer on probability
theory, providing some essential background and outlining conventions
that we will use throughout the thesis.  Readers familiar with the differences between a probability
and a probability density and between a random variable and an outcome may wish skip to this
Chapter, referring back as needed for clarification on any conventions undertaken.  Others
will hopefully find it to be a gentle introduction to the key concepts that will be needed to
be understood to follow the rest of the thesis.  Notation has been chosen with accessibility 
as the primary aim and we will avoid
the use measure theory except were it is absolutely necessary.  Nonetheless, we note that a measure theoretic
approach to probability is essential for a more rigorous understanding and refer
the interested reader to~\cite{durrett2010probability}.

\section{Random Variables, Outcomes, and Events}
\label{sec:prob:random}

A \emph{random variable} is a variable
whose realization is currently unknown, such that it can take on multiple different
values or \emph{outcomes}.\footnote{Technically speaking, outcomes are points in \emph{sample
		space} and random variables are measurable functions from outcomes to a measurable
	space.  As we said though, we are trying to avoid the niceties of measure theory when
	 possible\dots}
A set of one of more outcomes is known as an \emph{event}.
For example, if we roll a fair six-sided dice then the result of the roll is a random variable, 
while rolling a $4$ is both a possible outcome and a possible event.  Rolling a number greater
or equal to $5$ on the other hand, is a possible event but not a possible outcome: it is a set
of two individual outcomes, namely rolling a $5$ and rolling a $6$.  Outcomes are
\emph{mutually exclusive}, i.e. if its not possible for two separate outcomes to occur in a
particular trial, but events are not - for example it is possible for both the events that we
roll and even number and we roll a number greater than $3$ to occur.

\section{Probabilities}
\label{sec:prob:prob}

A \emph{probability} is
the chance of an event occurring.  For example, if we denote the output
of our dice roll as $X$, then we can say that $P(X=4) = 1/6$ or that $P(X\le3) = 0.5$.  Here
$X=4$ and $X\le3$ are events for the random variable $X$ with probabilities of $1/6$ and $0.5$
respectively.   A probability of $0$ indicates that an event has no chance of happening, 
for example the probability that we role an $8$, while
a probability of $1$ indicates it is certain to happen, for example the probability 
that we role a number less than $7$.  All probabilities much thus lie (inclusively) between $0$ and $1$.  
We will regularly use the shorthand $P(x)$ to denote the probability of the event $P(X=x)$.  
We reiterate the importance distinction between the random variable $X$ and the outcome $x$:
the former has an unknown value (e.g. the result of the dice roll) and the latter is
a fixed possible realization of the random variable (e.g. rolling a $4$).
All the same, we will at times be intentionally carefree about delineating between
random variables and outcomes, except for when the distinction is explicitly necessary.

Somewhat surprisingly, there are two competing (and often incompatible)
formal definitions for what probabilities actually mean.  The frequentist definition of
probability is that it is the average proportion of the time an event will occur if the trial is 
repeated infinitely many times.  The Bayesian definition of probability is that it is the subjective
belief that an event will occur in the process of incomplete information.  Both viewpoints have
strengths and weaknesses and we will avoid being drawn into one of the biggest debates in
science, noting only that the philosophical differences between the two are typically completely
detached from the practical differences between the resulting
algorithms~\citep{steinhardt2012beyond}, despite the former all too often be used to argue
the superiority of one approach over another.

A \emph{conditional probability} is the probability of an event given that another event has occurred.
For example, the conditional probability that we roll a $4$ with a dice given that we have rolled a $3$ or
higher is $P(X=4 | X\ge3) = 0.25$.  More typically, we will condition on events that are separate but
correlated to the event we care about.  For example, the probability of dying of lung cancer is higher
is you smoke.  The process of updating a probability using the information from another event is
known as conditioning on that event.  For example, one can condition the probability that a football team
will win the league this seasons on the results from their first few games.  

\section{The Laws of Probability}
\label{sec:prob:laws}

Though not technically axiomatic, the mathematical laws of probability can be summarized by the \emph{product rule}
and the \emph{sum rule}.  Remarkably, almost all of probability stems from these two simple rules.
The product rule states that the probability of two events occurring is the probability of one of the events
occurring times the probability of the over event happening given the first event happened, namely
\begin{align}
\label{eq:prob:prod}
P(A,B) := P(A \cap B) = P(A|B) P(B) =  P(B|A) P(A)
\end{align}
where we have introduced $P(A,B)$ as a shorthand for the probability that both the events $A$ and $B$ occur.
An immediate consequence of the product rule is Bayes' rule,
\begin{align}
P(A|B) = \frac{P(B|A)P(A)}{P(B)},
\end{align}
which we return at length in Section~\ref{sec:bayes:paradigm}.

The sum rule has a number of different representations, the most general of which is that 
the probability that either $A$ or $B$ occurs, $P(A\cup B)$, is given by
\begin{align}
\label{eq:prob:sum}
P(A\cup B) = P(A) + P(B) - P(A, B).
\end{align}
The intuition of the sum rule is perhaps easiest to see by considering that
\[
P(B) - P(A \cap B) = P(B)(1-P(A|B)) = P(B, \neg A)
\]
 is the probability of $B$ and 
not $A$.  $A\cup B$ can only occur if $A$ occurs or if $B$ and not $A$ occurs.  As it is not
possible for both these events to occur, the probability of either event must be the sum of the
probability of each separate event.
There are a number of immediate consequences of the sum rule.  For example, if $A$ and $B$ are
mutually exclusive then $P(A\cup B) = P(A) + P(B)$.  As outcomes are mutually exclusive, it
follows from the sum rule and the axioms of probability that the sum of the probabilities
for each possible outcome is equal to $1$.  We can also use this to
define the concept of \emph{marginalizing} out a random variable $Y$ as
\begin{align}
\label{eq:prob:marginal}
P(X=x) = \sum_{i} P(X=x,Y=y_i)
\end{align}
where the sum is over all the possible outcomes of $Y$.  Here $P(X=x)$ is known as the
\emph{marginal probability} of $X$ and $P(X=x,Y=y)$ as the \emph{join probability} of $X$
and $Y$.

Conditional probabilities follow  the same key results as unconditional probabilities, but it 
should be noted that they do not define probability distributions over the conditioning term.  
For example, $P(A|B)$ is a probability distribution over $A$ with all the corresponding 
requirements, but is not a distribution over $B$.  Therefore,
for example, it is possible to have for example $\sum_{i} P(A|B=b_i) >1$.

\section{Probability Densities}
\label{sec:prob:den}

Thus far we have presumed that our random variables a discrete, i.e. that there is some fixed
number possible outcomes.\footnote{Technically speaking, discrete random variables can also
	take on a \emph{countable} number of values.  For example, the Poisson distribution is defined
	over $0,1,2,\dots,\infty$ and is discrete distribution with infinite possible outcomes.  
	However, this countable infinity is much smaller that the \emph{uncountably infinite} number
	of possible outcomes for continuous random variables.}
Things get somewhat more complicated if our variables are continuous.  Consider for example
the probability that a runner takes exactly $\pi$ hours to run a marathon $P(X=\pi)$.  
Clearly the probability
of this particular event is zero, $P(X=\pi)=0$, as is the probability of the runner taking any other exact time
to complete the race: we have an infinite number of possible outcomes, each with zero probability
(presuming the runner finishes the race).  Thankfully, the notion of an event that we previously
introduced comes to our rescue.  For example, the event that the runner takes between $3$ and
$4$ hours is has non-zero probability: $P(3\le X \le 4) \neq 0$.  Here our event itself include
an infinite number of possible outcomes and even though each individual outcome had
zero probability, the combination of \emph{uncountably infinite} many such outcomes need
not also have zero probability.  To more usefully characterize probability in such cases, we can
define a \emph{probability density function} which reflects the relative probability of areas of
the space of outcomes.  We can define this more precisely by considering the probability
of being some small area of the space of size $\delta x$.  Presuming that the probability density
$p_{X}(x)$ is roughly constant within our small area, we can say that 
$p_{X}(x)\delta x \approx P(x\le X <x+\delta x)$ and thus 
$p_{X}(x) = \lim\limits_{\delta\rightarrow0} \frac{P(x\le X <x+\delta x)}{\delta x}$.  More formally
we can define the probability density as satisfying
\begin{align}
\label{eq:prob:density}
P(X\in \mathcal{A}) = \int_{x\in\mathcal{A}} p_{X}(x) dx
\end{align}
where $X\in \mathcal{A}$ means the event that $X$ is in $\mathcal{A}$.  We can further
use this to define the \emph{cumulative distribution function} $P(X\le x)$, which is the probability
that $X$ is less than equal to the outcome $x$
\begin{align}
\label{eq:prob:cumulative}
P(X\le x) = \int_{-\infty}^{x} p_{X}(u) du,
\end{align}
where $u$ is a dummy variable.

Most of the key rules of probability theory, such as the sum rule, the product rule, and Bayes
rule, apply equally well to continuous and discrete random variables.  All that is required
is that we replace the summations with integrals over the appropriate variable. 

In the rest of the thesis will drop the notation $p_{X}(x)$, using simply $p(x)$ instead.
The main rationale for this is will we regularly use probability density functions that we do
not actually sample from.  For example, in importance sampling we will sample from one
distribution but evaluate its density under another.  In these scenarios, it may not be possible
to link a random variable to each density.  We will instead make it explicit what distribution
of density a random variable is drawn from using the notation $X\sim p(x)$.  For the reasons
outlined in the next section, we will also often be carefree at distinguishing between probabilities
and probability densities as this oft-used convention will regularly be highly convenient when
dealing with variables that could be discrete or continuous.  We will also regularly be carefree about
distinguishing between random variables and outcomes by using loose notations such as
$x\sim p(x)$ when the delineation is not necessary in the context.

%As in
%the case of probabilities, we will usually use the short hand $p(x)$ for the probability density
%$p(X=x)$ outside of this Chapter.

\section{Measures}
\label{sec:prob:measure}

Consider now if there is also a probability that the runner does not finish the race which
we denote as the outcome $X=\infty$.  As we
have thus-far introduced them, neither the concept of a probability or a probability density
seem to be suitable for this case: every outcome other than $X=\infty$ has zero probability,
but $X=\infty$ seems to have infinite probability density.  To solve this conundrum we 
have to briefly break our promise to avoid measure theory.  A measure can be thought of
as something that assigns a size to a set of objects.  In the context of probability theory,
measures are used to assign probabilities to events, remembering that events represent 
sets of outcomes.  The measure assigned to event including all possible outcomes is thus $1$,
the measure assigned to the empty set is $0$.  We can generalize the concept of a probability 
density by defining if with respect to an appropriate measure.  In the context of~\eqref{eq:prob:density}, we can informally think of the $dx$ term as representing the measure.
This will now allow us to
deal with more complicated scenarios by setting up appropriate measures to the problem at hand.

In the continuous example
we implicitly used (a scaled version of)\footnote{In more formal settings, one normally defines
	 probability measures on the $\sigma$-algebra of the sample space so that the 
	 Lebesgue measure is used exactly.  Here we are defining them on the 
	 realizations of random variables themselves for exposition and so are used a scaled version
	 of the Lebesgue measure.  The same reasoning applies to our use of the counting measure.}
the \emph{Lebesgue measure} which corresponds to
the standard euclidean notion of size, coinciding with the concepts of length, area, and
volume in 1, 2, and 3 dimensions respectively.  In the discrete examples, we have implicitly
used the notion of a counting measure which simply counts the number of outcomes which
lead to a particular event.   For the example where the runner might not finish, we can
use a \emph{mixed measure} that is some scaling of the counting measure for the outcome
$X=\infty$ and a scaling of the Lebesgue measure everywhere else.  

Because of this generalization, we will often use densities to represent both discrete
and continuous probabilities throughout the thesis.  In particular, this will be convenient
for representing results that apply to both cases.

It is important to note that when we define densities-measure pairs in this way, these 
definitions are not in general not unique for an underlying random process.  For example, 
in the case of the runner who might not finish the marathon, we could assign any density 
we want to the event $T=\infty$ provided we adjust the respective measure accordingly.
A particularly important case of this is in a so-called \emph{change of variables}.  Imagine
that a random variable $Y=g(X)$ is a deterministic function of another random variable $X$.
Given a probability density function for $X$, we can define a probability density function
on $Y$ using
\begin{align}
\label{eq:prob:change}
p(y)dy = p(x)dx = p(g^{-1}(y))dx
\end{align} 
where $p(y)$ and $p(x)$ are the respective probability densities for $Y$ and $X$
respectively.  Rearranging we see that, for one dimensional problems, 
\begin{align}
\label{eq:prob:change2}
p(y) = \left|\frac{dg^{-1}(y))}{dy}\right|p(g^{-1}(y)).
\end{align}
For the multidimensional case, the derivative is replaced by the determinant of the
Jacobian for the inverse mapping.  Note that by~\eqref{eq:prob:density}, changing
variables does not change the value of actual probabilities or exceptions (see
 Section~\ref{sec:prob:expt}).  However,~\eqref{eq:prob:change}
still has the importance consequence that the optimum of a probability distribution
depends on the parameterization.  For example, if we parameterizing the same model with
either $X$ or $\log X$ will lead to different value of the most likely value of the parameter
$x^*$, i.e.
\begin{align}
x^* = \argmax_{x} p(x) \neq g^{-1} \left(\argmax_{g(x)} p(g(x))\right)
\end{align}
in general.

\section{Expectations and Variances}
\label{sec:prob:expt}

The \emph{expected value} $\E [X]$, or mean, of a random variable $X$ is the average value that the variable
will take if an infinite number of independent draws are made.  Its definition is easiest convey using 
probability density notation
\begin{align}
\label{eq:prob:expt}
\E [X] = \int x p(x)dx.
\end{align}
In the discrete case this leads to $\E [X] = \sum_{i} x_i P(X=x_i)$.  Because expectations are defined by a random
variable (rather than a distribution), they
average over all the contained randomness, e.g. $\E [f(X,Y)]=\iint f(x,y) p(x,y)dxdy$.  However, if we
wish to average only with respect to part of the randomness in a system, we can instead use a conditional
expectation for example
\begin{align}
\label{eq:prob:cond-expt}
\E [f(X,Y) | Y=y] = \int f(x,y) p(x|y) dx,
\end{align}
for which we will sometimes use the shorthand $\E [f(X,Y) | Y]$.

It will also sometimes be convenient to implicitly define the random variable and conditioning for an 
expectation for which we use the slightly loose notation
\begin{align}
\label{eq:prob:expt-2}
\E_{p(x|y)} \left[f(x,y,z)\right] = \int f(x,y,z) p(x|y) dx,
\end{align}
where we have implicitly defined the random variables $Y \sim p(y)$ and $X \sim p(x | Y=y)$,
we are calculating $\E \left[f(X,Y,z) | Y=y\right]$,
and this resulting expectation is a function of $z$.  One can informally think about this
as being the expectation of $f(x,y,z)$ with respect to $p(x|y)$:
our expectation is only over the randomness associated with drawing from $p(x|y)$.  Thus,
for example, we are not taking an expectation over nor condition upon $z$ in this example
-- it is treated as a fixed input, even if it is a random variable externally to this expectation.
Depending on the context, we will switch between these different notations for expectation
and will sometimes simply using the integral form directly.

Denoting the mean of a random variable $X$ as $\mu=\E [X]$, the \emph{variance} of $X$
is defined using any one of the following equivalent forms
\begin{align}
\var (X) = \E \left[(X-\mu)^2\right] = \int (x-\mu)^2 p(x)dx = \E [X^2] - \mu^2 =
\int x^2 p(x) dx - \mu^2.
\end{align}
In other words, it is the average squared distance of a variable from its mean.
Its square root, the \emph{standard deviation}, informally forms an estimate for the average
amount of variation of the variable from its mean and has units which are the same
as the data.  We will use the same notational conventions as for expectations when defining
variances.

The variance is a particular case of the more general concept of  a covariance between
two random variables $X$ and $Y$.  Defining $\mu_X=\E [X]$ and $\mu_Y=\E [Y]$
then the covariance is defined by any one of the following equivalent forms
\begin{align}
\cov& (X,Y) = \E \left[(X-\mu_X)(Y-\mu_Y)\right] = \iint (x-\mu_X)(y-\mu_Y) p(x,y) dxdy \nonumber \\
&= \E \left[XY\right]-\E \left[X\right]\E \left[Y\right] = \iint xyp(x,y) dxdy - \left(\int xp(x) dx\right)
\left(\int yp(y) dy\right).
\end{align}
The covariance between two variables measures the joint variability of two random
variables.  It is perhaps easiest to interpret through the definition of correlation
(or more specifically Pearson's correlation coefficient) which is the correlation
scaled by the standard deviation of each of the variables:
\begin{align}
\corr (X,Y) = \frac{\cov (X,Y)}{\sqrt{\var(X) \var(Y)}}.
\end{align}
The correlation between two variables is always in the range $[-1,1]$.  Positive
correlations indicate that when one variable is relatively larger, the other variable
also tends to be larger.  The higher the correlation, the more strongly this relationship
holds: if the correlation is $1$ then one variable is \emph{linearly} dependent on the other.  The
same hold for negative correlations except that when one variable increases, the other tends
to decrease.  Independent variables have a correlation (and thus covariance) of zero.
Note that correlation is not causation.