% !TEX root = ../main.tex

\chapter{A Brief Introduction to Probability Theory}
\label{chp:prob}

Before going into the main content of the paper, we first provide a quick primer on probability
theory, giving some essential background and outlining conventions
that we will use throughout the thesis.  Readers familiar with the differences between a probability
and a probability density and between a random variable and an outcome may want to skip to this
chapter, referring back as needed for clarification on any conventions undertaken.  Others
will hopefully find it to be a gentle introduction to the key concepts that will be needed to
be understood to follow the rest of the thesis.  Notation has been chosen with accessibility 
as the primary aim and we will avoid
the use measure theory where possible.  Nonetheless, a measure theoretic
approach to probability is essential for a more rigorous understanding and refer
the interested reader to~\cite{durrett2010probability}.

\section{Random Variables, Outcomes, and Events}
\label{sec:prob:random}

A \emph{random variable} is a variable
whose realization is currently unknown, such that it can take on multiple different
values or \emph{outcomes}.\footnote{In a more formal, measure-theoretic framework, outcomes are points in \emph{sample
		space} and random variables are measurable functions from outcomes to a measurable
	space.}
A set of one of more outcomes is known as an \emph{event}.
For example, if we roll a fair six-sided dice then the result of the roll is a random variable 
while rolling a $4$ is both a possible outcome and a possible event.  Rolling a number greater
or equal to $5$, on the other hand, is a possible event but not a possible outcome: it is a set
of two individual outcomes, namely rolling a $5$ and rolling a $6$.  Outcomes are
\emph{mutually exclusive}, that is, it is not possible for two separate outcomes to occur for a particular trial, e.g.
we cannot roll both a $2$ and $4$ with a single throw.
Events, on the other hand, are not. For example, it is possible for both the events that we
roll and even number and we roll a number greater than $3$ to occur.

\section{Probabilities}
\label{sec:prob:prob}

A \emph{probability} is
the chance of an event occurring.  For example, if we denote the output
of our dice roll as $X$, then we can say that $P(X=4) = 1/6$ or that $P(X\le3) = 0.5$.  Here
$X=4$ and $X\le3$ are events for the random variable $X$ with probabilities of $1/6$ and $0.5$
respectively.   A probability of $0$ indicates that an event has no chance of happening, 
for example the probability that we role an $8$, while
a probability of $1$ indicates it is certain to happen, for example, the probability 
that we roll a positive number.  All probabilities must thus lie between $0$ and $1$ 
(inclusively).  
A \emph{distribution} of a random variable provides the probabilities of each possible 
outcome for that random variable occurring.

Though, we will regularly use the shorthand $P(x)$ to denote the probability of the 
event $P(X=x)$,
we reiterate the important distinction between the random variable $X$ and the outcome $x$:
the former has an unknown value (e.g. the result of the dice roll) and the latter is
a fixed possible realization of the random variable (e.g. rolling a $4$).
All the same, we will at times be intentionally carefree about delineating between
random variables and outcomes, except for when the distinction is explicitly necessary.

Somewhat surprisingly, there are two competing (and sometimes incompatible)
interpretations of probability.  The frequentist interpretation of
probability is the average proportion of the time an event will occur if a trial is 
repeated infinitely many times.  The Bayesian interpretation of probability is that it is the subjective
belief that an event will occur in the process of incomplete information.  Both viewpoints have
strengths and weaknesses and we will avoid being drawn into one of the biggest debates in
science, noting only that the philosophical differences between the two are typically completely
detached from the practical differences between the resulting machine learning or statistics
methods (see Section~\ref{sec:bayes:religions}), despite these philosophical differences all too 
often being used to argue the superiority of the resultant algorithms~\citep{gelman2011induction,steinhardt2012beyond}.

\section{Conditioning and Independence}
\label{sec:prob:cond}

A \emph{conditional probability} is the probability of an event given that another event has occurred.
For example, the conditional probability that we roll a $4$ with a dice given that we have rolled a $3$ or
higher is $P(X=4 | X\ge3) = 0.25$.  More typically, we will condition upon events that are separate but
correlated to the event we care about.  For example, the probability of dying of lung cancer is higher
if you smoke.  The process of updating a probability using the information from another event is
known as conditioning on that event.  For example, one can condition the probability that a football team will win the league on the results from their first few games.

Events are \emph{independent} if the occurrence of one event does not affect the probability of 
the occurrence of the other event.  Similarly, random variables are independent if the outcome of
one random variable does not affect the distribution of the other.  Independence of random variables
indicates the probability of that variable is the same as the conditional probability given the other
variable, i.e. if $X$ and $Y$ are independent, $P(X=x)=P(X=x|Y=y)$ for all possible $y$.  Note that 
independence does not necessarily carry over when adding or removing a conditioning:
if $X$ and $Y$ are independent, this does not necessarily mean that $P(X=x|A) = P(X=x|A,Y=y)$
for some event $A$.  For example, the probability that the next driver to pass a speed camera
is speeding and that the speed camera is
malfunctioning can be reasonably presumed to be independent.  However, conditioned on the
event that the speed camera is triggered, the two are clearly not independent as if the camera is working,
this would indicate that the driver is speeding.
If $P(X=x|A) = P(X=x|A,Y=y)$ holds, then $X$ and
$Y$ are known as conditionally dependent given $A$.  In the same way that independence does not
imply conditional independence, conditional independence does not imply non-conditional independence.

\section{The Laws of Probability}
\label{sec:prob:laws}

Though not technically axiomatic, the mathematical laws of probability can be summarized by the \emph{product rule}
and the \emph{sum rule}.  Remarkably, almost all of probability stems from these two simple rules.
The product rule states that the probability of two events occurring is the probability of one of the events
occurring times the conditional probability of the other event happening given the first event happened, namely
\begin{align}
\label{eq:prob:prod}
P(A,B) := P(A \cap B) = P(A|B) P(B) =  P(B|A) P(A)
\end{align}
where we have introduced $P(A,B)$ as a shorthand for the probability that both the events $A$ and $B$ occur.
An immediate consequence of the product rule is Bayes' rule,
\begin{align}
P(A|B) = \frac{P(B|A)P(A)}{P(B)},
\end{align}
which we return at length in Section~\ref{sec:bayes:paradigm}.
Another is that for independent random variables, the joint distribution is the product of the
individual probabilities: $P(A,B)=P(A)P(B)$.

The sum rule has a number of different representations, the most general of which is that 
the probability that either $A$ or $B$ occurs, $P(A\cup B)$, is given by
\begin{align}
\label{eq:prob:sum}
P(A \cup B) = P(A) + P(B) - P(A, B).
\end{align}
The intuition of the sum rule is perhaps easiest to see by considering that
\[
P(B) - P(A, B) = P(B)(1-P(A|B)) = P(B, \neg A)
\]
 is the probability of $B$ and 
not $A$.  Now $A\cup B$ can only occur if $A$ occurs or if $B$ occurs and not $A$.  As it is not
possible for both these events to occur, the probability of either event must be the sum of the
probability of each separate event, leading to~\eqref{eq:prob:sum}.
There are a number of immediate consequences of the sum rule.  For example, if $A$ and $B$ are
mutually exclusive then $P(A\cup B) = P(A) + P(B)$.  As outcomes are mutually exclusive, it
follows from the sum rule and the axioms of probability that the sum of the probabilities
for each possible outcome is equal to $1$.  We can also use this to
define the concept of \emph{marginalizing} out a random variable $Y$ as
\begin{align}
\label{eq:prob:marginal}
P(X=x) = \sum_{i} P(X=x,Y=y_i)
\end{align}
where the sum is over all the possible outcomes of $Y$.  Here $P(X=x)$ is known as the
\emph{marginal probability} of $X$ and $P(X=x,Y=y)$ as the \emph{joint probability} of $X$
and $Y$.

Conditional probabilities follow  the same key results as unconditional probabilities, but it 
should be noted that they do not define probability distributions over the conditioning term.  
For example, $P(A|B)$ is a probability distribution over $A$ with all the corresponding 
requirements, but is not a distribution over $B$.  Therefore,
for example, it is possible to have $\sum_{i} P(A|B=b_i) >1$.  We instead refer to $P(A|B)$
as the \emph{likelihood} of $B$, given the occurrence of event $A$.

\section{Probability Densities}
\label{sec:prob:den}

Thus far we have presumed that our random variables are discrete, i.e. that there is some fixed
number of possible outcomes.\footnote{Technically speaking, discrete random variables can also
	take on a \emph{countably infinite} number of values, e.g. the Poisson distribution is defined
	over $0,1,2,\dots,\infty$.  
	However, this countable infinity is much smaller than the \emph{uncountably infinite} number
	of possible outcomes for continuous random variables.}
Things get somewhat more complicated if our variables are continuous.  Consider for example
the probability that a runner takes exactly $\pi$ (i.e. $3.14\dots$) hours to run a marathon $P(X=\pi)$.  
Clearly, the probability
of this particular event is zero, $P(X=\pi)=0$, as is the probability of the runner taking any other exact time
to complete the race: we have an infinite number of possible outcomes, each with zero probability
(presuming the runner finishes the race).  Thankfully, the notion of an event that we previously
introduced comes to our rescue.  For example, the event that the runner takes between $3$ and
$4$ hours has non-zero probability: $P(3\le X \le 4) \neq 0$.  Here our event includes
an infinite number of possible outcomes and even though each individual outcome had
zero probability, the combination of \emph{uncountably infinitely} many such outcomes need
not also have zero probability.  To more usefully characterize probability in such cases, we can
define a \emph{probability density function} which reflects the relative probability of areas of
the space of outcomes.  We can define this by considering the probability
of being some small area of the space of size $\delta x$.  Presuming that the probability density
$p_{X}(x)$ is roughly constant within our small area, we can say that 
$p_{X}(x)\delta x \approx P(x\le X <x+\delta x)$ and thus 
$p_{X}(x) = \lim\limits_{\delta\rightarrow0} \frac{P(x\le X <x+\delta x)}{\delta x}$.  More 
precisely we can define the probability density as satisfying
\begin{align}
\label{eq:prob:density}
P(X\in \mathcal{A}) = \int_{x\in\mathcal{A}} p_{X}(x) dx
\end{align}
where $X\in \mathcal{A}$ means the event that $X$ is in $\mathcal{A}$.  We can similarly
define the \emph{cumulative distribution function} $P(X\le x)$, which is the probability
that $X$ is less than equal to the outcome $x$
\begin{align}
\label{eq:prob:cumulative}
P(X\le x) = \int_{-\infty}^{x} p_{X}(u) du,
\end{align}
where $u$ is a dummy variable.
Most of the key rules of probability theory, such as the sum rule, the product rule, and Bayes
rule, apply equally well to continuous and discrete random variables.  All that is required
is that we replace the summations with integrals over the appropriate variable. 

In the rest of the thesis will drop the notation $p_{X}(x)$, using simply $p(x)$ instead.
The main rationale for this is that we will regularly use probability density functions that we do
not actually sample from.  For example, in importance sampling, we will sample from one
distribution but evaluate its density under another.  In these scenarios, it may not be possible
to link a random variable to each density.  We will instead make it explicit what distribution
a random variable is drawn from using the notation $X\sim p(x)$.  
%For the reasons
%outlined in the next section, we will also often be carefree at distinguishing between probabilities
%and probability densities as this oft-used convention will regularly be highly convenient when
%dealing with variables that could be discrete or continuous.  
However, we will regularly be carefree about
distinguishing between random variables and outcomes by using loose notations such as
$x\sim p(x)$ when the delineation is not necessary in the context.

%As in
%the case of probabilities, we will usually use the short hand $p(x)$ for the probability density
%$p(X=x)$ outside of this Chapter.

\section{Measures}
\label{sec:prob:measure}

Consider now if there is also a probability that the runner does not finish the race which
we denote as the outcome $X=\infty$.  As we
have thus-far introduced them, neither the concept of a probability or a probability density
seem to be suitable for this case: every outcome other than $X=\infty$ has zero probability,
but $X=\infty$ seems to have infinite probability density.  To solve this conundrum we 
have to briefly break our promise to avoid measure theory.  A \emph{measure} can be thought of
as something that assigns a size to a set of objects.
\emph{Probability measures} assign probabilities to events, remembering that events represent 
sets of outcomes, and thus are used to define a more formal notion of probability that we have
previously discussed. 
The measure assigned to an event including all possible outcomes is thus $1$,
while the measure assigned to the empty set is $0$.  

We can generalize the concept of a probability density to arbitrary random variables
by formalizing its definition as being with respect to an appropriate \emph{reference measure}.  Somewhat confusingly,
this reference measure  is typically not a probability measure.  Consider the case of the continuous densities examined
in Section~\ref{sec:prob:den}.
Here we have implicitly used the \emph{Lebesgue measure} as the reference measure, which corresponds to
the standard Euclidean notion of size, coinciding with the concepts of length, area, and
volume in 1, 2, and 3 dimensions respectively.  In~\eqref{eq:prob:density} then $dx$ indicated 
integration with respect to a Lebesgue measure, with $\int_{x\in\mathcal{A}} dx$ being equal
to the hypervolume of $\mathcal{A}$ (e.g. area of $\mathcal{A}$ in two dimensions).  
Our reference measure is, therefore, clearly not a probability measure as $\int_{x\in\real} dx = \infty$.
Our probability measure here can be informally thought of as $p(x)dx$, so that  
$\int_{x\in\mathcal{A}} p(x)dx = P(x\in\mathcal{A})$.\footnote{More formally, the density is derived
	from the probability measure and reference measure rather than the other way around:
	it is the Radon-Nikodym derivative of the probability measure with respect to the reference measure.}
In the discrete cases, we can define a probability density $p(x)=P(X=x)$ by
using the notion of a \emph{counting measure} for reference, which simply counts the number of outcomes which
lead to a particular event.   Note that we were not free to choose any arbitrary measure for any given
random variable.  
We cannot use a counting measure as reference for continuous random variables
or the Lebesgue measure for discrete random variables because, for example, the Lebesgue measure would assign zero
measure to all possible events for the latter.  Nonetheless, the reference measure we use is not necessarily
unique either.  For example, we could apply a constant $c$ scaling the reference measure and
then scale the density by $1/c$.  For notional convenience, we will refer to $dx$ (or equivalent) 
as our reference measure elsewhere in the thesis.

For the example where the runner might not finish, we can
use a \emph{mixed measure}.  Perhaps the easiest way to think about this is to think about
the runner's time $X$ as being a deterministic function of two random variables: the first
a discrete random variable $Y\in\{0,1\}$ that dictates where the runner finishes ($Y=1$) or not ($Y=0$) and the second 
a continuous random variable $Z\in\real^+$ that specifies a distribution over run times assuming the
runner finishes (such that $Z$ always exists but only equals $X$ if the runner finishes).  
%We can now describe our joint density as $p(y=Y,z=Z) = p(Y=y)p(z=Z)$ where $p(Y=y)$
%is defined with respect to a counting measure (such that $p(Y=y)=P(Y=y$) and $p(z=Z)$.
Now $X$ is a function of $Y$ and $Z$ and we can define the probability of the event
$X\in \mathcal{A}$ as follows
\begin{align*}
P(X \in \mathcal{A}) &= \sum_{y \in \{0,1\}} \int_{z\in \real} P(Y=y) p(z) \mathbb{I}(X(y,z) \in \mathcal{A}) dz \\
&= P(Y=0) \mathbb{I}(\infty\in \mathcal{A})  + P(Y=1) \int_{z\in \mathcal{A}\backslash\infty} p(z) dz
\end{align*}
where $\mathbb{I}(\cdot)$ is the identity function evaluating to $1$ if its
input holds true and $0$ otherwise, and we have used the independence between 
$Y$ and $Z$.  By definition of a probability density, we also
have that $P(X \in \mathcal{A}) = \int_{x\in\mathcal{A}} p(x) d\mu(x)$ for density $p(x)$
with respect to measure $d\mu(x)$, where we switched notation from $dx$ to $d\mu(x)$ to express
the fact that the measure now depends explicitly on the value of $x$, i.e. our measure
is non-stationary in $x$.  For $x\neq\infty$ then it is natural for $d\mu(x)$ to correspond to the
Lebesgue measure, noting $\int_{x\in \real\backslash\infty} p(x) d\mu(x) = 1-P(X=\infty)$.
For $x=\infty$ it is natural for $d\mu(x)$ to correspond to some scaling of the counting
measure.  For discrete distributions it is natural to use the counting measure exactly (i.e. set the
scaling to one), but here this could lead to a slightly misleading density given that $X=\infty$
is infinitely times more probable than any other $X$.  For example, this could mean that
$\argmax_x p(x) \neq \infty$ which could quickly lead to confusion.  A more illustrative
choice might use the counting measure scaled by an arbitrarily small value (such that
the density is scaled by large value), though strictly speaking this scaling should not be $0$.

For most applications,\footnote{An important
	exception to this is optimization.}
the choice between valid reference measures will turn out to be
inconsequential (provided we are consistent in our choice when considering multiple
densities), as it does not affect the distribution of random variables themselves 
or effect things such as expectations (see Section~\ref{sec:prob:expt}).  
The key point we wish to instead make is that, regardless of variable type, we can
(implicitly) always define a probability density function.\footnote{Well, almost always.  But we
	are not going to open that can of worms, see e.g.~\cite{durrett2010probability}.}  
The generalization also allows us
to put probability distributions of variables that are neither discrete nor continuous type.
For example, we might want to put a distribution over the space of strings.  
Because of this generalization, we will talk mostly in terms of probability densities throughout
this thesis, even though variables may often be discrete (for which the density will 
represent a probability).

We finish the section by considering the relationship between random variables which
are deterministic functions of one another.  This important case is known as a
\emph{change of variables}.  Imagine
that a random variable $Y=g(X)$ is a deterministic function of another random variable $X$.
Given a probability density function for $X$, we can define a probability density function
on $Y$ using
\begin{align}
\label{eq:prob:change}
p(y)dy = p(x)dx = p(g^{-1}(y))dg^{-1}(y)
\end{align} 
where $p(y)$ and $p(x)$ are the respective probability densities for $Y$ and $X$
with measures $dx$ and $dy$,  the latter of which is known as a \emph{push-forward} measure.
Rearranging we see that, for one-dimensional problems, 
\begin{align}
\label{eq:prob:change2}
p(y) = \left|\frac{dg^{-1}(y))}{dy}\right|p(g^{-1}(y)).
\end{align}
For the multidimensional case, the derivative is replaced by the determinant of the
Jacobian for the inverse mapping.  Note that by~\eqref{eq:prob:density}, changing
variables does not change the value of actual probabilities or exceptions (see
 Section~\ref{sec:prob:expt}).  However,~\eqref{eq:prob:change}
still has the important consequence that the optimum of a probability distribution
depends on the parameterization.  For example, parameterizing the same model with
either $X$ or $\log X$ will lead to a different most likely value of the parameter
$x^*$, i.e., in general,
\begin{align}
x^* = \argmax_{x} p(x) \neq g^{-1} \left(\argmax_{g(x)} p(g(x))\right).
\end{align}

\section{Expectations and Variances}
\label{sec:prob:expt}

The \emph{expected value} $\E [X]$, or mean, of a random variable $X$ is the average value that the variable
will take if an infinite number of independent draws are made.  Its definition is easiest to convey using 
probability density notation as follows
\begin{align}
\label{eq:prob:expt}
\E [X] = \int x p(x)dx.
\end{align}
In the discrete case this leads to $\E [X] = \sum_{i} x_i P(X=x_i)$.  Because expectations are defined by a random
variable (rather than a density), they
average over all the contained randomness, e.g. $\E [f(X,Y)]=\iint f(x,y) p(x,y)dxdy$.  However, if we
wish to average only with respect to part of the randomness in a system, we can instead use a conditional
expectation for example
\begin{align}
\label{eq:prob:cond-expt}
\E [f(X,Y) | Y=y] = \int f(x,y) p(x|y) dx,
\end{align}
for which we will sometimes use the shorthand $\E [f(X,Y) | Y]$.
It will also sometimes be convenient to implicitly define the random variable and conditioning for an 
expectation for which we use the slightly loose notation
\begin{align}
\label{eq:prob:expt-2}
\E_{p(x|y)} \left[f(x,y,z)\right] = \int f(x,y,z) p(x|y) dx,
\end{align}
where we have implicitly defined the random variables $Y \sim p(y)$ and $X \sim p(x | Y=y)$,
we are calculating $\E \left[f(X,Y,z) | Y=y\right]$,
and this resulting expectation is a function of (deterministic variable) $z$.  One can informally think about this
as being the expectation of $f(x,y,z)$ with respect to $p(x|y)dx$:
our expectation is only over the randomness associated with drawing from $p(x|y)$.  
%Thus,
%for example, we are not taking an expectation over nor condition upon $z$ in this example
%-- it is treated as a fixed input, even if it is a random variable externally to this expectation.
%Depending on the context, we will switch between these different notations for expectation
%and will sometimes simply using the integral form directly.

Denoting the mean of a random variable $X$ as $\mu=\E [X]$, the \emph{variance} of $X$
is defined using any one of the following equivalent forms
\begin{align}
\var (X) = \E \left[(X-\mu)^2\right] = \int (x-\mu)^2 p(x)dx = \E [X^2] - \mu^2 =
\int x^2 p(x) dx - \mu^2.
\end{align}
In other words, it is the average squared distance of a variable from its mean.
Its square root, the \emph{standard deviation}, informally forms an estimate of the average
amount of variation of the variable from its mean and has units which are the same
as the data.  We will use the same notational conventions as for expectations when defining
variances.

The variance is a particular case of the more general concept of  a covariance between
two random variables $X$ and $Y$.  Defining $\mu_X=\E [X]$ and $\mu_Y=\E [Y]$
then the covariance is defined by any one of the following equivalent forms
\begin{align}
\cov& (X,Y) = \E \left[(X-\mu_X)(Y-\mu_Y)\right] = \iint (x-\mu_X)(y-\mu_Y) p(x,y) dxdy \nonumber \\
&= \E \left[XY\right]-\E \left[X\right]\E \left[Y\right] = \iint xyp(x,y) dxdy - \left(\int xp(x) dx\right)
\left(\int yp(y) dy\right).
\end{align}
The covariance between two variables measures the joint variability of two random
variables.  It is perhaps easiest to interpret through the definition of correlation
(or more specifically, Pearson's correlation coefficient) which is the correlation
scaled by the standard deviation of each of the variables
\begin{align}
\corr (X,Y) = \frac{\cov (X,Y)}{\sqrt{\var(X) \var(Y)}}.
\end{align}
The correlation between two variables is always in the range $[-1,1]$.  Positive
correlations indicate that when one variable is relatively larger, the other variable
also tends to be larger.  The higher the correlation, the more strongly this relationship
holds: if the correlation is $1$ then one variable is \emph{linearly} dependent on the other.  The
same holds for negative correlations except that when one variable increases, the other tends
to decrease.  Independent variables have a correlation (and thus covariance) of zero.
Note that correlation is not causation.